<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F08%2F16%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[GRE AWA MODEL ESSAYS]]></title>
    <url>%2F2018%2F08%2F09%2F2018-08-09-GRE-AWA-MODEL-ESSAYS-ISSUE-1%2F</url>
    <content type="text"><![CDATA[Issue 1“We can usually learn much more from people whose views we share than from people whose views contradict our own.”; disagreement can cause stress and inhibit learning.” Essay&emsp;&emsp;Do we learn more from people whose ideas we share in common than from those whose ideas contradict ours? The speaker deems so, for the reason that disagreement can cause stress and inhibit learning. I concede that undue discord can impede learning. Otherwise, in my view we learn far more from discourse and debate with those whose ideas we oppose than from people whose ideas are in accord with our own. 第一段：作者复述了一下题目中的问题。随后表达了自己的观点，但我们可以看出，作者并不是完全赞同或完全反对一种观点，而是说“我承认争论会阻碍学习，但在另一方面，我认为从观点不同的人那里会学到更多的东西。”这虽然是一边倒的写法，但是倒的不严重，从下文我们也可以看出，作者并不是一直在说“we can learn more from opponents”，而是先用两段的内容写了”debate”的坏处，再用两段写了自己依然认为”we can learn more from people with different views”。这好像是介于“一边倒”与“折中式”的一种论证过程。 首先，“Do we learn more from people whose ideas we share in common than from those whose ideas contradict ours? The speaker deems so, for the reason that disagreement can cause stress and inhibit learning.”这基本就是照抄了题目，为了引出话题。我们可以学习一下这个用法“The speaker deems so, for the reason that…”。 其次作者用两句话分别说明了自己的两个主要观点。第一个“I concede that undue discord can impede learning.”作者说“我承认过度的争吵可以阻碍学习。”这里作者很聪明，表达的观点不是“discord impedes learning”而是”undue discord impedes learning”，下面两段也都是在说“undue discord”为什么会”impede learning”。第二个“Otherwise, in my view we learn far from discourse and debate with those whose ideas we oppose than from those whose ideas are in accord with our own.”说明作者实际上，就是认为“discourse and debate facilitate(help) learning”。(这里的otherwise我不知道具体该怎么理解。) 所以这样看来，虽然作者貌似是偏向于“折中式”，但实际上还是“一边倒”的写法，之前我对文章的理解有误，因为不认识undue是什么意思，所以单词很重要啊。作者也给我们提供了一个很好的写作方法，如果素材不够，我们不一定要一直说某一方面的好，也可以说另一方面有多差，或者另一方面在很极端的情况下就会变的差了。其实我们阅读下文论述也可以知道，作者想表达“undue discord can impede learning”但是“rational and reasonable debate help learning”的意思。可见文章的大局观，从第一段便确定了文章的论述内容。第1段引出论题，确立观点。第2、3段“admit that undue discord impede learning”，第4、5段细说“rational and reasonable debate facilitate(help) learning”，第6段summary。文章写作思路清晰，之后写也好写，该写啥就写啥就行了。 contradict v. 与…对立（这个单词我认识，我只知道它是“对立”的意思，但我从来不知道怎么用这个词，要不要加介词，加什么介词，所以尽管认识这个单词，但是从来也不会使用它，所以今后在学习过程中要积累，记住单词的用法，不仅仅是在文章中能看懂，也要将其使用到自己的文章当中） contradict 这个词的用法很简单，A contradict B，A与B对立 eg. Your actions contradict your words. share in common 以后可以用这个短语来表达立场相同的人”people whose ideas we share in common” concede v. 不情愿的承认 to admit sth in an unwillingly way undue adj. 过度的 more than is reasonable or necessary discord n. 不和，争吵 impede v. 阻碍；妨碍 discourse n. 交谈（这个单词单纯的指两个人的交谈，并没有争论的意思）: the use of word to exchange thoughts and ideas in accord with：accord n. 一致；符合：a formal or official agreement，所以我们又学习了一种说法表示“立场相同的人”：people whose ideas are in accord with our own &emsp;&emsp;Admittedly, under some circumstances disagreement with others can be counterproductive to learning. For supporting examples one need look no further than a television set. On today’s typical television or radio talk show, disagreement usually manifests itself in meaningless rhetorical bouts and shouting matches, during which opponents vie to have their own message heard, but have little interest either in finding common ground with or in acknowledging the merits of the opponent’s viewpoint. Understandably, neither the combatants nor the viewers learn anything meaningful. In fact, these battles only serve to reinforce the predispositions and biases of all concerned. The end result is that learning is impeded. 第二段: 开头“Admittedly(不可否认的)”对应第一段的“concede: unwilling to agree”。“Admittedly, under some circumstances disagreement with others can be counterproductive to learning.”也就是第一段的“undue(under some circumstances) discord can impede learning”。第一句话表达本段的论点，接下来开始举example。”For supporting examples one need to look no further than a television set.”这里最值得我们学习的地方是，我们习惯用“for example / for instance”来举例子，而这里作者使用的是“For supporting examples”这里虽然只是加了一个supporting，但是却脱颖而出，不是大众式的“for example”，与之类似，我们可以在以后的写作中写“For supporting examples, …”, “For supporting instance, …”。“no further than”后面接一个谁都知道的example，所以以后觉得自己举的example太common太simple，就可以用“For supporting examples, one need to look no further than (something common and simple)”，小事折射出大道理。“On today’s typical television or radio talk show, disagreement usually manifest itself in meaningless rhetorical bouts and shouting matches, during which opponents vie to have their own message heard, but have little interest either in finding common ground with or in acknowledging the merits of the opponent’s viewpoint.”这句话比较长，但是结构不难理解，在如今的“typical television and radio talk show”中，人们只是在不停的说自己的观点(opponents vie to have their own message heard)，完全不顾别人的观点(have little interest either in finding common ground with or in acknowledging the merits of the opponents’ viewpoints)，这也是当今社会的一大问题，我们都知道，辩论赛很重要的一个环节就是双方都要先确定好某一个基础(common ground)，咱俩在这个基础之上，再辩论。然而当今社会大家简直是胡乱一气，我说我的，你说你的，他说他的(meaningless rhetorical bouts and shouting matches)，这样的交流简直毫无意义“meaningless”。”Understandably, neither the combatants nor the viewers learn anything meaningful. In fact, there battles only serve to reinforce the predispositions and biases of all concerned. The end result is that learning is impeded.”最后结尾，无论是combatants还是viewers都不能从中受益。这种battles只会加深双方的不和predispositions and biases。最后简单一句话直扣第一段的观点“The end result is that learning is impeded.” manifest v. 使显现，使显露，表明: to show something clearly, especially a feeling, an attitude or a quality rhetorical adj. 语言上的，文字上的，辞藻华丽（却往往无内涵的） bout n. 一回合，较量，一阵 vie v. 竞争：to compete with others in an attempt to get or win something common ground 一致之处，共同点：a basis agreed to by all parties for reaching a mutual understanding combatant n. 战士；好斗者（combat n. 战斗） predisposition n. 倾向 &emsp;&emsp;Disagreement can also inhibit learning when two opponents disagree on fundamental assumptions needed for meaningful discourse and debate. For example, a student of paleontology learns little about the evolution of an animal species under current study by debating with an individual whose religious belief system precludes the possibility of evolution to begin with. And, economics and finance students learn little about the dynamics of a laissez-faire system by debating with a socialist whose view is that a centralized power should control all economic activity. 第三段: 之前看首段后对文章内容的猜测再一次出现了错误，这就是“一边倒”式的文章，我还以为前期会缓和一下。那么接着看第三段如何说明“undue discord can impede learning”。”Disagreement can also inhibit learning when two opponents disagreement on fundamental assumptions needed for meaningful discourse and debate.”这与我们之前说的一致，一个”rational, reasonable, meaningful”的“discourse and debate”必须要有”agreement on fundamental assumptions”。”For example, a student of paleontology learns little about the evolution of an animal species under current study by debating with an individual whose religious belief system precludes the possibility of evolution to begin with.”这个例子很生动形象，一个paleontology专业的人和一个不信仰进化论的人debate前沿的动物种类问题，那个paleontology的哥们能学到啥呢，啥也学不到。这就说明了opponents有一个agreement on fundamental assumptions是多么重要。“And, economics and finance students learn little about the dynamics of a laissez-faire system be debating with a socialist whose view is that a centralized power should control all economic activities.”这里举了另一个形象的例子，经济学的学生们和一个认为“a centralized power should control all economic activities”的政治学家辩论，他们能学到自由市场的活泼性、多样性（dynamics of a laissez-faire system）吗，不能。第三段主要举了两个例子来阐述观点：没有agreements on fundamental assumptions，那么只能导致debate impedes learning的后果了。 paleontology n. 古生物学（这单词，记吧，没别的招，多看几遍就记住了） preclude v. 排除 laissez-faire n. 自由放任政策；放任主义（我个人的理解是指完全放开的市场经济，政府不加以干涉） &emsp;&emsp;Aside from the foregoing two provisos, however, I fundamentally disagree with the speaker’s claim. Assuming common ground between two rational and reasonable opponents willing to debate on intellectual merits, both opponents stand to gain much from that debate. Indeed it is primarily through such debate that human knowledge advances, whether at the personal, community, or global level. 第四段: 从这段开始，文章开始转折。前面都在说“debate impedes learning”，但作者的观点是“debate facilitates(helps) learning”啊。那么现在，就开始转折吧。“Aside from the foregoing two provisions, however, I fundamentally disagree with the speaker’s claim.”然而，除了前面所述的两点，我非常同意“debate facilitates(helps) learning”这一说法。这里我们可以学习一下文章结构的转换，前面写了“(undue) discourse and debate impede learning”现在要写“(rational and reasonable) debate facilitates(helps) learning”，怎么转折呢，就用“Aside from the foregoing two provisions, however, I fundamentally agree with blah blah…”。“Assuming common ground between two rational and reasonable opponents willing to debate on intellectual merits, both opponents stand to gain much from that debate.”与前文形成对比，假设现在有common ground了，那么双方opponents都会gain much from that debate。”Indeed it is primarily through such debate that human knowledge advances, whether at the personal, community or, global level.”这里说的很精彩，正是通过这样的辩论，人类知识才得以进步，无论从个人，社会还是国家层面(whether at personal, community, or global level)。这句话很值得记住，可以在很多题材的文章中升华主题。“Indeed it is primarily through such debate that human knowledge advances, whether at personal, community, or global level.” foregoing adj. 前面的；先前的；前述的 provision n. 条款 &emsp;&emsp;At the personal level, by listening to their parents’ rationale for their seemingly oppressive rules and policies teenagers can learn how certain behaviors naturally carry certain undesirable consequences. At the same time, by listening to their teenagers concerns about autonomy and about peer pressures parents can learn the valuable lesson that effective parenting and control are two different things. At the community level, through dispassionate dialogue an environmental activist can come to understand the legitimate economic concerns of those whose jobs depend on the continued profitable operation of a factory. Conversely, the latter might stand to learn much about the potential public health price to be paid by ensuring job growth and a low unemployment rate. Finally, at the global level, two nations with opposing political or economic interests can reach mutually beneficial agreements by striving to understand the other’s legitimate concerns for its national security, its political sovereignty, the stability of its economy and currency, and so forth. 第五段：这一段实为精彩。分别从个人层面（personal level），社会层面（community level），和国家层面（global level）阐述了“rational and reasonable debate”的重要性和益处。首先是个人层面”At the personal level, by listening to their parents’ rationale for their seemingly oppressive rules and policies, teenagers can learn how certain behaviors naturally carry out undesirable consequences. At the same time, by listening to their teenagers concerns about autonomy and peer pressure, parents can learn the valuable lesson that effective parenting and control are two different things.”这里从个人层面（personal level），写了孩子与家长的矛盾与不和，双方都会从中学到东西。孩子从“parents’ rationale for seemingly oppressive rules and policies”可以学到“how certain behaviors naturally carry out undesirable consequences”，家长从”teenagers concerns about autonomy and peer pressure”可以学到”the valuable lesson that effective parenting and control are two different things”。接下来是社会层面（community level），“At the community level, through dispassionate dialogue an environmental activist can come to understand the legitimate economic concerns of those whose jobs depend on the continued profitable operation of a factory. Conversely, the latter might stand to learn much about the potential public health price to be paid by ensuring job growth and a low unemployment rate.”这里从社会层面（community level），选取了环境保护主义者和立法者之间心平气和的谈话(dispassionate dialogue)，环境保护主义者(environmental activist)可以理解有些人的工作完全依赖日夜不停的工厂(understand the legitimate economic concerns of those whose jobs depend on the continued profitable operation of a factory)，立法者和管理者也会了解到追求低失业率给环境和人民健康付出的潜在代价(the potential public health price to be paid by ensuring job growth and a low unemployment rate)。最后从全球层面（global level），”Finally, at the global level, two nations with opposing political and economical interests can reach mutually beneficial agreement by striving to understand the other’s legitimate concerns for its national security, political sovereignty,the stability of its economic and currency, and so forth.”。从国家层面，政治经济利益相违背的国家(two nations with opposing political and economic interests)，通过理解对方在国家安全、政治主权和经济与货币稳定性的担忧(by striving to understand the other’s legitimate concerns for its national security, political sovereignty, and the stability of its economic and currency, and so forth)，就可以实现共赢的利益(can reach mutually beneficial agreements)。这段通过三个层面的举例论证，表达出“debate facilitate(helps) learning”的意思，而且其含义已经远远扩大，小到个人，大到国家。 rationale n. 基本原理 oppressive adj. 压迫的；沉重的 dispassionate adj. 心平气和的；平心静气的(passionate 激情的) sovereignty n. 主权（reign v. 统治） and so forth 等等，诸如此类 continuing in the same way &emsp;&emsp;In sum, unless two opponents in a debate are each willing to play on the same field and by the same rules, I concede that disagreement can impede learning. Otherwise, reasoned discourse and debate between people with opposing viewpoints is the very foundation upon which human knowledge advances. Accordingly, on balance the speaker is fundamentally correct. 第六段：最后一段要总结啦。”In sum, unless two opponents in a debate are each willing to play on the same file and by the same rules, I concede that disagreement can impede learning.”。除非两者能公平的辩论(play on the same field and by the same rules)，那那个speaker就说的对呗。”Otherwise, reasoned discourse and debate between people with opposing viewpoint is the very foundation upon which human knowledge advances. Accordingly, on balance the speaker is fundamentally correct.”最后两句用到了前几段出现的话再次升华，最后很幽默的说一句，speaker的话也倒不是不对昂。 accordingly adv. 因此；相应的；于是 on balance 总之 写在文章的最后&emsp;&emsp;现在已经1:51am了，这篇文章大概敲了快四个小时吧，以前从没写过GRE的作文，这也是差不多第一次看别人的GRE作文，第一印象就是字数好多，要求的字数也好多，再其次就是这篇文章真的好多好多的词和短语和结构用法都不会，但通过查字典，自己分析，感觉在这一篇文章中学到了太多太多的东西，也再一次认识到了自己知识的匮乏。这些东西也不是一蹴而就的，需要不断的积累，不断的磨练，你也不知道你在什么时候会走到哪一步，也许这就是学习有趣的地方吧，很多都是个未知数。可是留给我们的时间还有多少呢，很多事情都是最近思考才想明白的，大一大二浪费了那么多宝贵的时间，因为无知，因为眼界狭窄，如今称不上眼界宽广的人，但比之前的自己，长大了许多。如今也开始努力，开始不再怀疑梦想，努力的去实现它。这条路必经布满靳棘，太容易让人放弃，而且你也一定是孤身一人的在走，在真正的困难面前，没有人帮的了你，你也心里清楚，就算有人要帮你，你也要拒绝，因为这是你的战斗，你必须一个人战胜它，才叫胜利。有时候也很享受这种生活，很充实，至少学了一天，我不会后悔，但如果玩了一天，那时间过去了就过去了，再也不会回来了。这就叫矫情吧，在痛苦中享受着被虐的快感。不知道这种生活会持续到什么时候，也不知道自己终究会坚持多久，但我知道，要想成功，必须坚持，而且要坚持到最后的最后。听过很多人的故事，但我们终究要写下我们自己的故事，而我希望，我能写的精彩一些。马上也就要开学了，不知道下学期，又会是怎样的自己，马上也就要毕业了，不知道下一站，会到哪里。人生这趟旅途，走走停停，停停走走，你永远不知道自己会走到哪里，也不知道自己在哪里驻足，和谁，看什么样的风景。 &emsp;&emsp;今天的口语还没练，网课还没刷，不矫情了，滚去学习了]]></content>
      <tags>
        <tag>GRE</tag>
        <tag>GRE Issue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning Course2 Week2 Programming Assignment]]></title>
    <url>%2F2018%2F08%2F08%2F2018-08-08-DeepLearning-C2W2-ProgrammingAssignment%2F</url>
    <content type="text"><![CDATA[Optimization Methods - NotesUntil now, you’ve always used Gradient Descent to update the parameters and minimize the cost. In this notebook, you will learn more advanced optimization methods that can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result. Gradient descent goes “downhill” on a cost function $J$. Think of it as trying to do this:img Figure 1 : Minimizing the cost is like finding the lowest point in a hilly landscape At each step of the training, you update your parameters following a certain direction to try to get to the lowest possible point. Notations: As usual, $\frac{\partial J}{\partial a } = $ da for any variable a. To get started, run the following code to import the libraries you will need.123456789101112131415import numpy as npimport matplotlib.pyplot as pltimport scipy.ioimport mathimport sklearnimport sklearn.datasetsfrom opt_utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagationfrom opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_datasetfrom testCases import *%matplotlib inlineplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray' 1 - Gradient DescentA simple optimization method in machine learning is gradient descent (GD). When you take gradient steps with respect to all $m$ examples on each step, it is also called Batch Gradient Descent. Warm-up exercise: Implement the gradient descent update rule. The gradient descent rule is, for $l = 1, …, L$:$$ W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \tag{1}$$$$ b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} \tag{2}$$ where L is the number of layers and $\alpha$ is the learning rate. All parameters should be stored in the parameters dictionary. Note that the iterator l starts at 0 in the for loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift l to l+1 when coding. 1234567891011121314151617181920212223242526272829# GRADED FUNCTION: update_parameters_with_gddef update_parameters_with_gd(parameters, grads, learning_rate): """ Update parameters using one step of gradient descent Arguments: parameters -- python dictionary containing your parameters to be updated: parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads -- python dictionary containing your gradients to update each parameters: grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl learning_rate -- the learning rate, scalar. Returns: parameters -- python dictionary containing your updated parameters """ L = len(parameters) // 2 # number of layers in the neural networks # Update rule for each parameter for l in range(L): ### START CODE HERE ### (approx. 2 lines) parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * grads["dW" + str(l+1)] parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * grads["db" + str(l+1)] ### END CODE HERE ### return parameters 1234567parameters, grads, learning_rate = update_parameters_with_gd_test_case()parameters = update_parameters_with_gd(parameters, grads, learning_rate)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"])) Expected Output: W1 [[ 1.63535156 -0.62320365 -0.53718766] [-1.07799357 0.85639907 -2.29470142]] b1 [[ 1.74604067] [-0.75184921]] W2 [[ 0.32171798 -0.25467393 1.46902454] [-2.05617317 -0.31554548 -0.3756023 ] [ 1.1404819 -1.09976462 -0.1612551 ]] b2 [[-0.88020257] [ 0.02561572] [ 0.57539477]] A variant of this is Stochastic Gradient Descent (SGD), which is equivalent to mini-batch gradient descent where each mini-batch has just 1 example. The update rule that you have just implemented does not change. What changes is that you would be computing gradients on just one training example at a time, rather than on the whole training set. The code examples below illustrate the difference between stochastic gradient descent and (batch) gradient descent. (Batch) Gradient Descent: 123456789101112X = data_inputY = labelsparameters = initialize_parameters(layers_dims)for i in range(0, num_iterations): # Forward propagation a, caches = forward_propagation(X, parameters) # Compute cost. cost = compute_cost(a, Y) # Backward propagation. grads = backward_propagation(a, caches, parameters) # Update parameters. parameters = update_parameters(parameters, grads) Stochastic Gradient Descent: 12345678910111213X = data_inputY = labelsparameters = initialize_parameters(layers_dims)for i in range(0, num_iterations): for j in range(0, m): # Forward propagation a, caches = forward_propagation(X[:,j], parameters) # Compute cost cost = compute_cost(a, Y[:,j]) # Backward propagation grads = backward_propagation(a, caches, parameters) # Update parameters. parameters = update_parameters(parameters, grads) In Stochastic Gradient Descent, you use only 1 training example before updating the gradients. When the training set is large, SGD can be faster. But the parameters will “oscillate” toward the minimum rather than converge smoothly. Here is an illustration of this: Note also that implementing SGD requires 3 for-loops in total: Over the number of iterations Over the $m$ training examples Over the layers (to update all parameters, from $(W^{[1]},b^{[1]})$ to $(W^{[L]},b^{[L]})$) In practice, you’ll often get faster results if you do not use neither the whole training set, nor only one training example, to perform each update. Mini-batch gradient descent uses an intermediate number of examples for each step. With mini-batch gradient descent, you loop over the mini-batches instead of looping over individual training examples. img What you should remember: The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step. You have to tune a learning rate hyperparameter $\alpha$. With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large). 2 - Mini-Batch Gradient descentLet’s learn how to build mini-batches from the training set (X, Y). There are two steps: Shuffle: Create a shuffled version of the training set (X, Y) as shown below. Each column of X and Y represents a training example. Note that the random shuffling is done synchronously between X and Y. Such that after the shuffling the $i^{th}$ column of X is the example corresponding to the $i^{th}$ label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches. img Partition: Partition the shuffled (X, Y) into mini-batches of size mini_batch_size (here 64). Note that the number of training examples is not always divisible by mini_batch_size. The last mini batch might be smaller, but you don’t need to worry about this. When the final mini-batch is smaller than the full mini_batch_size, it will look like this: img Exercise: Implement random_mini_batches. We coded the shuffling part for you. To help you with the partitioning step, we give you the following code that selects the indexes for the $1^{st}$ and $2^{nd}$ mini-batches:123first_mini_batch_X = shuffled_X[:, 0 : mini_batch_size]second_mini_batch_X = shuffled_X[:, mini_batch_size : 2 * mini_batch_size]... Note that the last mini-batch might end up smaller than mini_batch_size=64. Let $\lfloor s \rfloor$ represents $s$ rounded down to the nearest integer (this is math.floor(s) in Python). If the total number of examples is not a multiple of mini_batch_size=64 then there will be $\lfloor \frac{m}{mini_batch_size}\rfloor$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be ($m-mini__batch__size \times \lfloor \frac{m}{mini_batch_size}\rfloor$). 1234567891011121314151617181920212223242526272829303132333435363738394041424344# GRADED FUNCTION: random_mini_batchesdef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0): """ Creates a list of random minibatches from (X, Y) Arguments: X -- input data, of shape (input size, number of examples) Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples) mini_batch_size -- size of the mini-batches, integer Returns: mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y) """ np.random.seed(seed) # To make your "random" minibatches the same as ours m = X.shape[1] # number of training examples mini_batches = [] # Step 1: Shuffle (X, Y) permutation = list(np.random.permutation(m)) shuffled_X = X[:, permutation] shuffled_Y = Y[:, permutation].reshape((1,m)) # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case. num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning for k in range(0, num_complete_minibatches): ### START CODE HERE ### (approx. 2 lines) mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size] mini_batch_Y = shuffled_Y[k * mini_batch_size : (k+1) * mini_batch_size] ### END CODE HERE ### mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) # Handling the end case (last mini-batch &lt; mini_batch_size) if m % mini_batch_size != 0: ### START CODE HERE ### (approx. 2 lines) mini_batch_X = shuffled_X[:, (k+1) * mini_batch_size : m] mini_batch_Y = shuffled_Y[(k+1) * mini_batch_size : m] ### END CODE HERE ### mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) return mini_batches 12345678910X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)print ("shape of the 1st mini_batch_X: " + str(mini_batches[0][0].shape))print ("shape of the 2nd mini_batch_X: " + str(mini_batches[1][0].shape))print ("shape of the 3rd mini_batch_X: " + str(mini_batches[2][0].shape))print ("shape of the 1st mini_batch_Y: " + str(mini_batches[0][1].shape))print ("shape of the 2nd mini_batch_Y: " + str(mini_batches[1][1].shape)) print ("shape of the 3rd mini_batch_Y: " + str(mini_batches[2][1].shape))print ("mini batch sanity check: " + str(mini_batches[0][0][0][0:3])) Expected Output: shape of the 1st mini_batch_X (12288, 64) shape of the 2nd mini_batch_X (12288, 64) shape of the 3rd mini_batch_X (12288, 20) shape of the 1st mini_batch_Y (1, 64) shape of the 2nd mini_batch_Y (1, 64) shape of the 3rd mini_batch_Y (1, 20) mini batch sanity check [ 0.90085595 -0.7612069 0.2344157 ] What you should remember: Shuffling and Partitioning are the two steps required to build mini-batches Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128. 3 - MomentumBecause mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations. Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill. img Exercise: Initialize the velocity. The velocity, $v$, is a python dictionary that needs to be initialized with arrays of zeros. Its keys are the same as those in the grads dictionary, that is:for $l =1,…,L$:12v["dW" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters["W" + str(l+1)])v["db" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters["b" + str(l+1)]) Note that the iterator l starts at 0 in the for loop while the first parameters are v[“dW1”] and v[“db1”] (that’s a “one” on the superscript). This is why we are shifting l to l+1 in the for loop. 1234567891011121314151617181920212223242526272829# GRADED FUNCTION: initialize_velocitydef initialize_velocity(parameters): """ Initializes the velocity as a python dictionary with: - keys: "dW1", "db1", ..., "dWL", "dbL" - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters. Arguments: parameters -- python dictionary containing your parameters. parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl Returns: v -- python dictionary containing the current velocity. v['dW' + str(l)] = velocity of dWl v['db' + str(l)] = velocity of dbl """ L = len(parameters) // 2 # number of layers in the neural networks v = &#123;&#125; # Initialize velocity for l in range(L): ### START CODE HERE ### (approx. 2 lines) v["dW" + str(l+1)] = np.zeros(parameters["W" + str(l+1)].shape) v["db" + str(l+1)] = np.zeros(parameters["b" + str(l+1)].shape) ### END CODE HERE ### return v 1234567parameters = initialize_velocity_test_case()v = initialize_velocity(parameters)print("v[\"dW1\"] = " + str(v["dW1"]))print("v[\"db1\"] = " + str(v["db1"]))print("v[\"dW2\"] = " + str(v["dW2"]))print("v[\"db2\"] = " + str(v["db2"])) Expected Output: v[“dW1”] [[ 0. 0. 0.] [ 0. 0. 0.]] v[“db1”] [[ 0.] [ 0.]] v[“dW2”] [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] v[“db2”] [[ 0.] [ 0.] [ 0.]] Exercise: Now, implement the parameters update with momentum. The momentum update rule is, for $l = 1, …, L$: $$ \begin{cases}v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}}\end{cases}\tag{3}$$ $$\begin{cases}v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}}\end{cases}\tag{4}$$ where L is the number of layers, $\beta$ is the momentum and $\alpha$ is the learning rate. All parameters should be stored in the parameters dictionary. Note that the iterator l starts at 0 in the for loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that’s a “one” on the superscript). So you will need to shift l to l+1 when coding. 123456789101112131415161718192021222324252627282930313233343536373839# GRADED FUNCTION: update_parameters_with_momentumdef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate): """ Update parameters using Momentum Arguments: parameters -- python dictionary containing your parameters: parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads -- python dictionary containing your gradients for each parameters: grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl v -- python dictionary containing the current velocity: v['dW' + str(l)] = ... v['db' + str(l)] = ... beta -- the momentum hyperparameter, scalar learning_rate -- the learning rate, scalar Returns: parameters -- python dictionary containing your updated parameters v -- python dictionary containing your updated velocities """ L = len(parameters) // 2 # number of layers in the neural networks # Momentum update for each parameter for l in range(L): ### START CODE HERE ### (approx. 4 lines) # compute velocities v["dW" + str(l+1)] = beta * v["dW" + str(l+1)] + (1 - beta) * grads["dW" + str(l+1)] v["db" + str(l+1)] = beta * v["db" + str(l+1)] + (1 - beta) * grads["db" + str(l+1)] # update parameters parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * v["dW" + str(l+1)] parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * v["db" + str(l+1)] ### END CODE HERE ### return parameters, v 1234567891011parameters, grads, v = update_parameters_with_momentum_test_case()parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"]))print("v[\"dW1\"] = " + str(v["dW1"]))print("v[\"db1\"] = " + str(v["db1"]))print("v[\"dW2\"] = " + str(v["dW2"]))print("v[\"db2\"] = " + str(v["db2"])) Expected Output: W1 [[ 1.62544598 -0.61290114 -0.52907334] [-1.07347112 0.86450677 -2.30085497]] b1 [[ 1.74493465] [-0.76027113]] W2 [[ 0.31930698 -0.24990073 1.4627996 ] [-2.05974396 -0.32173003 -0.38320915] [ 1.13444069 -1.0998786 -0.1713109 ]] b2 [[-0.87809283] [ 0.04055394] [ 0.58207317]] v[“dW1”] [[-0.11006192 0.11447237 0.09015907] [ 0.05024943 0.09008559 -0.06837279]] v[“db1”] [[-0.01228902] [-0.09357694]] v[“dW2”] [[-0.02678881 0.05303555 -0.06916608] [-0.03967535 -0.06871727 -0.08452056] [-0.06712461 -0.00126646 -0.11173103]] v[“db2”] [[ 0.02344157] [ 0.16598022] [ 0.07420442]] Note that: The velocity is initialized with zeros. So the algorithm will take a few iterations to “build up” velocity and start to take bigger steps. If $\beta = 0$, then this just becomes standard gradient descent without momentum. How do you choose $\beta$? The larger the momentum $\beta$ is, the smoother the update because the more we take the past gradients into account. But if $\beta$ is too big, it could also smooth out the updates too much. Common values for $\beta$ range from 0.8 to 0.999. If you don’t feel inclined to tune this, $\beta = 0.9$ is often a reasonable default. Tuning the optimal $\beta$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$. What you should remember: Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent. You have to tune a momentum hyperparameter $\beta$ and a learning rate $\alpha$. 4 - AdamAdam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. How does Adam work? It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). It updates parameters in a direction based on combining information from “1” and “2”. The update rule is, for $l = 1, …, L$: $$\begin{cases}v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \v^{corrected}{dW^{[l]}} = \frac{v{dW^{[l]}}}{1 - (\beta_1)^t} \s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \s^{corrected}{dW^{[l]}} = \frac{s{dW^{[l]}}}{1 - (\beta_1)^t} \W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}{dW^{[l]}}}{\sqrt{s^{corrected}{dW^{[l]}}} + \varepsilon}\end{cases}$$where: t counts the number of steps taken of Adam L is the number of layers $\beta_1$ and $\beta_2$ are hyperparameters that control the two exponentially weighted averages. $\alpha$ is the learning rate $\varepsilon$ is a very small number to avoid dividing by zero As usual, we will store all parameters in the parameters dictionary Exercise: Initialize the Adam variables $v, s$ which keep track of the past information. Instruction: The variables $v, s$ are python dictionaries that need to be initialized with arrays of zeros. Their keys are the same as for grads, that is:for $l = 1, …, L$:1234v["dW" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters["W" + str(l+1)])v["db" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters["b" + str(l+1)])s["dW" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters["W" + str(l+1)])s["db" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters["b" + str(l+1)]) 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: initialize_adamdef initialize_adam(parameters) : """ Initializes v and s as two python dictionaries with: - keys: "dW1", "db1", ..., "dWL", "dbL" - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters. Arguments: parameters -- python dictionary containing your parameters. parameters["W" + str(l)] = Wl parameters["b" + str(l)] = bl Returns: v -- python dictionary that will contain the exponentially weighted average of the gradient. v["dW" + str(l)] = ... v["db" + str(l)] = ... s -- python dictionary that will contain the exponentially weighted average of the squared gradient. s["dW" + str(l)] = ... s["db" + str(l)] = ... """ L = len(parameters) // 2 # number of layers in the neural networks v = &#123;&#125; s = &#123;&#125; # Initialize v, s. Input: "parameters". Outputs: "v, s". for l in range(L): ### START CODE HERE ### (approx. 4 lines) v["dW" + str(l+1)] = np.zeros((parameters["W" +str(l+1)].shape)) v["db" + str(l+1)] = np.zeros((parameters["b" +str(l+1)].shape)) s["dW" + str(l+1)] = np.zeros((parameters["W" +str(l+1)].shape)) s["db" + str(l+1)] = np.zeros((parameters["b" +str(l+1)].shape)) ### END CODE HERE ### return v, s 1234567891011parameters = initialize_adam_test_case()v, s = initialize_adam(parameters)print("v[\"dW1\"] = " + str(v["dW1"]))print("v[\"db1\"] = " + str(v["db1"]))print("v[\"dW2\"] = " + str(v["dW2"]))print("v[\"db2\"] = " + str(v["db2"]))print("s[\"dW1\"] = " + str(s["dW1"]))print("s[\"db1\"] = " + str(s["db1"]))print("s[\"dW2\"] = " + str(s["dW2"]))print("s[\"db2\"] = " + str(s["db2"])) Expected Output: v[“dW1”] [[ 0. 0. 0.] [ 0. 0. 0.]] v[“db1”] [[ 0.] [ 0.]] v[“dW2”] [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] v[“db2”] [[ 0.] [ 0.] [ 0.]] s[“dW1”] [[ 0. 0. 0.] [ 0. 0. 0.]] s[“db1”] [[ 0.] [ 0.]] s[“dW2”] [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] s[“db2”] [[ 0.] [ 0.] [ 0.]] Exercise: Now, implement the parameters update with Adam. Recall the general update rule is, for $l = 1, …, L$: $$\begin{cases}v_{W^{[l]}} = \beta_1 v_{W^{[l]}} + (1 - \beta_1) \frac{\partial J }{ \partial W^{[l]} } \v^{corrected}{W^{[l]}} = \frac{v{W^{[l]}}}{1 - (\beta_1)^t} \s_{W^{[l]}} = \beta_2 s_{W^{[l]}} + (1 - \beta_2) (\frac{\partial J }{\partial W^{[l]} })^2 \s^{corrected}{W^{[l]}} = \frac{s{W^{[l]}}}{1 - (\beta_2)^t} \W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}{W^{[l]}}}{\sqrt{s^{corrected}{W^{[l]}}}+\varepsilon}\end{cases}$$ Note that the iterator l starts at 0 in the for loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift l to l+1 when coding. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# GRADED FUNCTION: update_parameters_with_adamdef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8): """ Update parameters using Adam Arguments: parameters -- python dictionary containing your parameters: parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads -- python dictionary containing your gradients for each parameters: grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl v -- Adam variable, moving average of the first gradient, python dictionary s -- Adam variable, moving average of the squared gradient, python dictionary learning_rate -- the learning rate, scalar. beta1 -- Exponential decay hyperparameter for the first moment estimates beta2 -- Exponential decay hyperparameter for the second moment estimates epsilon -- hyperparameter preventing division by zero in Adam updates Returns: parameters -- python dictionary containing your updated parameters v -- Adam variable, moving average of the first gradient, python dictionary s -- Adam variable, moving average of the squared gradient, python dictionary """ L = len(parameters) // 2 # number of layers in the neural networks v_corrected = &#123;&#125; # Initializing first moment estimate, python dictionary s_corrected = &#123;&#125; # Initializing second moment estimate, python dictionary # Perform Adam update on all parameters for l in range(L): # Moving average of the gradients. Inputs: "v, grads, beta1". Output: "v". ### START CODE HERE ### (approx. 2 lines) v["dW" + str(l+1)] = beta1 * v["dW" + str(l+1)] + (1 - beta1) * grads["dW" + str(l+1)] v["db" + str(l+1)] = beta1 * v["db" + str(l+1)] + (1 - beta1) * grads["db" + str(l+1)] ### END CODE HERE ### # Compute bias-corrected first moment estimate. Inputs: "v, beta1, t". Output: "v_corrected". ### START CODE HERE ### (approx. 2 lines) v_corrected["dW" + str(l+1)] = v["dW" + str(l+1)] / (1 - np.power(beta1, t)) v_corrected["db" + str(l+1)] = v["db" + str(l+1)] / (1 - np.power(beta1, t)) ### END CODE HERE ### # Moving average of the squared gradients. Inputs: "s, grads, beta2". Output: "s". ### START CODE HERE ### (approx. 2 lines) s["dW" + str(l+1)] = beta2 * s["dW" + str(l+1)] + (1 - beta2) * np.power(grads["dW" + str(l+1)], 2) s["db" + str(l+1)] = beta2 * s["db" + str(l+1)] + (1 - beta2) * np.power(grads["db" + str(l+1)], 2) ### END CODE HERE ### # Compute bias-corrected second raw moment estimate. Inputs: "s, beta2, t". Output: "s_corrected". ### START CODE HERE ### (approx. 2 lines) s_corrected["dW" + str(l+1)] = s["dW" + str(l+1)] / (1 - np.power(beta2, t)) s_corrected["db" + str(l+1)] = s["db" + str(l+1)] / (1 - np.power(beta2, t)) ### END CODE HERE ### # Update parameters. Inputs: "parameters, learning_rate, v_corrected, s_corrected, epsilon". Output: "parameters". ### START CODE HERE ### (approx. 2 lines) parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * v_corrected["dW" + str(l+1)] / (np.sqrt(s_corrected["dW" + str(l+1)]) + epsilon) parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * v_corrected["db" + str(l+1)] / (np.sqrt(s_corrected["db" + str(l+1)]) + epsilon) ### END CODE HERE ### return parameters, v, s 123456789101112131415parameters, grads, v, s = update_parameters_with_adam_test_case()parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t = 2)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"]))print("v[\"dW1\"] = " + str(v["dW1"]))print("v[\"db1\"] = " + str(v["db1"]))print("v[\"dW2\"] = " + str(v["dW2"]))print("v[\"db2\"] = " + str(v["db2"]))print("s[\"dW1\"] = " + str(s["dW1"]))print("s[\"db1\"] = " + str(s["db1"]))print("s[\"dW2\"] = " + str(s["dW2"]))print("s[\"db2\"] = " + str(s["db2"])) Expected Output: W1 [[ 1.63178673 -0.61919778 -0.53561312] [-1.08040999 0.85796626 -2.29409733]] b1 [[ 1.75225313] [-0.75376553]] W2 [[ 0.32648046 -0.25681174 1.46954931] [-2.05269934 -0.31497584 -0.37661299] [ 1.14121081 -1.09245036 -0.16498684]] b2 [[-0.88529978] [ 0.03477238] [ 0.57537385]] v[“dW1”] [[-0.11006192 0.11447237 0.09015907] [ 0.05024943 0.09008559 -0.06837279]] v[“db1”] [[-0.01228902] [-0.09357694]] v[“dW2”] [[-0.02678881 0.05303555 -0.06916608] [-0.03967535 -0.06871727 -0.08452056] [-0.06712461 -0.00126646 -0.11173103]] v[“db2”] [[ 0.02344157] [ 0.16598022] [ 0.07420442]] s[“dW1”] [[ 0.00121136 0.00131039 0.00081287] [ 0.0002525 0.00081154 0.00046748]] s[“db1”] [[ 1.51020075e-05] [ 8.75664434e-04]] s[“dW2”] [[ 7.17640232e-05 2.81276921e-04 4.78394595e-04] [ 1.57413361e-04 4.72206320e-04 7.14372576e-04] [ 4.50571368e-04 1.60392066e-07 1.24838242e-03]] s[“db2”] [[ 5.49507194e-05] [ 2.75494327e-03] [ 5.50629536e-04]] You now have three working optimization algorithms (mini-batch gradient descent, Momentum, Adam). Let’s implement a model with each of these optimizers and observe the difference. 5 - Model with different optimization algorithmsLets use the following “moons” dataset to test the different optimization methods. (The dataset is named “moons” because the data from each of the two classes looks a bit like a crescent-shaped moon.) 1train_X, train_Y = load_dataset() We have already implemented a 3-layer neural network. You will train it with: Mini-batch Gradient Descent: it will call your function: update_parameters_with_gd() Mini-batch Momentum: it will call your functions: initialize_velocity() and update_parameters_with_momentum() Mini-batch Adam: it will call your functions: initialize_adam() and update_parameters_with_adam() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, num_epochs = 10000, print_cost = True): """ 3-layer neural network model which can be run in different optimizer modes. Arguments: X -- input data, of shape (2, number of examples) Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples) layers_dims -- python list, containing the size of each layer learning_rate -- the learning rate, scalar. mini_batch_size -- the size of a mini batch beta -- Momentum hyperparameter beta1 -- Exponential decay hyperparameter for the past gradients estimates beta2 -- Exponential decay hyperparameter for the past squared gradients estimates epsilon -- hyperparameter preventing division by zero in Adam updates num_epochs -- number of epochs print_cost -- True to print the cost every 1000 epochs Returns: parameters -- python dictionary containing your updated parameters """ L = len(layers_dims) # number of layers in the neural networks costs = [] # to keep track of the cost t = 0 # initializing the counter required for Adam update seed = 10 # For grading purposes, so that your "random" minibatches are the same as ours # Initialize parameters parameters = initialize_parameters(layers_dims) # Initialize the optimizer if optimizer == "gd": pass # no initialization required for gradient descent elif optimizer == "momentum": v = initialize_velocity(parameters) elif optimizer == "adam": v, s = initialize_adam(parameters) # Optimization loop for i in range(num_epochs): # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch seed = seed + 1 minibatches = random_mini_batches(X, Y, mini_batch_size, seed) for minibatch in minibatches: # Select a minibatch (minibatch_X, minibatch_Y) = minibatch # Forward propagation a3, caches = forward_propagation(minibatch_X, parameters) # Compute cost cost = compute_cost(a3, minibatch_Y) # Backward propagation grads = backward_propagation(minibatch_X, minibatch_Y, caches) # Update parameters if optimizer == "gd": parameters = update_parameters_with_gd(parameters, grads, learning_rate) elif optimizer == "momentum": parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate) elif optimizer == "adam": t = t + 1 # Adam counter parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon) # Print the cost every 1000 epoch if print_cost and i % 1000 == 0: print ("Cost after epoch %i: %f" %(i, cost)) if print_cost and i % 100 == 0: costs.append(cost) # plot the cost plt.plot(costs) plt.ylabel('cost') plt.xlabel('epochs (per 100)') plt.title("Learning rate = " + str(learning_rate)) plt.show() return parameters You will now run this 3 layer neural network with each of the 3 optimization methods. 5.1 - Mini-batch Gradient descentRun the following code to see how the model does with mini-batch gradient descent. 12345678910111213# train 3-layer modellayers_dims = [train_X.shape[0], 5, 2, 1]parameters = model(train_X, train_Y, layers_dims, optimizer = "gd")# Predictpredictions = predict(train_X, train_Y, parameters)# Plot decision boundaryplt.title("Model with Gradient Descent optimization")axes = plt.gca()axes.set_xlim([-1.5,2.5])axes.set_ylim([-1,1.5])plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y) 5.2 - Mini-batch gradient descent with momentumRun the following code to see how the model does with momentum. Because this example is relatively simple, the gains from using momemtum are small; but for more complex problems you might see bigger gains. 12345678910111213# train 3-layer modellayers_dims = [train_X.shape[0], 5, 2, 1]parameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = "momentum")# Predictpredictions = predict(train_X, train_Y, parameters)# Plot decision boundaryplt.title("Model with Momentum optimization")axes = plt.gca()axes.set_xlim([-1.5,2.5])axes.set_ylim([-1,1.5])plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y) 5.3 - Mini-batch with Adam modeRun the following code to see how the model does with Adam.12345678910111213# train 3-layer modellayers_dims = [train_X.shape[0], 5, 2, 1]parameters = model(train_X, train_Y, layers_dims, optimizer = "adam")# Predictpredictions = predict(train_X, train_Y, parameters)# Plot decision boundaryplt.title("Model with Adam optimization")axes = plt.gca()axes.set_xlim([-1.5,2.5])axes.set_ylim([-1,1.5])plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y) 5.4 - Summary optimization method accuracy cost shape Gradient descent 79.7% oscillations Momentum 79.7% oscillations Adam 94% smoother Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm. Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster. Some advantages of Adam include: Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) Usually works well even with little tuning of hyperparameters (except $\alpha$) References: Adam paper: https://arxiv.org/pdf/1412.6980.pdf]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
</search>
