<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F08%2F16%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[Liulishuo English Reading Day7]]></title>
    <url>%2F2018%2F08%2F09%2F2018-08-09-liulishuo-7%2F</url>
    <content type="text"><![CDATA[How the ‘brainy’ book become a publishing phenomenon brainy adj. 聪明的；智商很高的 publishing phenomenon This is a story about a book that just kept selling, catching publishers, booksellers and even its author off guard. The book is Sapiens, by the Israeli academic Yuval Noah Harari, published in the UK in September 2014. It’s a recondite work of evolutionary history charting the development of humankind through a scholarly examination of our ability to cooperate as a species. catch sb off guard: 使某人措手不及 recondite adj. 高深的；深奥的 chart v. 记录 scholarly adj. 学术视角的 Sapiens has sold a further half million copies, establishing itself firmly at the top of the bestseller lists. The book’s wild success is symptomatic of a broader trend in our book-buying habits: a surge in the popularity of intelligent, challenging nonfiction, often books that are several years old. wild success 巨大的成功 symptomatic of sth 某物存在的显示 eg. Jealous is usually symptomatic of low self-esteem. 嫉妒通常是不自信的显示。 surge n. 激增，突然上涨 It was trade publication, the Bookseller, that first noted the rise of what it called the “brainy backlist”. It also highlighted a concomitant fall in the sales of the books that had been such a staple of publishers’ catalogues—celebrity biographies. Mark Richards, publisher at John Murray Press, sees the return to serious works of nonfiction as a response to the spirit of the age. “People have a hunger both for information and facts, and for nuanced exploration of issues, of a sort that books are in a prime position to provide.” he says. trade publication: 在某一个领域内发行的杂志期刊 backlist n. 在售的图书目录 concomitant adj. describe sth that is happening and connected with another thing 同时发生的；伴随的 an increase in student numbers and the concomitant in class size staple n. 订书钉；某物的主要部分或主要产品 staple of publishers’ catalogue celebrity biography 名人传记 have a hunger for sth nuanced adj. 有细微差别的 At a time when politics is more furious and fragmented than ever, when technology is colonizing our everyday existence, when medicine is reshaping our lives, we still look to books to make sense of things, to feel ourselves part of a great communal effort to understand our age. These are serious times and they demand serious, intelligent and challenging books. furious adj. 极度生气的；怒不可遏的 make sense of things: understand something that is difficult or has no clear meaning communal n. 在较大的社会内部不同的群体之间的 communal harmony 群体和谐 这个故事与一本卖到停不下来的畅销书有关，此书的畅销程度连出版商、书商、甚至是作者本人都没有预料到。这本书就是来自以色列学者尤瓦尔·诺亚·赫拉利的《人类简史》，于 2014 年 9 月在英国发行。这是一部关于进化史的深奥的作品，通过从学术视角检验人类作为一个物种所具有的协作能力，记录了人类的发展。 《人类简史》一书已加售五十万册，坐稳了冠军畅销书的位置。该书的巨大成功反应了我们购书习惯中的一个更广泛的趋势，那就是智识类的、具有挑战性的非虚构作品受到追捧，销量突增，且这些作品通常已经出版几年了。 贸易出版杂志 Bookseller 是第一个注意到此类图书的崛起的，它将其称为“烧脑存书目录”。该杂志同时也强调，出版商们的常年主打书目——名人传记类书籍的销量随之下降了。马克·理查德是约翰·穆雷出版社的一名出版人，他把人们对纪实类严肃作品的回归视为对时代精神的一种回应，他说：“人们渴望信息和真相，也渴求对严肃议题进行细微差别上的探索，想要满足这种渴求，书籍是不二之选。” 在一个政治活动比以往任何时候都怒不可遏和支离破碎的时代，在一个科技殖民着我们的日常生活的时代，在一个医学重塑着我们的生命的时代，我们仍然盼望着书本能帮助我们理解万物，能让我们感觉自己是群体努力的一份子，帮助我们去理解所处的时代。这是严肃的时代，所以需要严肃的、有智识的、有挑战性的书本与之呼应。 优秀表达 At a time when politic is more furious and fragmented than ever, when technology is colonizing our everyday existence, when medicine is reshaping our lives, we still look to books to make sense of things, to feel ourselves part of a great communal effort to understand our age.]]></content>
      <tags>
        <tag>English Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GRE AWA MODEL ESSAYS]]></title>
    <url>%2F2018%2F08%2F09%2F2018-08-09-GRE-AWA-MODEL-ESSAYS-ISSUE-1%2F</url>
    <content type="text"><![CDATA[Issue 1“We can usually learn much more from people whose views we share than from people whose views contradict our own.”; disagreement can cause stress and inhibit learning.” Essay&emsp;&emsp;Do we learn more from people whose ideas we share in common than from those whose ideas contradict ours? The speaker deems so, for the reason that disagreement can cause stress and inhibit learning. I concede that undue discord can impede learning. Otherwise, in my view we learn far more from discourse and debate with those whose ideas we oppose than from people whose ideas are in accord with our own. 第一段：作者复述了一下题目中的问题。随后表达了自己的观点，但我们可以看出，作者并不是完全赞同或完全反对一种观点，而是说“我承认争论会阻碍学习，但在另一方面，我认为从观点不同的人那里会学到更多的东西。”这虽然是一边倒的写法，但是倒的不严重，从下文我们也可以看出，作者并不是一直在说“we can learn more from opponents”，而是先用两段的内容写了”debate”的坏处，再用两段写了自己依然认为”we can learn more from people with different views”。这好像是介于“一边倒”与“折中式”的一种论证过程。 首先，“Do we learn more from people whose ideas we share in common than from those whose ideas contradict ours? The speaker deems so, for the reason that disagreement can cause stress and inhibit learning.”这基本就是照抄了题目，为了引出话题。我们可以学习一下这个用法“The speaker deems so, for the reason that…”。 其次作者用两句话分别说明了自己的两个主要观点。第一个“I concede that undue discord can impede learning.”作者说“我承认过度的争吵可以阻碍学习。”这里作者很聪明，表达的观点不是“discord impedes learning”而是”undue discord impedes learning”，下面两段也都是在说“undue discord”为什么会”impede learning”。第二个“Otherwise, in my view we learn far from discourse and debate with those whose ideas we oppose than from those whose ideas are in accord with our own.”说明作者实际上，就是认为“discourse and debate facilitate(help) learning”。(这里的otherwise我不知道具体该怎么理解。) 所以这样看来，虽然作者貌似是偏向于“折中式”，但实际上还是“一边倒”的写法，之前我对文章的理解有误，因为不认识undue是什么意思，所以单词很重要啊。作者也给我们提供了一个很好的写作方法，如果素材不够，我们不一定要一直说某一方面的好，也可以说另一方面有多差，或者另一方面在很极端的情况下就会变的差了。其实我们阅读下文论述也可以知道，作者想表达“undue discord can impede learning”但是“rational and reasonable debate help learning”的意思。可见文章的大局观，从第一段便确定了文章的论述内容。第1段引出论题，确立观点。第2、3段“admit that undue discord impede learning”，第4、5段细说“rational and reasonable debate facilitate(help) learning”，第6段summary。文章写作思路清晰，之后写也好写，该写啥就写啥就行了。 contradict v. 与…对立（这个单词我认识，我只知道它是“对立”的意思，但我从来不知道怎么用这个词，要不要加介词，加什么介词，所以尽管认识这个单词，但是从来也不会使用它，所以今后在学习过程中要积累，记住单词的用法，不仅仅是在文章中能看懂，也要将其使用到自己的文章当中） contradict 这个词的用法很简单，A contradict B，A与B对立 eg. Your actions contradict your words. share in common 以后可以用这个短语来表达立场相同的人”people whose ideas we share in common” concede v. 不情愿的承认 to admit sth in an unwillingly way undue adj. 过度的 more than is reasonable or necessary discord n. 不和，争吵 impede v. 阻碍；妨碍 discourse n. 交谈（这个单词单纯的指两个人的交谈，并没有争论的意思）: the use of word to exchange thoughts and ideas in accord with：accord n. 一致；符合：a formal or official agreement，所以我们又学习了一种说法表示“立场相同的人”：people whose ideas are in accord with our own &emsp;&emsp;Admittedly, under some circumstances disagreement with others can be counterproductive to learning. For supporting examples one need look no further than a television set. On today’s typical television or radio talk show, disagreement usually manifests itself in meaningless rhetorical bouts and shouting matches, during which opponents vie to have their own message heard, but have little interest either in finding common ground with or in acknowledging the merits of the opponent’s viewpoint. Understandably, neither the combatants nor the viewers learn anything meaningful. In fact, these battles only serve to reinforce the predispositions and biases of all concerned. The end result is that learning is impeded. 第二段: 开头“Admittedly(不可否认的)”对应第一段的“concede: unwilling to agree”。“Admittedly, under some circumstances disagreement with others can be counterproductive to learning.”也就是第一段的“undue(under some circumstances) discord can impede learning”。第一句话表达本段的论点，接下来开始举example。”For supporting examples one need to look no further than a television set.”这里最值得我们学习的地方是，我们习惯用“for example / for instance”来举例子，而这里作者使用的是“For supporting examples”这里虽然只是加了一个supporting，但是却脱颖而出，不是大众式的“for example”，与之类似，我们可以在以后的写作中写“For supporting examples, …”, “For supporting instance, …”。“no further than”后面接一个谁都知道的example，所以以后觉得自己举的example太common太simple，就可以用“For supporting examples, one need to look no further than (something common and simple)”，小事折射出大道理。“On today’s typical television or radio talk show, disagreement usually manifest itself in meaningless rhetorical bouts and shouting matches, during which opponents vie to have their own message heard, but have little interest either in finding common ground with or in acknowledging the merits of the opponent’s viewpoint.”这句话比较长，但是结构不难理解，在如今的“typical television and radio talk show”中，人们只是在不停的说自己的观点(opponents vie to have their own message heard)，完全不顾别人的观点(have little interest either in finding common ground with or in acknowledging the merits of the opponents’ viewpoints)，这也是当今社会的一大问题，我们都知道，辩论赛很重要的一个环节就是双方都要先确定好某一个基础(common ground)，咱俩在这个基础之上，再辩论。然而当今社会大家简直是胡乱一气，我说我的，你说你的，他说他的(meaningless rhetorical bouts and shouting matches)，这样的交流简直毫无意义“meaningless”。”Understandably, neither the combatants nor the viewers learn anything meaningful. In fact, there battles only serve to reinforce the predispositions and biases of all concerned. The end result is that learning is impeded.”最后结尾，无论是combatants还是viewers都不能从中受益。这种battles只会加深双方的不和predispositions and biases。最后简单一句话直扣第一段的观点“The end result is that learning is impeded.” manifest v. 使显现，使显露，表明: to show something clearly, especially a feeling, an attitude or a quality rhetorical adj. 语言上的，文字上的，辞藻华丽（却往往无内涵的） bout n. 一回合，较量，一阵 vie v. 竞争：to compete with others in an attempt to get or win something common ground 一致之处，共同点：a basis agreed to by all parties for reaching a mutual understanding combatant n. 战士；好斗者（combat n. 战斗） predisposition n. 倾向 &emsp;&emsp;Disagreement can also inhibit learning when two opponents disagree on fundamental assumptions needed for meaningful discourse and debate. For example, a student of paleontology learns little about the evolution of an animal species under current study by debating with an individual whose religious belief system precludes the possibility of evolution to begin with. And, economics and finance students learn little about the dynamics of a laissez-faire system by debating with a socialist whose view is that a centralized power should control all economic activity. 第三段: 之前看首段后对文章内容的猜测再一次出现了错误，这就是“一边倒”式的文章，我还以为前期会缓和一下。那么接着看第三段如何说明“undue discord can impede learning”。”Disagreement can also inhibit learning when two opponents disagreement on fundamental assumptions needed for meaningful discourse and debate.”这与我们之前说的一致，一个”rational, reasonable, meaningful”的“discourse and debate”必须要有”agreement on fundamental assumptions”。”For example, a student of paleontology learns little about the evolution of an animal species under current study by debating with an individual whose religious belief system precludes the possibility of evolution to begin with.”这个例子很生动形象，一个paleontology专业的人和一个不信仰进化论的人debate前沿的动物种类问题，那个paleontology的哥们能学到啥呢，啥也学不到。这就说明了opponents有一个agreement on fundamental assumptions是多么重要。“And, economics and finance students learn little about the dynamics of a laissez-faire system be debating with a socialist whose view is that a centralized power should control all economic activities.”这里举了另一个形象的例子，经济学的学生们和一个认为“a centralized power should control all economic activities”的政治学家辩论，他们能学到自由市场的活泼性、多样性（dynamics of a laissez-faire system）吗，不能。第三段主要举了两个例子来阐述观点：没有agreements on fundamental assumptions，那么只能导致debate impedes learning的后果了。 paleontology n. 古生物学（这单词，记吧，没别的招，多看几遍就记住了） preclude v. 排除 laissez-faire n. 自由放任政策；放任主义（我个人的理解是指完全放开的市场经济，政府不加以干涉） &emsp;&emsp;Aside from the foregoing two provisos, however, I fundamentally disagree with the speaker’s claim. Assuming common ground between two rational and reasonable opponents willing to debate on intellectual merits, both opponents stand to gain much from that debate. Indeed it is primarily through such debate that human knowledge advances, whether at the personal, community, or global level. 第四段: 从这段开始，文章开始转折。前面都在说“debate impedes learning”，但作者的观点是“debate facilitates(helps) learning”啊。那么现在，就开始转折吧。“Aside from the foregoing two provisions, however, I fundamentally disagree with the speaker’s claim.”然而，除了前面所述的两点，我非常同意“debate facilitates(helps) learning”这一说法。这里我们可以学习一下文章结构的转换，前面写了“(undue) discourse and debate impede learning”现在要写“(rational and reasonable) debate facilitates(helps) learning”，怎么转折呢，就用“Aside from the foregoing two provisions, however, I fundamentally agree with blah blah…”。“Assuming common ground between two rational and reasonable opponents willing to debate on intellectual merits, both opponents stand to gain much from that debate.”与前文形成对比，假设现在有common ground了，那么双方opponents都会gain much from that debate。”Indeed it is primarily through such debate that human knowledge advances, whether at the personal, community or, global level.”这里说的很精彩，正是通过这样的辩论，人类知识才得以进步，无论从个人，社会还是国家层面(whether at personal, community, or global level)。这句话很值得记住，可以在很多题材的文章中升华主题。“Indeed it is primarily through such debate that human knowledge advances, whether at personal, community, or global level.” foregoing adj. 前面的；先前的；前述的 provision n. 条款 &emsp;&emsp;At the personal level, by listening to their parents’ rationale for their seemingly oppressive rules and policies teenagers can learn how certain behaviors naturally carry certain undesirable consequences. At the same time, by listening to their teenagers concerns about autonomy and about peer pressures parents can learn the valuable lesson that effective parenting and control are two different things. At the community level, through dispassionate dialogue an environmental activist can come to understand the legitimate economic concerns of those whose jobs depend on the continued profitable operation of a factory. Conversely, the latter might stand to learn much about the potential public health price to be paid by ensuring job growth and a low unemployment rate. Finally, at the global level, two nations with opposing political or economic interests can reach mutually beneficial agreements by striving to understand the other’s legitimate concerns for its national security, its political sovereignty, the stability of its economy and currency, and so forth. 第五段：这一段实为精彩。分别从个人层面（personal level），社会层面（community level），和国家层面（global level）阐述了“rational and reasonable debate”的重要性和益处。首先是个人层面”At the personal level, by listening to their parents’ rationale for their seemingly oppressive rules and policies, teenagers can learn how certain behaviors naturally carry out undesirable consequences. At the same time, by listening to their teenagers concerns about autonomy and peer pressure, parents can learn the valuable lesson that effective parenting and control are two different things.”这里从个人层面（personal level），写了孩子与家长的矛盾与不和，双方都会从中学到东西。孩子从“parents’ rationale for seemingly oppressive rules and policies”可以学到“how certain behaviors naturally carry out undesirable consequences”，家长从”teenagers concerns about autonomy and peer pressure”可以学到”the valuable lesson that effective parenting and control are two different things”。接下来是社会层面（community level），“At the community level, through dispassionate dialogue an environmental activist can come to understand the legitimate economic concerns of those whose jobs depend on the continued profitable operation of a factory. Conversely, the latter might stand to learn much about the potential public health price to be paid by ensuring job growth and a low unemployment rate.”这里从社会层面（community level），选取了环境保护主义者和立法者之间心平气和的谈话(dispassionate dialogue)，环境保护主义者(environmental activist)可以理解有些人的工作完全依赖日夜不停的工厂(understand the legitimate economic concerns of those whose jobs depend on the continued profitable operation of a factory)，立法者和管理者也会了解到追求低失业率给环境和人民健康付出的潜在代价(the potential public health price to be paid by ensuring job growth and a low unemployment rate)。最后从全球层面（global level），”Finally, at the global level, two nations with opposing political and economical interests can reach mutually beneficial agreement by striving to understand the other’s legitimate concerns for its national security, political sovereignty,the stability of its economic and currency, and so forth.”。从国家层面，政治经济利益相违背的国家(two nations with opposing political and economic interests)，通过理解对方在国家安全、政治主权和经济与货币稳定性的担忧(by striving to understand the other’s legitimate concerns for its national security, political sovereignty, and the stability of its economic and currency, and so forth)，就可以实现共赢的利益(can reach mutually beneficial agreements)。这段通过三个层面的举例论证，表达出“debate facilitate(helps) learning”的意思，而且其含义已经远远扩大，小到个人，大到国家。 rationale n. 基本原理 oppressive adj. 压迫的；沉重的 dispassionate adj. 心平气和的；平心静气的(passionate 激情的) sovereignty n. 主权（reign v. 统治） and so forth 等等，诸如此类 continuing in the same way &emsp;&emsp;In sum, unless two opponents in a debate are each willing to play on the same field and by the same rules, I concede that disagreement can impede learning. Otherwise, reasoned discourse and debate between people with opposing viewpoints is the very foundation upon which human knowledge advances. Accordingly, on balance the speaker is fundamentally correct. 第六段：最后一段要总结啦。”In sum, unless two opponents in a debate are each willing to play on the same file and by the same rules, I concede that disagreement can impede learning.”。除非两者能公平的辩论(play on the same field and by the same rules)，那那个speaker就说的对呗。”Otherwise, reasoned discourse and debate between people with opposing viewpoint is the very foundation upon which human knowledge advances. Accordingly, on balance the speaker is fundamentally correct.”最后两句用到了前几段出现的话再次升华，最后很幽默的说一句，speaker的话也倒不是不对昂。 accordingly adv. 因此；相应的；于是 on balance 总之 写在文章的最后&emsp;&emsp;现在已经1:51am了，这篇文章大概敲了快四个小时吧，以前从没写过GRE的作文，这也是差不多第一次看别人的GRE作文，第一印象就是字数好多，要求的字数也好多，再其次就是这篇文章真的好多好多的词和短语和结构用法都不会，但通过查字典，自己分析，感觉在这一篇文章中学到了太多太多的东西，也再一次认识到了自己知识的匮乏。这些东西也不是一蹴而就的，需要不断的积累，不断的磨练，你也不知道你在什么时候会走到哪一步，也许这就是学习有趣的地方吧，很多都是个未知数。可是留给我们的时间还有多少呢，很多事情都是最近思考才想明白的，大一大二浪费了那么多宝贵的时间，因为无知，因为眼界狭窄，如今称不上眼界宽广的人，但比之前的自己，长大了许多。如今也开始努力，开始不再怀疑梦想，努力的去实现它。这条路必经布满靳棘，太容易让人放弃，而且你也一定是孤身一人的在走，在真正的困难面前，没有人帮的了你，你也心里清楚，就算有人要帮你，你也要拒绝，因为这是你的战斗，你必须一个人战胜它，才叫胜利。有时候也很享受这种生活，很充实，至少学了一天，我不会后悔，但如果玩了一天，那时间过去了就过去了，再也不会回来了。这就叫矫情吧，在痛苦中享受着被虐的快感。不知道这种生活会持续到什么时候，也不知道自己终究会坚持多久，但我知道，要想成功，必须坚持，而且要坚持到最后的最后。听过很多人的故事，但我们终究要写下我们自己的故事，而我希望，我能写的精彩一些。马上也就要开学了，不知道下学期，又会是怎样的自己，马上也就要毕业了，不知道下一站，会到哪里。人生这趟旅途，走走停停，停停走走，你永远不知道自己会走到哪里，也不知道自己在哪里驻足，和谁，看什么样的风景。 &emsp;&emsp;今天的口语还没练，网课还没刷，不矫情了，滚去学习了]]></content>
      <tags>
        <tag>GRE</tag>
        <tag>GRE Issue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Liulishuo English Reading Day6]]></title>
    <url>%2F2018%2F08%2F08%2F2018-08-08-liulishuo-6%2F</url>
    <content type="text"><![CDATA[Therapists are augmenting eating-disorder treatment with virtual-reality headsets therapist n. 心理治疗师 eating-disorder 饮食失调 virtual-reality VR Though most consumers aren’t yet sure what to do with VR, some psychologists are using it to treat anorexia and bulimia. anorexia n. 厌食症 bulimia n. 贪食症 The details: According to the Wall Street Journal, therapists are using VR along with more standard therapy practices to ease patients’ body-and food-related issues. alone with sth therapy practices ease patients’ body-and food-related issues For instance, one doctor combines cognitive behavioral therapy with VR to initially place patients in a relaxing scene. After that, he uses VR to show them an environment that will be more stressful, like a restaurant, and helps them work through any anxiety. combine A with B cognitive behavior therapy cognitive: connected with thinking or conscience mental processes CBT [心理学术语]认知行为疗法，利用谈话作为治疗方法，解决患者的心理、行为问题 initially: at the beginning work through 解决，一步一步的解决 Does it help? A meta-analysis from French and Australian researchers, which was published in April, looked at 26 previous studies and found they indicate that VR is “an acceptable and promising therapeutic tool” for eating-disorder patients. meta-analysis：元分析，将多个研究结果整合在一起的统计方法 an acceptable and promising therapeutic tool Might make sense: VR has rapidly gotten better and cheaper; you can buy a self-contained Oculus Go headset for $199. As VR technology improves, virtual situations will get more realistic while staying far easier to control than real-world settings. And there’s already precedent for using VR for better health: the technology has long been studied for its potential as a painkiller, for instance. self-contained 自给自足的，这里指设备齐全，功能齐全 precedent n. 先例，前例 break precedent 开先例，开先河 unprecedented adj. 史无前例的 painkiller n. 止痛药 Translation虽然大多数消费者还不确定如何面对虚拟现实技术（VR），但一些心理学家正在用它来治疗厌食症和贪食症。 更多细节：根据《华尔街日报》，治疗师们现在不仅使用一些较为标准的治疗方法，还会运用虚拟现实技术来缓解患者的饮食健康问题。 例如，一位医生将认知行为疗法与虚拟现实技术结合起来，这个疗法首先将患者置于轻松的场景中。 在那之后，他使用虚拟现实技术向他们展示一个压力更大的环境，例如餐馆，以此帮助他们解决任何焦虑。 这个疗法有用吗？ 来自法国和澳大利亚的研究人员于4月份发表了一项元分析，统合了之前的 26 项研究后发现，这些研究表明虚拟现实对于饮食失调患者来说是“一种可接受且有前景的治疗工具”。 这事儿可能靠谱：虚拟现实发展迅速，也更加便宜；花 199 美元就可以买到一个独立的 Oculus Go 头戴设备。 随着虚拟现实技术的改进，虚拟情境将变得更加真实，同时比真实世界的场景更容易控制。 并且，已经有使用虚拟现实来改善健康状况的先例：例如，有研究称该技术一直有作为止痛药的潜力。]]></content>
      <tags>
        <tag>English Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GRE Reading 200-1]]></title>
    <url>%2F2018%2F08%2F08%2F2018-08-08-GRE-Reading-200-1%2F</url>
    <content type="text"><![CDATA[2 Francoise Duparc Passage &amp; Questions All of Francoise Duparc’s surviving paintings blend portraiture and genre. Her subjects appear to be acquaintances whom she has asked to pose; she has captured both their self-consciousness and the spontaneity of their everyday activities, the depiction of which characterizes genre painting. But genre painting, especially when it portrayed members of the humblest classes, was never popular in eighteenth-century France. The Le Nain brothers and Georges de La Tour, who also chose such themes, were largely ignored. Their present high standing is due to a different, more democratic political climate and to different aesthetic values: we no longer require artists to provide ideal images of humanity for our moral edification but rather regard such idealization as a falsification of the truth. Duparc gives no improving message and discreetly refrains from judging her subjects. In brief, her works neither elevate nor instruct. This restraint largely explains her lack of popular success during her lifetime, even if her talent did not go completely unrecognized by her eighteenth-century French contemporaries. According to the passage, modern viewers are not likely to value which of the following qualities in a painting? (A) The technical elements of the painting (B) The spontaneity of the painting (C) The moral lesson imparted by the painting (D) The degree to which the painting realistically depicts its subject (E) The degree to which the artist’s personality is revealed in the painting If the history of Duparc’s artistic reputation were to follow that of the Le Nain brothers and Georges de La Tour, present-day assessments of her work would be likely to contain which of the following? (A) An evaluation that accords high status to her work (B) Acknowledgement of her technical expertise but dismissal of her subject matter as trivial (C) Agreement with assessments made in her own time but acknowledgements of the exceptional quality of a few of her paintings (D) Placement of her among the foremost artists of her century (E) A reclassification of her work as portraiture rather than genre paintingIt can be inferred from the passage that the term “genre painting” would most likely apply to which of the following? (A) A painting depicting a glorious moment of victory following a battle (B) A painting illustrating a narrative from the Bible (C) A portrayal of a mythological Greek goddess (D) A portrayal of a servant engaged in his work (E) A formal portrait of an eighteenth-century kingThe argument of the passage best supports which of the following contentions concerning judgments of artistic work? (A) Aesthetic judgments can be influenced by the political beliefs of those making the judgment. (B) Judgments of the value of an artist’s work made by his or her contemporaries must be discounted before a true judgment can be made. (C) Modern aesthetic taste is once again moving in the direction of regarding idealistic painting as the most desirable form of painting. (D) In order to be highly regarded, an artist cannot be solely identified with one particular kind of painting. (E) Spontaneity is the most valuable quality a portrait painter can have. Questions Analysis According to the passage, modern viewers are not likely to value which of the following qualities in a painting? (A) The technical elements of the painting (B) The spontaneity of the painting (C) The moral lesson imparted by the painting (D) The degree to which the painting realistically depicts its subject (E) The degree to which the artist’s personality is revealed in the painting Location:”…we no longer require artists to provide ideal images of humanity for our moral edification but rather regard such idealization as a falsification of the truth. ” Answer: C If the history of Duparc’s artistic reputation were to follow that of the Le Nain brothers and Georges de La Tour, present-day assessments of her work would be likely to contain which of the following? (A) An evaluation that accords high status to her work (B) Acknowledgement of her technical expertise but dismissal of her subject matter as trivial (C) Agreement with assessments made in her own time but acknowledgements of the exceptional quality of a few of her paintings (D) Placement of her among the foremost artists of her century (E) A reclassification of her work as portraiture rather than genre painting(瞎说，这个肯定不对) Location: “Their present high standing is due to a different, more democratic political climate and to different aesthetic values” Answer: A It can be inferred from the passage that the term “genre painting” would most likely apply to which of the following? (A) A painting depicting a glorious moment of victory following a battle (B) A painting illustrating a narrative from the Bible (C) A portrayal of a mythological Greek goddess (D) A portrayal of a servant engaged in his work (E) A formal portrait of an eighteenth-century king Location: POE Answer: D The argument of the passage best supports which of the following contentions concerning judgments of artistic work? (A) Aesthetic judgments can be influenced by the political beliefs of those making the judgment. (B) Judgments of the value of an artist’s work made by his or her contemporaries must be discounted before a true judgment can be made. (C) Modern aesthetic taste is once again moving in the direction of regarding idealistic painting as the most desirable form of painting. (D) In order to be highly regarded, an artist cannot be solely identified with one particular kind of painting. (E) Spontaneity is the most valuable quality a portrait painter can have. Location: “Their present high standing is due to a different, more democratic political climate and to different aesthetic values: ” Answer: A Words blend: mix, combineportraiture n. 肖像画genre n. 风俗画discreetly adj. 谨慎的refrain v. 避免；抑制falsification n. 编造；伪造；歪曲foremost adj. :the most important or famouscontention n. 争论；辩论；论点 3 Paule Marshall’s Brown Girl Passage &amp; Questions Paule Marshall’s Brown Girl, Brownstones (1959) was a landmark in the depiction of female characters in Black American literature. Marshall avoided the oppressed and tragic heroine in conflict with White society that had been typical of the protest novels of the early twentieth century. Like her immediate predecessors, Zora Neale Hurston and Gwendolyn Brooks, she focused her novel on an ordinary Black woman’s search for identity within the context of a Black community. But Marshall extended the analysis of Black female characters begun by Hurston and Brooks by depicting her heroine’s development in terms of the relationship between her Barbadian American parents, and by exploring how male and female roles were defined by their immigrant culture, which in turn was influenced by the materialism of White America. By placing characters within a wider cultural context, Marshall attacked racial and sexual stereotypes and paved the way for explorations of race, class, and gender in the novels of the 1970’s. The passage is primarily concerned with (A) comparing the works of three Black American authors (B) describing common themes in Black American literature (C) discussing an important work in Black American literature (D) providing insights about Black American literature in the early twentieth century (E) providing historical information about the writing of Black American novels in the second half the twentieth centuryAccording to the passage, Hurston, Brooks, and Marshall are alike in that they (A) did not examine the effects of White culture on their characters’ lives (B) were heavily influenced by the protest novels of the early twentieth century (C) used Black communities as the settings for their novels (D) wrote primarily about the difficulties their characters encountered in White culture (E) wrote exclusively about female characters and the experiences of womenThe author’s description of the way in which Marshall depicts her heroine’s development is most probably intended to (A) continue the discussion of similarities in the works of Brooks, Hurston, and Marshall (B) describe the specific racial and sexual stereotypes that Marshall attacked (C) contrast the characters in Marshall’s novels with those in later works (D) show how Marshall extends the portrayal of character initiated by her predecessors (E) compare themes in Marshall’s early work with themes in her later novelsIt can be inferred that the author of the passage would describe Brown Girl, Brownstones as being (A) completely different from novels written before 1959 (B) highly influenced by novels written in the early twentieth century (C) similar to the protest novels that preceded it (D) important in the late 1950’s but dated today (E) an important influence on novels written in the 1970’s 4 Laura Gilpin Passage &amp; Questions Before Laura Gilpin (1891-1979), few women in the history of photography had so devoted themselves to chronicling the landscape. Other women had photographed the land, but none can be regarded as a landscape photographer with a sustained body of work documenting the physical terrain. Anne Brigman often photographed woodlands and coastal areas, but they were generally settings for her artfully placed subjects. Dorothea Lange’s landscapes were always conceived of as counterparts to her portraits of rural women. At the same time that Gilpin’s interest in landscape work distinguished her from most other women photographers, her approach to landscape photography set her apart from men photographers who, like Gilpin, documented the western United States. Western American landscape photography grew out of a male tradition, pioneered by photographers attached to government and commercial survey teams that went west in the 1860’s and 1870’s. These explorer-photographers documented the West that their employers wanted to see: an exotic and majestic land shaped by awesome natural forces, unpopulated and ready for American settlement. The next generation of male photographers, represented by Ansel Adams and Eliot Porter, often worked with conservationist groups rather than government agencies or commercial companies, but they nonetheless preserved the “heroic” style and maintained the role of respectful outsider peering in with reverence at a fragile natural world. For Gilpin, by contrast, the landscape was neither an empty vista awaiting human settlement nor a jewel-like scene resisting human intrusion, but a peopled landscape with a rich history and tradition of its own, an environment that shaped and molded the lives of its inhabitants. Her photographs of the Rio Grande, for example, consistently depict the river in terms of its significance to human culture: as a source of irrigation water, a source of food for livestock, and a provider of town sites. Also instructive is Gilpin’s general avoidance of extreme close-ups of her natural subjects: for her, emblematic details could never suggest the intricacies of the interrelationship between people and nature that made the landscape a compelling subject. While it is dangerous to draw conclusions about a “feminine” way of seeing from the work of one woman, it can nonetheless be argued that Gilpin’s unique approach to landscape photography was analogous to the work of many women writers who, far more than their male counterparts, described the landscape in terms of its potential to sustain human life. Gilpin never spoke of herself as a photographer with a feminine perspective: she eschewed any discussion of gender as it related to her work and maintained little interest in interpretations that relied on the concept of a “woman’s eye.” Thus it is ironic that her photographic evocation of a historical landscape should so clearly present a distinctively feminine approach to landscape photography. Which of the following best expresses the main idea of the passage? (A) Gilpin’s landscape photographs more accurately documented the Southwest than did the photographs of explorers and conservationists. (B) Gilpin’s style of landscape photography substantially influenced the heroic style practiced by her male counterparts. (C) The labeling of Gilpin’s style of landscape photography as feminine ignores important ties between it and the heroic style. (D) Gilpin’s work exemplifies an arguably feminine style of landscape photography that contrasts with the style used by her male predecessors. (E) Gilpin’s style was strongly influenced by the work of women writers who described the landscape in terms of its relationship to people.It can be inferred from the passage that the teams mentioned in line 19 were most interested in which of the following aspects of the land in the western United States? (A) Its fragility in the face of increased human intrusion (B) Its role in shaping the lives of indigenous peoples (C) Its potential for sustaining future settlements (D) Its importance as an environment for rare plants and animals (E) Its unusual vulnerability to extreme natural forcesThe author of the passage claims that which of the following is the primary reason why Gilpin generally avoided extreme close-ups of natural subjects? (A) Gilpin believed that pictures of natural details could not depict the interrelationship between the land and humans. (B) Gilpin considered close-up photography to be too closely associated with her predecessors. (C) Gilpin believed that all of her photographs should include people in them. (D) Gilpin associated close-up techniques with photography used for commercial purposes. (E) Gilpin feared that pictures of small details would suggest an indifference to the fragility of the land as a whole.The passage suggests that a photographer who practiced the heroic style would be most likely to emphasize which of the following in a photographic series focusing on the Rio Grande? (A) Indigenous people and their ancient customs relating to the river (B) The exploits of navigators and explorers (C) Unpopulated, pristine parts of the river and its surroundings (D) Existing commercial ventures that relied heavily on the river (E) The dams and other monumental engineering structures built on the riverIt can be inferred from the passage that the first two generations of landscape photographers in the western United States had which of the following in common? (A) They photographed the land as an entity that had little interaction with human culture. (B) They advanced the philosophy that photographers should resist alliances with political or commercial groups. (C) They were convinced that the pristine condition of the land needed to be preserved by government action. (D) They photographed the land as a place ready for increased settlement. (E) They photographed only those locations where humans had settled.Based on the description of her works in the passage, which of the following would most likely be a subject for a photograph taken by Gilpin? (A) A vista of a canyon still untouched by human culture (B) A portrait of a visitor to the West against a desert backdrop (C) A view of historic Native American dwellings carved into the side of a natural cliff (D) A picture of artifacts from the West being transported to the eastern United States for retail sale (E) An abstract pattern created by the shadows of clouds on the desertThe author of the passage mentions women writers in the third paragraph most likely in order to (A) counter a widely held criticism of her argument (B) bolster her argument that Gilpin’s style can be characterized as a feminine style (C) suggest that Gilpin took some of her ideas for photographs from landscape descriptions by women writers (D) clarify the interrelationship between human culture and the land that Gilpin was attempting to capture (E) offer an analogy between photographic close-ups and literary descriptions of small details Answers: D C A C A C B Words chronicle v. 记录；把…载入编年史 sustained adj. 持久的；经久不衰的 conceive v. 想到；设想；怀孕 explorer-photographer: 在寻找新大陆，新世界过程中照相的人 reverence n. 敬畏；尊敬；尊严 honor and respect pristine adj. 原始的：not changed by people: left in its nature state vista n. （农村、城市等的）景色 instructive adj. 有教育意义的 close-up n. 特写镜头 emblematic adj. 作为象征的；典型的 feminine adj. 女性的 eschew v. 避开；戒绝 avoid especially you do not think it is right, proper, etc. evocation n. 引起；唤起 （evoke的名词形式）]]></content>
      <tags>
        <tag>GRE</tag>
        <tag>GRE Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning Course2 Week2 Programming Assignment]]></title>
    <url>%2F2018%2F08%2F08%2F2018-08-08-DeepLearning-C2W2-ProgrammingAssignment%2F</url>
    <content type="text"><![CDATA[Optimization Methods - NotesUntil now, you’ve always used Gradient Descent to update the parameters and minimize the cost. In this notebook, you will learn more advanced optimization methods that can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result. Gradient descent goes “downhill” on a cost function $J$. Think of it as trying to do this:img Figure 1 : Minimizing the cost is like finding the lowest point in a hilly landscape At each step of the training, you update your parameters following a certain direction to try to get to the lowest possible point. Notations: As usual, $\frac{\partial J}{\partial a } = $ da for any variable a. To get started, run the following code to import the libraries you will need.123456789101112131415import numpy as npimport matplotlib.pyplot as pltimport scipy.ioimport mathimport sklearnimport sklearn.datasetsfrom opt_utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagationfrom opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_datasetfrom testCases import *%matplotlib inlineplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray' 1 - Gradient DescentA simple optimization method in machine learning is gradient descent (GD). When you take gradient steps with respect to all $m$ examples on each step, it is also called Batch Gradient Descent. Warm-up exercise: Implement the gradient descent update rule. The gradient descent rule is, for $l = 1, …, L$:$$ W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \tag{1}$$$$ b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} \tag{2}$$ where L is the number of layers and $\alpha$ is the learning rate. All parameters should be stored in the parameters dictionary. Note that the iterator l starts at 0 in the for loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift l to l+1 when coding. 1234567891011121314151617181920212223242526272829# GRADED FUNCTION: update_parameters_with_gddef update_parameters_with_gd(parameters, grads, learning_rate): """ Update parameters using one step of gradient descent Arguments: parameters -- python dictionary containing your parameters to be updated: parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads -- python dictionary containing your gradients to update each parameters: grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl learning_rate -- the learning rate, scalar. Returns: parameters -- python dictionary containing your updated parameters """ L = len(parameters) // 2 # number of layers in the neural networks # Update rule for each parameter for l in range(L): ### START CODE HERE ### (approx. 2 lines) parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * grads["dW" + str(l+1)] parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * grads["db" + str(l+1)] ### END CODE HERE ### return parameters 1234567parameters, grads, learning_rate = update_parameters_with_gd_test_case()parameters = update_parameters_with_gd(parameters, grads, learning_rate)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"])) Expected Output: W1 [[ 1.63535156 -0.62320365 -0.53718766] [-1.07799357 0.85639907 -2.29470142]] b1 [[ 1.74604067] [-0.75184921]] W2 [[ 0.32171798 -0.25467393 1.46902454] [-2.05617317 -0.31554548 -0.3756023 ] [ 1.1404819 -1.09976462 -0.1612551 ]] b2 [[-0.88020257] [ 0.02561572] [ 0.57539477]] A variant of this is Stochastic Gradient Descent (SGD), which is equivalent to mini-batch gradient descent where each mini-batch has just 1 example. The update rule that you have just implemented does not change. What changes is that you would be computing gradients on just one training example at a time, rather than on the whole training set. The code examples below illustrate the difference between stochastic gradient descent and (batch) gradient descent. (Batch) Gradient Descent: 123456789101112X = data_inputY = labelsparameters = initialize_parameters(layers_dims)for i in range(0, num_iterations): # Forward propagation a, caches = forward_propagation(X, parameters) # Compute cost. cost = compute_cost(a, Y) # Backward propagation. grads = backward_propagation(a, caches, parameters) # Update parameters. parameters = update_parameters(parameters, grads) Stochastic Gradient Descent: 12345678910111213X = data_inputY = labelsparameters = initialize_parameters(layers_dims)for i in range(0, num_iterations): for j in range(0, m): # Forward propagation a, caches = forward_propagation(X[:,j], parameters) # Compute cost cost = compute_cost(a, Y[:,j]) # Backward propagation grads = backward_propagation(a, caches, parameters) # Update parameters. parameters = update_parameters(parameters, grads) In Stochastic Gradient Descent, you use only 1 training example before updating the gradients. When the training set is large, SGD can be faster. But the parameters will “oscillate” toward the minimum rather than converge smoothly. Here is an illustration of this: Note also that implementing SGD requires 3 for-loops in total: Over the number of iterations Over the $m$ training examples Over the layers (to update all parameters, from $(W^{[1]},b^{[1]})$ to $(W^{[L]},b^{[L]})$) In practice, you’ll often get faster results if you do not use neither the whole training set, nor only one training example, to perform each update. Mini-batch gradient descent uses an intermediate number of examples for each step. With mini-batch gradient descent, you loop over the mini-batches instead of looping over individual training examples. img What you should remember: The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step. You have to tune a learning rate hyperparameter $\alpha$. With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large). 2 - Mini-Batch Gradient descentLet’s learn how to build mini-batches from the training set (X, Y). There are two steps: Shuffle: Create a shuffled version of the training set (X, Y) as shown below. Each column of X and Y represents a training example. Note that the random shuffling is done synchronously between X and Y. Such that after the shuffling the $i^{th}$ column of X is the example corresponding to the $i^{th}$ label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches. img Partition: Partition the shuffled (X, Y) into mini-batches of size mini_batch_size (here 64). Note that the number of training examples is not always divisible by mini_batch_size. The last mini batch might be smaller, but you don’t need to worry about this. When the final mini-batch is smaller than the full mini_batch_size, it will look like this: img Exercise: Implement random_mini_batches. We coded the shuffling part for you. To help you with the partitioning step, we give you the following code that selects the indexes for the $1^{st}$ and $2^{nd}$ mini-batches:123first_mini_batch_X = shuffled_X[:, 0 : mini_batch_size]second_mini_batch_X = shuffled_X[:, mini_batch_size : 2 * mini_batch_size]... Note that the last mini-batch might end up smaller than mini_batch_size=64. Let $\lfloor s \rfloor$ represents $s$ rounded down to the nearest integer (this is math.floor(s) in Python). If the total number of examples is not a multiple of mini_batch_size=64 then there will be $\lfloor \frac{m}{mini_batch_size}\rfloor$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be ($m-mini__batch__size \times \lfloor \frac{m}{mini_batch_size}\rfloor$). 1234567891011121314151617181920212223242526272829303132333435363738394041424344# GRADED FUNCTION: random_mini_batchesdef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0): """ Creates a list of random minibatches from (X, Y) Arguments: X -- input data, of shape (input size, number of examples) Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples) mini_batch_size -- size of the mini-batches, integer Returns: mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y) """ np.random.seed(seed) # To make your "random" minibatches the same as ours m = X.shape[1] # number of training examples mini_batches = [] # Step 1: Shuffle (X, Y) permutation = list(np.random.permutation(m)) shuffled_X = X[:, permutation] shuffled_Y = Y[:, permutation].reshape((1,m)) # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case. num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning for k in range(0, num_complete_minibatches): ### START CODE HERE ### (approx. 2 lines) mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size] mini_batch_Y = shuffled_Y[k * mini_batch_size : (k+1) * mini_batch_size] ### END CODE HERE ### mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) # Handling the end case (last mini-batch &lt; mini_batch_size) if m % mini_batch_size != 0: ### START CODE HERE ### (approx. 2 lines) mini_batch_X = shuffled_X[:, (k+1) * mini_batch_size : m] mini_batch_Y = shuffled_Y[(k+1) * mini_batch_size : m] ### END CODE HERE ### mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) return mini_batches 12345678910X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)print ("shape of the 1st mini_batch_X: " + str(mini_batches[0][0].shape))print ("shape of the 2nd mini_batch_X: " + str(mini_batches[1][0].shape))print ("shape of the 3rd mini_batch_X: " + str(mini_batches[2][0].shape))print ("shape of the 1st mini_batch_Y: " + str(mini_batches[0][1].shape))print ("shape of the 2nd mini_batch_Y: " + str(mini_batches[1][1].shape)) print ("shape of the 3rd mini_batch_Y: " + str(mini_batches[2][1].shape))print ("mini batch sanity check: " + str(mini_batches[0][0][0][0:3])) Expected Output: shape of the 1st mini_batch_X (12288, 64) shape of the 2nd mini_batch_X (12288, 64) shape of the 3rd mini_batch_X (12288, 20) shape of the 1st mini_batch_Y (1, 64) shape of the 2nd mini_batch_Y (1, 64) shape of the 3rd mini_batch_Y (1, 20) mini batch sanity check [ 0.90085595 -0.7612069 0.2344157 ] What you should remember: Shuffling and Partitioning are the two steps required to build mini-batches Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128. 3 - MomentumBecause mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations. Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill. img Exercise: Initialize the velocity. The velocity, $v$, is a python dictionary that needs to be initialized with arrays of zeros. Its keys are the same as those in the grads dictionary, that is:for $l =1,…,L$:12v["dW" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters["W" + str(l+1)])v["db" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters["b" + str(l+1)]) Note that the iterator l starts at 0 in the for loop while the first parameters are v[“dW1”] and v[“db1”] (that’s a “one” on the superscript). This is why we are shifting l to l+1 in the for loop. 1234567891011121314151617181920212223242526272829# GRADED FUNCTION: initialize_velocitydef initialize_velocity(parameters): """ Initializes the velocity as a python dictionary with: - keys: "dW1", "db1", ..., "dWL", "dbL" - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters. Arguments: parameters -- python dictionary containing your parameters. parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl Returns: v -- python dictionary containing the current velocity. v['dW' + str(l)] = velocity of dWl v['db' + str(l)] = velocity of dbl """ L = len(parameters) // 2 # number of layers in the neural networks v = &#123;&#125; # Initialize velocity for l in range(L): ### START CODE HERE ### (approx. 2 lines) v["dW" + str(l+1)] = np.zeros(parameters["W" + str(l+1)].shape) v["db" + str(l+1)] = np.zeros(parameters["b" + str(l+1)].shape) ### END CODE HERE ### return v 1234567parameters = initialize_velocity_test_case()v = initialize_velocity(parameters)print("v[\"dW1\"] = " + str(v["dW1"]))print("v[\"db1\"] = " + str(v["db1"]))print("v[\"dW2\"] = " + str(v["dW2"]))print("v[\"db2\"] = " + str(v["db2"])) Expected Output: v[“dW1”] [[ 0. 0. 0.] [ 0. 0. 0.]] v[“db1”] [[ 0.] [ 0.]] v[“dW2”] [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] v[“db2”] [[ 0.] [ 0.] [ 0.]] Exercise: Now, implement the parameters update with momentum. The momentum update rule is, for $l = 1, …, L$: $$ \begin{cases}v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}}\end{cases}\tag{3}$$ $$\begin{cases}v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}}\end{cases}\tag{4}$$ where L is the number of layers, $\beta$ is the momentum and $\alpha$ is the learning rate. All parameters should be stored in the parameters dictionary. Note that the iterator l starts at 0 in the for loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that’s a “one” on the superscript). So you will need to shift l to l+1 when coding. 123456789101112131415161718192021222324252627282930313233343536373839# GRADED FUNCTION: update_parameters_with_momentumdef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate): """ Update parameters using Momentum Arguments: parameters -- python dictionary containing your parameters: parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads -- python dictionary containing your gradients for each parameters: grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl v -- python dictionary containing the current velocity: v['dW' + str(l)] = ... v['db' + str(l)] = ... beta -- the momentum hyperparameter, scalar learning_rate -- the learning rate, scalar Returns: parameters -- python dictionary containing your updated parameters v -- python dictionary containing your updated velocities """ L = len(parameters) // 2 # number of layers in the neural networks # Momentum update for each parameter for l in range(L): ### START CODE HERE ### (approx. 4 lines) # compute velocities v["dW" + str(l+1)] = beta * v["dW" + str(l+1)] + (1 - beta) * grads["dW" + str(l+1)] v["db" + str(l+1)] = beta * v["db" + str(l+1)] + (1 - beta) * grads["db" + str(l+1)] # update parameters parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * v["dW" + str(l+1)] parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * v["db" + str(l+1)] ### END CODE HERE ### return parameters, v 1234567891011parameters, grads, v = update_parameters_with_momentum_test_case()parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"]))print("v[\"dW1\"] = " + str(v["dW1"]))print("v[\"db1\"] = " + str(v["db1"]))print("v[\"dW2\"] = " + str(v["dW2"]))print("v[\"db2\"] = " + str(v["db2"])) Expected Output: W1 [[ 1.62544598 -0.61290114 -0.52907334] [-1.07347112 0.86450677 -2.30085497]] b1 [[ 1.74493465] [-0.76027113]] W2 [[ 0.31930698 -0.24990073 1.4627996 ] [-2.05974396 -0.32173003 -0.38320915] [ 1.13444069 -1.0998786 -0.1713109 ]] b2 [[-0.87809283] [ 0.04055394] [ 0.58207317]] v[“dW1”] [[-0.11006192 0.11447237 0.09015907] [ 0.05024943 0.09008559 -0.06837279]] v[“db1”] [[-0.01228902] [-0.09357694]] v[“dW2”] [[-0.02678881 0.05303555 -0.06916608] [-0.03967535 -0.06871727 -0.08452056] [-0.06712461 -0.00126646 -0.11173103]] v[“db2”] [[ 0.02344157] [ 0.16598022] [ 0.07420442]] Note that: The velocity is initialized with zeros. So the algorithm will take a few iterations to “build up” velocity and start to take bigger steps. If $\beta = 0$, then this just becomes standard gradient descent without momentum. How do you choose $\beta$? The larger the momentum $\beta$ is, the smoother the update because the more we take the past gradients into account. But if $\beta$ is too big, it could also smooth out the updates too much. Common values for $\beta$ range from 0.8 to 0.999. If you don’t feel inclined to tune this, $\beta = 0.9$ is often a reasonable default. Tuning the optimal $\beta$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$. What you should remember: Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent. You have to tune a momentum hyperparameter $\beta$ and a learning rate $\alpha$. 4 - AdamAdam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. How does Adam work? It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). It updates parameters in a direction based on combining information from “1” and “2”. The update rule is, for $l = 1, …, L$: $$\begin{cases}v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \v^{corrected}{dW^{[l]}} = \frac{v{dW^{[l]}}}{1 - (\beta_1)^t} \s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \s^{corrected}{dW^{[l]}} = \frac{s{dW^{[l]}}}{1 - (\beta_1)^t} \W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}{dW^{[l]}}}{\sqrt{s^{corrected}{dW^{[l]}}} + \varepsilon}\end{cases}$$where: t counts the number of steps taken of Adam L is the number of layers $\beta_1$ and $\beta_2$ are hyperparameters that control the two exponentially weighted averages. $\alpha$ is the learning rate $\varepsilon$ is a very small number to avoid dividing by zero As usual, we will store all parameters in the parameters dictionary Exercise: Initialize the Adam variables $v, s$ which keep track of the past information. Instruction: The variables $v, s$ are python dictionaries that need to be initialized with arrays of zeros. Their keys are the same as for grads, that is:for $l = 1, …, L$:1234v["dW" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters["W" + str(l+1)])v["db" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters["b" + str(l+1)])s["dW" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters["W" + str(l+1)])s["db" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters["b" + str(l+1)]) 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: initialize_adamdef initialize_adam(parameters) : """ Initializes v and s as two python dictionaries with: - keys: "dW1", "db1", ..., "dWL", "dbL" - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters. Arguments: parameters -- python dictionary containing your parameters. parameters["W" + str(l)] = Wl parameters["b" + str(l)] = bl Returns: v -- python dictionary that will contain the exponentially weighted average of the gradient. v["dW" + str(l)] = ... v["db" + str(l)] = ... s -- python dictionary that will contain the exponentially weighted average of the squared gradient. s["dW" + str(l)] = ... s["db" + str(l)] = ... """ L = len(parameters) // 2 # number of layers in the neural networks v = &#123;&#125; s = &#123;&#125; # Initialize v, s. Input: "parameters". Outputs: "v, s". for l in range(L): ### START CODE HERE ### (approx. 4 lines) v["dW" + str(l+1)] = np.zeros((parameters["W" +str(l+1)].shape)) v["db" + str(l+1)] = np.zeros((parameters["b" +str(l+1)].shape)) s["dW" + str(l+1)] = np.zeros((parameters["W" +str(l+1)].shape)) s["db" + str(l+1)] = np.zeros((parameters["b" +str(l+1)].shape)) ### END CODE HERE ### return v, s 1234567891011parameters = initialize_adam_test_case()v, s = initialize_adam(parameters)print("v[\"dW1\"] = " + str(v["dW1"]))print("v[\"db1\"] = " + str(v["db1"]))print("v[\"dW2\"] = " + str(v["dW2"]))print("v[\"db2\"] = " + str(v["db2"]))print("s[\"dW1\"] = " + str(s["dW1"]))print("s[\"db1\"] = " + str(s["db1"]))print("s[\"dW2\"] = " + str(s["dW2"]))print("s[\"db2\"] = " + str(s["db2"])) Expected Output: v[“dW1”] [[ 0. 0. 0.] [ 0. 0. 0.]] v[“db1”] [[ 0.] [ 0.]] v[“dW2”] [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] v[“db2”] [[ 0.] [ 0.] [ 0.]] s[“dW1”] [[ 0. 0. 0.] [ 0. 0. 0.]] s[“db1”] [[ 0.] [ 0.]] s[“dW2”] [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] s[“db2”] [[ 0.] [ 0.] [ 0.]] Exercise: Now, implement the parameters update with Adam. Recall the general update rule is, for $l = 1, …, L$: $$\begin{cases}v_{W^{[l]}} = \beta_1 v_{W^{[l]}} + (1 - \beta_1) \frac{\partial J }{ \partial W^{[l]} } \v^{corrected}{W^{[l]}} = \frac{v{W^{[l]}}}{1 - (\beta_1)^t} \s_{W^{[l]}} = \beta_2 s_{W^{[l]}} + (1 - \beta_2) (\frac{\partial J }{\partial W^{[l]} })^2 \s^{corrected}{W^{[l]}} = \frac{s{W^{[l]}}}{1 - (\beta_2)^t} \W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}{W^{[l]}}}{\sqrt{s^{corrected}{W^{[l]}}}+\varepsilon}\end{cases}$$ Note that the iterator l starts at 0 in the for loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift l to l+1 when coding. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# GRADED FUNCTION: update_parameters_with_adamdef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8): """ Update parameters using Adam Arguments: parameters -- python dictionary containing your parameters: parameters['W' + str(l)] = Wl parameters['b' + str(l)] = bl grads -- python dictionary containing your gradients for each parameters: grads['dW' + str(l)] = dWl grads['db' + str(l)] = dbl v -- Adam variable, moving average of the first gradient, python dictionary s -- Adam variable, moving average of the squared gradient, python dictionary learning_rate -- the learning rate, scalar. beta1 -- Exponential decay hyperparameter for the first moment estimates beta2 -- Exponential decay hyperparameter for the second moment estimates epsilon -- hyperparameter preventing division by zero in Adam updates Returns: parameters -- python dictionary containing your updated parameters v -- Adam variable, moving average of the first gradient, python dictionary s -- Adam variable, moving average of the squared gradient, python dictionary """ L = len(parameters) // 2 # number of layers in the neural networks v_corrected = &#123;&#125; # Initializing first moment estimate, python dictionary s_corrected = &#123;&#125; # Initializing second moment estimate, python dictionary # Perform Adam update on all parameters for l in range(L): # Moving average of the gradients. Inputs: "v, grads, beta1". Output: "v". ### START CODE HERE ### (approx. 2 lines) v["dW" + str(l+1)] = beta1 * v["dW" + str(l+1)] + (1 - beta1) * grads["dW" + str(l+1)] v["db" + str(l+1)] = beta1 * v["db" + str(l+1)] + (1 - beta1) * grads["db" + str(l+1)] ### END CODE HERE ### # Compute bias-corrected first moment estimate. Inputs: "v, beta1, t". Output: "v_corrected". ### START CODE HERE ### (approx. 2 lines) v_corrected["dW" + str(l+1)] = v["dW" + str(l+1)] / (1 - np.power(beta1, t)) v_corrected["db" + str(l+1)] = v["db" + str(l+1)] / (1 - np.power(beta1, t)) ### END CODE HERE ### # Moving average of the squared gradients. Inputs: "s, grads, beta2". Output: "s". ### START CODE HERE ### (approx. 2 lines) s["dW" + str(l+1)] = beta2 * s["dW" + str(l+1)] + (1 - beta2) * np.power(grads["dW" + str(l+1)], 2) s["db" + str(l+1)] = beta2 * s["db" + str(l+1)] + (1 - beta2) * np.power(grads["db" + str(l+1)], 2) ### END CODE HERE ### # Compute bias-corrected second raw moment estimate. Inputs: "s, beta2, t". Output: "s_corrected". ### START CODE HERE ### (approx. 2 lines) s_corrected["dW" + str(l+1)] = s["dW" + str(l+1)] / (1 - np.power(beta2, t)) s_corrected["db" + str(l+1)] = s["db" + str(l+1)] / (1 - np.power(beta2, t)) ### END CODE HERE ### # Update parameters. Inputs: "parameters, learning_rate, v_corrected, s_corrected, epsilon". Output: "parameters". ### START CODE HERE ### (approx. 2 lines) parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * v_corrected["dW" + str(l+1)] / (np.sqrt(s_corrected["dW" + str(l+1)]) + epsilon) parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * v_corrected["db" + str(l+1)] / (np.sqrt(s_corrected["db" + str(l+1)]) + epsilon) ### END CODE HERE ### return parameters, v, s 123456789101112131415parameters, grads, v, s = update_parameters_with_adam_test_case()parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t = 2)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"]))print("v[\"dW1\"] = " + str(v["dW1"]))print("v[\"db1\"] = " + str(v["db1"]))print("v[\"dW2\"] = " + str(v["dW2"]))print("v[\"db2\"] = " + str(v["db2"]))print("s[\"dW1\"] = " + str(s["dW1"]))print("s[\"db1\"] = " + str(s["db1"]))print("s[\"dW2\"] = " + str(s["dW2"]))print("s[\"db2\"] = " + str(s["db2"])) Expected Output: W1 [[ 1.63178673 -0.61919778 -0.53561312] [-1.08040999 0.85796626 -2.29409733]] b1 [[ 1.75225313] [-0.75376553]] W2 [[ 0.32648046 -0.25681174 1.46954931] [-2.05269934 -0.31497584 -0.37661299] [ 1.14121081 -1.09245036 -0.16498684]] b2 [[-0.88529978] [ 0.03477238] [ 0.57537385]] v[“dW1”] [[-0.11006192 0.11447237 0.09015907] [ 0.05024943 0.09008559 -0.06837279]] v[“db1”] [[-0.01228902] [-0.09357694]] v[“dW2”] [[-0.02678881 0.05303555 -0.06916608] [-0.03967535 -0.06871727 -0.08452056] [-0.06712461 -0.00126646 -0.11173103]] v[“db2”] [[ 0.02344157] [ 0.16598022] [ 0.07420442]] s[“dW1”] [[ 0.00121136 0.00131039 0.00081287] [ 0.0002525 0.00081154 0.00046748]] s[“db1”] [[ 1.51020075e-05] [ 8.75664434e-04]] s[“dW2”] [[ 7.17640232e-05 2.81276921e-04 4.78394595e-04] [ 1.57413361e-04 4.72206320e-04 7.14372576e-04] [ 4.50571368e-04 1.60392066e-07 1.24838242e-03]] s[“db2”] [[ 5.49507194e-05] [ 2.75494327e-03] [ 5.50629536e-04]] You now have three working optimization algorithms (mini-batch gradient descent, Momentum, Adam). Let’s implement a model with each of these optimizers and observe the difference. 5 - Model with different optimization algorithmsLets use the following “moons” dataset to test the different optimization methods. (The dataset is named “moons” because the data from each of the two classes looks a bit like a crescent-shaped moon.) 1train_X, train_Y = load_dataset() We have already implemented a 3-layer neural network. You will train it with: Mini-batch Gradient Descent: it will call your function: update_parameters_with_gd() Mini-batch Momentum: it will call your functions: initialize_velocity() and update_parameters_with_momentum() Mini-batch Adam: it will call your functions: initialize_adam() and update_parameters_with_adam() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, num_epochs = 10000, print_cost = True): """ 3-layer neural network model which can be run in different optimizer modes. Arguments: X -- input data, of shape (2, number of examples) Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples) layers_dims -- python list, containing the size of each layer learning_rate -- the learning rate, scalar. mini_batch_size -- the size of a mini batch beta -- Momentum hyperparameter beta1 -- Exponential decay hyperparameter for the past gradients estimates beta2 -- Exponential decay hyperparameter for the past squared gradients estimates epsilon -- hyperparameter preventing division by zero in Adam updates num_epochs -- number of epochs print_cost -- True to print the cost every 1000 epochs Returns: parameters -- python dictionary containing your updated parameters """ L = len(layers_dims) # number of layers in the neural networks costs = [] # to keep track of the cost t = 0 # initializing the counter required for Adam update seed = 10 # For grading purposes, so that your "random" minibatches are the same as ours # Initialize parameters parameters = initialize_parameters(layers_dims) # Initialize the optimizer if optimizer == "gd": pass # no initialization required for gradient descent elif optimizer == "momentum": v = initialize_velocity(parameters) elif optimizer == "adam": v, s = initialize_adam(parameters) # Optimization loop for i in range(num_epochs): # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch seed = seed + 1 minibatches = random_mini_batches(X, Y, mini_batch_size, seed) for minibatch in minibatches: # Select a minibatch (minibatch_X, minibatch_Y) = minibatch # Forward propagation a3, caches = forward_propagation(minibatch_X, parameters) # Compute cost cost = compute_cost(a3, minibatch_Y) # Backward propagation grads = backward_propagation(minibatch_X, minibatch_Y, caches) # Update parameters if optimizer == "gd": parameters = update_parameters_with_gd(parameters, grads, learning_rate) elif optimizer == "momentum": parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate) elif optimizer == "adam": t = t + 1 # Adam counter parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon) # Print the cost every 1000 epoch if print_cost and i % 1000 == 0: print ("Cost after epoch %i: %f" %(i, cost)) if print_cost and i % 100 == 0: costs.append(cost) # plot the cost plt.plot(costs) plt.ylabel('cost') plt.xlabel('epochs (per 100)') plt.title("Learning rate = " + str(learning_rate)) plt.show() return parameters You will now run this 3 layer neural network with each of the 3 optimization methods. 5.1 - Mini-batch Gradient descentRun the following code to see how the model does with mini-batch gradient descent. 12345678910111213# train 3-layer modellayers_dims = [train_X.shape[0], 5, 2, 1]parameters = model(train_X, train_Y, layers_dims, optimizer = "gd")# Predictpredictions = predict(train_X, train_Y, parameters)# Plot decision boundaryplt.title("Model with Gradient Descent optimization")axes = plt.gca()axes.set_xlim([-1.5,2.5])axes.set_ylim([-1,1.5])plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y) 5.2 - Mini-batch gradient descent with momentumRun the following code to see how the model does with momentum. Because this example is relatively simple, the gains from using momemtum are small; but for more complex problems you might see bigger gains. 12345678910111213# train 3-layer modellayers_dims = [train_X.shape[0], 5, 2, 1]parameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = "momentum")# Predictpredictions = predict(train_X, train_Y, parameters)# Plot decision boundaryplt.title("Model with Momentum optimization")axes = plt.gca()axes.set_xlim([-1.5,2.5])axes.set_ylim([-1,1.5])plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y) 5.3 - Mini-batch with Adam modeRun the following code to see how the model does with Adam.12345678910111213# train 3-layer modellayers_dims = [train_X.shape[0], 5, 2, 1]parameters = model(train_X, train_Y, layers_dims, optimizer = "adam")# Predictpredictions = predict(train_X, train_Y, parameters)# Plot decision boundaryplt.title("Model with Adam optimization")axes = plt.gca()axes.set_xlim([-1.5,2.5])axes.set_ylim([-1,1.5])plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y) 5.4 - Summary optimization method accuracy cost shape Gradient descent 79.7% oscillations Momentum 79.7% oscillations Adam 94% smoother Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm. Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster. Some advantages of Adam include: Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) Usually works well even with little tuning of hyperparameters (except $\alpha$) References: Adam paper: https://arxiv.org/pdf/1412.6980.pdf]]></content>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Blog Test]]></title>
    <url>%2F2018%2F08%2F07%2F2018-08-07-Blog-Test%2F</url>
    <content type="text"><![CDATA[Blog test Blog testBlog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test Blog test 本文是“Blog Test” ;) 尽请期待：]]></content>
      <tags>
        <tag>tags</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Liulishuo English Reading Day5]]></title>
    <url>%2F2018%2F08%2F07%2F2018-08-07-liulishuo-5%2F</url>
    <content type="text"><![CDATA[Trump blasts back after Times publisher decries ‘enemy of the people’ attacks blast v. 猛烈抨击 decry v. to say publicly and forcefully that you regard (something) as bad, wrong, etc. TImes: New York Times ‘enemy of the people’ 人民公敌 Hours after saying he had a “very good meeting” with the publisher of the New York Times about his labelling the press the “enemy of the people”, Donald Trump launched a blistering attack on “anti-Trump haters in the dying newspaper industry”. about 后面具体讲述meeting的内容 label v. 给…贴标签 label sb/sth (as) sth eg. label the baby naughty blistering adj. 无情的，严酷的: very harsh or powerful, extremely intense or severe blister n. 水泡 特朗普声称自己与纽约时报的发行人进行了一次“很好的会面”，讨论了自己将媒体称为“人民的敌人”的行为。然而几个小时之后，特朗普发起了猛烈的攻击，矛头直指那些“垂死的报纸行业中反对特朗普的仇恨者们”。 “The failing New York Times and the Amazon Washington Post do nothing but write bad stories even on very positive achievements,” Trump tweeted. “And they will never change!” Earlier, the White House confirmed Trump met Times’ publisher AG Sulzberger on 20 July. Sulzberger then issued a statement that opened an exchange over whose activities were more damaging to America. do nothing but 除了…什么都不做 eg. We do nothing but fight. issue a statement exchange 原来表示“交换”，但在作名词的时候，可以指“语言方面的交换”，也就是“对话、争论” eg. a heated debate, argument exchange （升级词汇！） over: exchange争论的是什么呢？ 特朗普发推称：“失败的纽约时报以及受亚马逊控制的华盛顿邮报，除了撰写负面新闻外什么都不做，就连一些积极的成就也能被写成负面新闻。并且他们永远不会改变！”早些时候，白宫方面确认，特朗普已于 7 月 20 日会见了纽约时报的发行人 A·G·苏兹伯格。苏兹伯格随后发表了一篇声明，由此开始了“特朗普与新闻界，究竟哪一方的行为给美国带来更多损害”的争论。 Sulzberger said he had raised “concerns about the president’s deeply troubling anti-press rhetoric” and “implored him to reconsider his broader attacks on journalism, which I believe are dangerous and harmful to our country”. rhetoric n. 修辞，修辞学，接在人后面指“煽动别人的花言巧语” implore v. 恳求 implore sb to do sth 苏兹伯格称他表达了“对总统令人深感不安的反媒体言辞的担忧”，并“请求他重新审视他对新闻界的广泛攻击，这一攻击在我看来对国家是危险而有害的。” He did not say how the president responded. Hours later, Trump obliged. In a multi-tweet rant, he claimed reports on “internal deliberations of our government … truly put the lives of many, not just journalists, at risk”. Claiming without evidence that “90% of media coverage of my administration is negative”, Trump said he would “not allow our great country to be sold out by anti-Trump haters in the dying newspaper industry”. oblige v. : do as sb asks in order to help or please them rant v. shout in a wild way deliberation n. 正式的讨论，商议: careful thought or discussion done in order to make a decision 他没有陈述总统在会谈中如何回应。不过几个小时后，特朗普帮他补充了这一缺失的信息。在连续多条推特的痛斥中，他声称那些关于“政府内部协商”的报道，“不仅仅是在拿记者们的生命冒险，更是真正地将许多人的生命置于危险之中。”在没有给出任何事实依据的情况下，他声称：“关于我的政府的媒体报道中，90%是负面新闻”。特朗普说，他“不会允许我们的伟大国家被垂死的报纸行业中那些反对特朗普的仇恨者们出卖”。 Sulzberger, 37, took over from his father on 1 January. In his statement, he said: “I told the president directly that I thought that his language was not just divisive but increasingly dangerous. I told him that although the phrase ‘fake news’ is untrue and harmful, I am far more concerned about his labelling journalists ‘the enemy of the people’.” take over from 接任 divisive adj. 引起分裂的，造成各方敌意的 eg. Unemployment is socially divisive. 无业、待业会引发社会分裂。 far more concerned 37 岁的苏兹伯格今年一月一日接替其父亲就任纽约时报发行人。在他的声明中，他表示：“我直截了当地告诉总统，我认为他的言论不仅是在引起敌意，而且愈发地危险。我告诉他，尽管‘假新闻’这一说法不实且有害，但是我更担心的是他给记者们冠上的‘人民的敌人’这样的标签。”]]></content>
      <tags>
        <tag>English Reading</tag>
      </tags>
  </entry>
</search>
