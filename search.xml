<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python学习-将dict写入txt文件]]></title>
    <url>%2F2018%2F10%2F28%2F2018-10-28-Python%E5%AD%A6%E4%B9%A0-%E5%B0%86dict%E5%86%99%E5%85%A5txt%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Python学习-将dict写入txt文件1a = ['Jin Wang', 'Jiawei Xu', 'Peiyang Shi', 'Zheqi Zhang'] 12for line in a: print(line) Jin Wang Jiawei Xu Peiyang Shi Zheqi Zhang 1234i = 1for line in a: print(str(i) + ',' + line) i = i + 1 1,Jin Wang 2,Jiawei Xu 3,Peiyang Shi 4,Zheqi Zhang]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习 - 写json爬虫]]></title>
    <url>%2F2018%2F10%2F28%2F2018-10-28-Python%E5%AD%A6%E4%B9%A0-%E5%86%99json%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[Python学习 - 写json爬虫@(python) Talk less, code more 这个程序利用百度地图API发送HTTP Get Request请求，将我们的GPS数据转化成坐标对应的json文件，里面包含了这个gps的行政级别的地区，即“国家”，“省份”，“城市名”等，输出为字典形式。 下面是浏览器输出的结果。 我们请求首先构造一个url，这是一个json文件的url。但是有一点需要注意，我们程序将json文件转化为dict形式的时候，由于百度地图API会自动在数据周围添加上一个括号，所以我们先要去掉这个括号，用re来完成，具体代码如下。 1234567891011121314151617181920212223242526272829# 此函数构造一个HTTP request 请求，请求对象为一个json格式的文件，并将json格式的文件转化为dict形式import requestsimport jsonimport redef req_json_to_dict(url): # 输入为一个"xxx/xxx/xxx.json"这样的url地址 # 构造HTTP GET Request，返回的网页内容存储到r中 r = requests.get(url) json_response = r.content.decode() # 删掉原始json数据两边的括号 p1 = re.compile(r'[(](.*?)[)]', re.S) pure_info = re.findall(p1, json_response)[0] # 将json字符串转换成dic字典对象 dict_json = json.loads(pure_info) return dict_jsonif __name__ == "__main__": url = "http://api.map.baidu.com/geocoder/v2/?callback=renderReverse&amp;location=30.882428000000004,75.832577&amp;output=json&amp;pois=1&amp;ak=vtZ2V5pp9VcMfvvpMf55SaDoCzLLYvGA" dict_json = req_json_to_dict(url) 我们运行一下程序，并查看每一步的变量的类型和值。 12# 首先，我们发送一个HTTP GET请求r = requests.get(url) 下面显示print(type(r))和print(r)的结果，以下同上：12&lt;class 'requests.models.Response'&gt;&lt;Response [200]&gt; 可以看出，r是一个HTTP响应，里面已经包含了我们需要的json信息。1234567print(type(r.content))print(r.content)# 输出&lt;class 'bytes'&gt;b'renderReverse&amp;&amp;renderReverse(&#123;"status":0,"result":&#123;"location":&#123;"lng":75.83257699999999,"lat":30.882427846565635&#125;,"formatted_address":"Ludhiana Bypass Road, Ludhiana, Punjab, India","business":"","addressComponent":&#123;"country":"India","country_code":31080,"country_code_iso":"IND","country_code_iso2":"IN","province":"Punjab","city":"Ludhiana","city_level":2,"district":"","town":"","adcode":"0","street":"Ludhiana Bypass Road","street_number":"","direction":"","distance":""&#125;,"pois":[],"roads":[],"poiRegions":[],"sematic_description":"","cityCode":32234&#125;&#125;)' 我们可以看到，这是一个二进制码，我们需要转成字符串，我们在一个bytes类型的数据后边使用deocde()函数，就可以将其转化为str类型12345678print(type(r.content.decode()))print(r.content.decode())# 输出&lt;class 'str'&gt;renderReverse&amp;&amp;renderReverse(&#123;"status":0,"result":&#123;"location":&#123;"lng":75.83257699999999,"lat":30.882427846565635&#125;,"formatted_address":"Ludhiana Bypass Road, Ludhiana, Punjab, India","business":"","addressComponent":&#123;"country":"India","country_code":31080,"country_code_iso":"IND","country_code_iso2":"IN","province":"Punjab","city":"Ludhiana","city_level":2,"district":"","town":"","adcode":"0","street":"Ludhiana Bypass Road","street_number":"","direction":"","distance":""&#125;,"pois":[],"roads":[],"poiRegions":[],"sematic_description":"","cityCode":32234&#125;&#125;) 可以看到，这就是一个str类型了。我们发现这个json文件里面有一个字符串把整个json信息包起来了，我们使用re将其去掉。 123 # 删掉原始json数据两边的括号p1 = re.compile(r'[(](.*?)[)]', re.S)pure_info = re.findall(p1, json_response)[0] 123456print(type(pure_info))print(pure_info)# 输出&lt;class 'str'&gt;&#123;"status":0,"result":&#123;"location":&#123;"lng":75.83257699999999,"lat":30.882427846565635&#125;,"formatted_address":"Ludhiana Bypass Road, Ludhiana, Punjab, India","business":"","addressComponent":&#123;"country":"India","country_code":31080,"country_code_iso":"IND","country_code_iso2":"IN","province":"Punjab","city":"Ludhiana","city_level":2,"district":"","town":"","adcode":"0","street":"Ludhiana Bypass Road","street_number":"","direction":"","distance":""&#125;,"pois":[],"roads":[],"poiRegions":[],"sematic_description":"","cityCode":32234&#125;&#125; 输出依然为字符串类型，但是删掉了两边的括号。然后我们将其转化为字典类型。 12# 将json字符串转换成dic字典对象dict_json = json.loads(pure_info) 123456789print(type(dict_json))print("###############################")print(dict_json)# 输出&lt;class 'dict'&gt;&#123;'status': 0, 'result': &#123;'location': &#123;'lng': 75.83257699999999, 'lat': 30.882427846565633&#125;, 'formatted_address': 'Ludhiana Bypass Road, Ludhiana, Punjab, India', 'business': '', 'addressComponent': &#123;'country': 'India', 'country_code': 31080, 'country_code_iso': 'IND', 'country_code_iso2': 'IN', 'province': 'Punjab', 'city': 'Ludhiana', 'city_level': 2, 'district': '', 'town': '', 'adcode': '0', 'street': 'Ludhiana Bypass Road', 'street_number': '', 'direction': '', 'distance': ''&#125;, 'pois': [], 'roads': [], 'poiRegions': [], 'sematic_description': '', 'cityCode': 32234&#125;&#125; 可以看出，我们已经将其转化为字典类型了，之后输出为字典类型的数据，便于后续对数据进行提取。 print版代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 此函数构造一个HTTP request 请求，请求对象为一个json格式的文件，并将json格式的文件转化为dict形式import requestsimport jsonimport redef req_json_to_dict(url): # url = "http://api.map.baidu.com/geocoder/v2/?callback=renderReverse&amp;location=30.882428000000004,75.832577&amp;output=json&amp;pois=1&amp;ak=vtZ2V5pp9VcMfvvpMf55SaDoCzLLYvGA" # 构造HTTP GET Request，返回的网页内容存储到r中 r = requests.get(url) print("###############################") print(type(r)) print("###############################") print(r) json_response = r.content.decode() print("###############################") print(type(r.content)) print("###############################") print(r.content) print("###############################") print(type(r.content.decode())) print("###############################") print(r.content.decode()) print("###############################") print(type(json_response)) print("###############################") print(json_response) # 删掉原始json数据两边的括号 p1 = re.compile(r'[(](.*?)[)]', re.S) pure_info = re.findall(p1, json_response)[0] print(type(pure_info)) print(pure_info) # 将json字符串转换成dic字典对象 dict_json = json.loads(pure_info) print("###############################") print(type(dict_json)) print("###############################") print(dict_json) return dict_jsonif __name__ == "__main__": url = "http://api.map.baidu.com/geocoder/v2/?callback=renderReverse&amp;location=30.882428000000004,75.832577&amp;output=json&amp;pois=1&amp;ak=vtZ2V5pp9VcMfvvpMf55SaDoCzLLYvGA" dict_json = req_json_to_dict(url) print(type(dict_json)) print(dict_json)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Json</tag>
        <tag>爬虫</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习-json.dumps和json.loads的用法]]></title>
    <url>%2F2018%2F10%2F28%2F2018-10-28-Python%E5%AD%A6%E4%B9%A0-json.dumps%E5%92%8Cjson.loads%E7%9A%84%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Python学习-json.dumps和json.loads的用法@(python) Talk less, code more 1import json 1a = &#123;'name': 'wang', 'age': 29&#125; 1print(type(a), '\n', a) &lt;class &apos;dict&apos;&gt; {&apos;name&apos;: &apos;wang&apos;, &apos;age&apos;: 29} json.dumps()将dict转化为str 1b = json.dumps(a) 1print(type(b), '\n', b) &lt;class &apos;str&apos;&gt; {&quot;name&quot;: &quot;wang&quot;, &quot;age&quot;: 29} json.loads()将str转化为dict 1c = json.loads(b) 1print(type(c), '\n', c) &lt;class &apos;dict&apos;&gt; {&apos;name&apos;: &apos;wang&apos;, &apos;age&apos;: 29}]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习-Series()转成str操作，读取gps]]></title>
    <url>%2F2018%2F10%2F22%2F2018-10-22-Pandas%E5%AD%A6%E4%B9%A0-Series%E8%BD%AC%E6%88%90str%E6%93%8D%E4%BD%9C-%E8%AF%BB%E5%8F%96gps%2F</url>
    <content type="text"><![CDATA[Pandas学习-Series()转成str操作，读取gps@(我的第一个笔记本) Talk less, code more 在DataFrame中取一列转化成Series类型，对Series类型进行字符串分割，便可以得到坐标信息。1234567891011import pandas as pddata_file = 'matchResult_10.txt'names = 'app_name,app_kind,app_type,ip,nation,gps,size'names = names.split(',')data = pd.read_csv(data_file, header=None, names=names)data['gps_xy'] = data['gps'].str.split('#')print(data['gps_xy']) 输出12345678910110 [30.882428, 75.832577]1 [22.9009009009009, 84.14595791325802]2 [20.81081081081081, 70.70283823391588]3 [26.27027027027027, 72.99908113768355]4 [26.7927864, 84.5345949]5 [24.4353437, 85.866535]6 [26.4265576, 76.6643728]7 [26.7927864, 84.5345949]8 [20.46846846846847, 75.00577673385992]9 [25.7397946, 77.4759334]Name: gps_xy, dtype: object]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习-pd.read_csv()添加header和names]]></title>
    <url>%2F2018%2F10%2F21%2F2018-10-21-Pandas%E5%AD%A6%E4%B9%A0-pd.read_csv()%E6%B7%BB%E5%8A%A0header%E5%92%8Cnames%2F</url>
    <content type="text"><![CDATA[Pandas学习-pd.read_csv()添加header和names@(python) Talk less, code more 首先，我们构造一个数据文件csv_read_test.txt，数据之间用,分隔开 1234Xiao Wang, 18, boy, ip1Xiao Hong, 17, girl, ip2Liu Chen, 20, boy, ip3Hua hua, 21, girl, ip4 12345678import pandas as pd# 定义file_namefile_name = 'csv_read_test.txt'# 直接读取，除了文件名不添加任何参数data_raw = pd.read_csv(file_name)# 输出我们的数据print(data_raw) 12345# 输出 Xiao Wang 18 boy ip10 Xiao Hong 17 girl ip21 Liu Chen 20 boy ip32 Hua hua 21 girl ip4 可见，默认将数据的第一行作为了数据头 123456789import pandas as pdfile_name = 'csv_read_test.txt'names = 'name, age, gender, ip_address'names = names.split(',')data_raw = pd.read_csv(file_name, header=None, names=names)print(data_raw) header = None的意思是告诉电脑，第一行的数据不是header，否则就会向上面一样，随后我们自己添加一个names参数，指定数据的数据头。我们看看结果 12345 name age gender ip_address0 Xiao Wang 18 boy ip11 Xiao Hong 17 girl ip22 Liu Chen 20 boy ip33 Hua hua 21 girl ip4 我们可以尝试一下D2D的数据集123456789import pandas as pddata_file = 'matchResult_10.txt'names = 'app_name, app_kind, app_type, ip, nation, gps, size'names = names.split(',')data = pd.read_csv(data_file, header=None, names=names)print(data) 输出1234567891011121314151617181920212223 app_name app_kind app_type \0 Samsung Multiroom Lifestyle app 1 FacebookMessenger_v4.0.1.13.1_2024494 News &amp; Magazines app 2 Subway Surf Arcade app 3 Jugnoo Maps &amp; Navigation app 4 hike Social app 5 WhatsApp Communication app 6 My Idea Business app 7 Root Checker Basic Business app 8 Score! Hero Sports app 9 Hindu Calendar Books &amp; Reference app ip nation gps size 0 124.253.207.200 IN 30.882428#75.832577 29.7 MB 1 27.97.73.132 IN 22.9009009009009#84.14595791325802 9.0 MB 2 203.88.23.43 IN 20.81081081081081#70.70283823391588 40.9 MB 3 106.77.209.139 IN 26.27027027027027#72.99908113768355 11.8 MB 4 106.76.154.53 IN 26.7927864#84.5345949 10 MB 5 223.176.23.92 IN 24.4353437#85.866535 2.9 MB 6 106.67.144.166 IN 26.4265576#76.6643728 6.7 MB 7 106.76.154.53 IN 26.7927864#84.5345949 2.4 MB 8 106.77.19.199 IN 20.46846846846847#75.00577673385992 92.0 MB 9 106.221.139.35 IN 25.7397946#77.4759334 7.6 MB]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习-将逗号分隔的字符串转化为列表]]></title>
    <url>%2F2018%2F10%2F21%2F2018-10-21-Python%E5%AD%A6%E4%B9%A0-%E5%B0%86%E9%80%97%E5%8F%B7%E5%88%86%E9%9A%94%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E5%8C%96%E4%B8%BA%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[Python学习-将逗号分隔的字符串转化为列表@(python) Talk less, code more 用split()函数可以将用逗号分隔的str转化为list12345678columns = 'app_name, app_kind, app_type, md5, sender, receiver, timestamp, ip, nation, gps, size'print('type = ', type(columns))print(columns)# 使用split尽心分割columns = columns.split(',')print('type = ', type(columns))print(columns) 123456# 输出type = &lt;class 'str'&gt;app_name, app_kind, app_type, md5, sender, receiver, timestamp, ip, nation, gps, size# 转化后输出type = &lt;class 'list'&gt;['app_name', ' app_kind', ' app_type', ' md5', ' sender', ' receiver', ' timestamp', ' ip', ' nation', ' gps', ' size']]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习-DataFrame()按列索引]]></title>
    <url>%2F2018%2F10%2F21%2F2018-10-21-Pandas%E5%AD%A6%E4%B9%A0-DataFrame()%E6%8C%89%E5%88%97%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[Pandas学习-DataFrame()按列索引@(python) Talk less, code more 12345678910import pandas as pddata_file = 'matchResult_10.txt'names = 'app_name, app_kind, app_type, ip, nation, gps, size'names = names.split(',')data = pd.read_csv(data_file, header=None, names=names)# 查找' gps'这一列的数据print(data[' gps']) 注意！！！：这里要注意一下空格的问题。因为定义数据头names的时候习惯性的在每个类型前面加了个空格（也就是习惯性的在逗号后面加了个空格），所以真正的索引值是&#39; gps&#39;而不是&#39;gps&#39;所以我们将前面的空格去掉。这也告诉我们，数据文件不要随便添加空格。 输出12345678910110 30.882428#75.8325771 22.9009009009009#84.145957913258022 20.81081081081081#70.702838233915883 26.27027027027027#72.999081137683554 26.7927864#84.53459495 24.4353437#85.8665356 26.4265576#76.66437287 26.7927864#84.53459498 20.46846846846847#75.005776733859929 25.7397946#77.4759334Name: gps, dtype: object 更改后的结果12345678910import pandas as pddata_file = 'matchResult_10.txt'names = 'app_name,app_kind,app_type,ip,nation,gps,size'names = names.split(',')data = pd.read_csv(data_file, header=None, names=names)# 查找' gps'这一列的数据print(data['gps']) 12345678910110 30.882428#75.8325771 22.9009009009009#84.145957913258022 20.81081081081081#70.702838233915883 26.27027027027027#72.999081137683554 26.7927864#84.53459495 24.4353437#85.8665356 26.4265576#76.66437287 26.7927864#84.53459498 20.46846846846847#75.005776733859929 25.7397946#77.4759334Name: gps, dtype: object 那这个data[&#39;gps&#39;]是什么类型的数据呢1234print(type(data['gps']))# 输出&lt;class 'pandas.core.series.Series'&gt;]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Digital Image Processing - Ch3 Spatial Domain Image Enhancement]]></title>
    <url>%2F2018%2F09%2F27%2F2018-09-27-Digital_Image_Processing_Ch3_Spatial_Domain_Image_Enhancement%2F</url>
    <content type="text"><![CDATA[Image Negative123im_input = im2double(imread('./Pics/Ch03/rose.jpg'));im_output = 1-im_input;figure, imshow([im_input im_output]), title('Gray-level Invert'); Log Transformations（对数变换）1234im_input = imread('./Pics/Ch03/spectrum.tif');im_output = 3*log(double(im_input)+1);figure, imshow(im_input); title('input image');figure, imshow(abs(im_output),[]); title('image Log transform'); Gamma correction（伽马校正）12345im_input = im2double(imread('./Pics/Ch03/MRI.jpg'));im_output1 = imadjust(im_input,[0;1], [0;1], 1.2);im_output2 = imadjust(im_input,[0;1], [0;1], 0.8);figure, imshow(im_input); title('input image');figure, imshow([im_output1 im_output2]); title('Gamma correction'); Contrast stretching（对比度拉伸）1234im_input = im2double(imread('.\Pics\Ch03\pollen.tif'));im_output = imadjust(im_input,[min(im_input(:)) max(im_input(:))], [0 1]);figure, imshow(im_input); title('input image');figure, imshow(im_output); title('Contrast stretch'); Histogram （直方图）12345678im_input = im2double(imread('.\Pics\Ch03\pollen.tif'));im_output = imadjust(im_input,[min(im_input(:)) max(im_input(:))], [0 1]);figure, imshow(im_input); title('input image'); figure, imshow(im_output); title('Contrast stretch');histogram(im_input,64,'EdgeColor','r','FaceAlpha',1,'FaceColor','r');hold on;histogram(im_output,64,'EdgeColor','k','FaceAlpha',0.2,'FaceColor','y');hold off; Histogram equalization（直方图均衡）123456789im_input = im2double(imread('.\Pics\Ch03\pollen.tif'));im_output = imadjust(im_input,[min(im_input(:)) max(im_input(:))], [0 1]);im_histeq = histeq(im_input);figure, imshow(im_input); title('input image'); figure, imshow(im_output); title('Contrast stretch');figure, imshow(im_histeq); title('Histgram equalization');histogram(im_input,64,'EdgeColor','r','FaceAlpha',0.5,'FaceColor','r');histogram(im_output,64,'EdgeColor','g','FaceAlpha',0.5,'FaceColor','g');histogram(im_histeq,64,'EdgeColor','b','FaceAlpha',0.5,'FaceColor','b'); Histogram specification（直方图规定化）123456789im_input = im2double(imread('.\Pics\Ch03\pollen.tif'));im_ref = im2double(imread('.\Pics\Ch01\gray.jpg'));im_output = imhistmatch(im_input,im_ref,64);figure, imshow(im_input); title('input image'); histogram(im_input,64,'EdgeColor','r','FaceAlpha',0.5,'FaceColor','r');figure, imshow(im_ref); title('Contrast stretch');histogram(im_ref,64,'EdgeColor','g','FaceAlpha',0.5,'FaceColor','g');figure, imshow(im_output); title('Histgram equalization');histogram(im_output,64,'EdgeColor','b','FaceAlpha',0.5,'FaceColor','b'); Adaptive histogram equalization（自适应直方图均衡）12345678910111213im_input = im2double(imread('.\Pics\Ch03\planet.png'));figure, imshow(im_input); title('input image'); histogram(im_input,64,'EdgeColor','r','FaceAlpha',0.5,'FaceColor','r');im_output1 = histeq(im_input);figure, imshow(im_output1); title('Histogram equalization result'); histogram(im_output1,64,'EdgeColor','r','FaceAlpha',0.5,'FaceColor','r');imwrite(im_output1,'result1.jpg');im_output2 = adapthisteq(im_input,'NumTiles',[25 25], 'ClipLimit', 0.05);figure, imshow(im_output2); title('Adaptive histogram equalization result');histogram(im_output2,64,'EdgeColor','g','FaceAlpha',0.5,'FaceColor','g');imwrite(im_output2,'result2.jpg'); 空域滤波基础11234567891011im_input = im2double(imread('.\Pics\Ch03\test.tif'));figure, imshow(im_input); title('input image'); H1=[1 1 1;1 1 1;1 1 1]/9;im_output1 = filter2(H1,im_input,'same');figure, imshow(im_output1); title('average filter1');H2 = fspecial('average',35);im_output2 = imfilter(im_input,H2,'symmetric','same','conv');figure, imshow(im_output2); title('average filter2'); 空域滤波基础21234567891011121314im_input = im2double(imread('.\Pics\Ch03\test.tif'));figure, imshow(im_input); title('input image'); H1 = fspecial('average',3);im_output1 = imfilter(im_input,H1);figure, imshow(im_output1); title('average filter');H2 = fspecial('gaussian',[3 3],0.5);im_output2 = imfilter(im_input,H2);figure, imshow(im_output2); title('Gaussian filter');H3 = fspecial('log',[5 5], 0.5);im_output3 = imfilter(im_input,H3);figure, imshow(im_output3); title('LoG filter'); 空域滤波基础312345678910im_input = im2double(imread('.\Pics\Ch03\test.tif'));figure, imshow(im_input); title('input image'); H1 = fspecial('average',3);im_output1 = imfilter(im_input,H1);figure, imshow(im_output1); title('average filter1');H2 = [1 2 1;2 4 2;1 2 1]/16;im_output2 = imfilter(im_input,H2);figure, imshow(im_output2); title('average filter2'); Smoothing Filtering (平滑滤波)12345678910111213141516im_input = im2double(imread('.\Pics\Ch03\test.tif')); %H1 = fspecial('average',3);H1 = [1 2 1;2 4 2;1 2 1]/16;im_output1 = imfilter(im_input,H1);H2 = fspecial('average',5);im_output2 = imfilter(im_input,H2);H3 = fspecial('average',9);im_output3 = imfilter(im_input,H3);H4 = fspecial('average',15);im_output4 = imfilter(im_input,H4);H5 = fspecial('average',35);im_output5 = imfilter(im_input,H5);figure, imshow([im_input im_output1;im_output2 im_output3;im_output4 im_output5]); title('smooting filter with different size of mask'); Median filter V.S Average Filter(平滑滤波)123456789im_input = im2double(imread('.\Pics\Ch03\noisy1.tif')); figure, imshow(im_input); title('input image'); H1 = fspecial('average',3);im_output1 = imfilter(im_input,H1);im_output2 = medfilt2(im_input);figure, imshow([im_output1 im_output2]); title('smooting filter v.s median filter'); Median filter V.S Average Filter(平滑滤波)2123456789im_input = im2double(imread('.\Pics\Ch03\noisy2.tif')); figure, imshow(im_input); title('input image'); H1 = fspecial('average',3);im_output1 = imfilter(im_input,H1);im_output2 = medfilt2(im_input,[5 5]);figure, imshow([im_output1 im_output2]); title('smooting filter v.s median filter'); Laplacian Operator1234567891011im_input = im2double(imread('.\Pics\Ch03\moon.tif')); figure, imshow(im_input); title('input image'); H1 = [0 1 0; 1 -4 1; 0 1 0];H2 = [1 1 1; 1 -8 1; 1 1 1];im_output1 = imfilter(im_input,H1);im_output2 = imfilter(im_input,H2);im_output3 = im_input-im_output2;%figure, imshow(im_output1,[]); title('Laplacian operator1');%figure, imshow(im_output2,[]); title('Laplacian operator2');figure, imshow([im_input im_output3]); title('sharpen using Laplacian operator'); Unsharpen Operator and highboosting filtering1234567891011im_input = im2double(imread('.\Pics\Ch03\moon.tif')); figure, imshow(im_input); title('input image'); filter_mask = fspecial('gaussian');im_output1 = im_input+1*(im_input - imfilter(im_input,filter_mask));im_output2 = im_input+10*(im_input - imfilter(im_input,filter_mask));figure, imshow(im_output1); title('Unsharpen operator');figure, imshow([im_output1 im_output2]); title('highboosting filter'); Gradient operator123456789101112im_input = im2double(imread('.\Pics\Ch03\machine.png')); figure, imshow(im_input); title('input image'); H1 = [1,1,1;0,0,0;-1,-1,-1];H2 = [1,2,1;0,0,0;-1,-2,-1];im_output1 = imfilter(im_input,H1);im_output2 = imfilter(im_input,H2);figure, imshow(im_output1); title('Prewitt operator');figure, imshow(im_output2); title('Sobel filter');]]></content>
      <categories>
        <category>Digital Image Processing</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
        <tag>Digital Image Processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Digital Image Processing - Ch2 Fundamentals]]></title>
    <url>%2F2018%2F09%2F26%2F2018-09-26-Digital_Image_Processing_Ch2_Fundamentals%2F</url>
    <content type="text"><![CDATA[Ch2 Fundamentals Human Visual Perception Image sensing and acquisition Image sampling and quantization Basic relationship between pixelsCodeImage interpolationmethod： Nearest-neighbor interpolation Bilinear interpolation Bicubic interpolation123456789101112im_inut = imread('./Pics/Ch02/weixins.bmp');im_1 = imresize(im_inut, 4,'nearest');im_2 = imresize(im_inut, 4,'Bilinear');im_3 = imresize(im_inut, 4,'Bicubic');figure, imshow(im_inut); title('image negatives method');figure, imshow(im_1); title('Nearest-neighbor interpolation');figure, imshow(im_2); title('Bilinear interpolation');figure, imshow(im_3); title('Bicubic interpolation');%imwrite(im_1,'weixins_neigh.bmp');%imwrite(im_2,'weixins_Bilinear.bmp');%imwrite(im_3,'weixins_Bicubic.bmp'); Output]]></content>
      <categories>
        <category>Digital Image Processing</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
        <tag>Digital Image Processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Digital Image Processing - Ch1 Introduction]]></title>
    <url>%2F2018%2F09%2F25%2F2018-09-25-Digital_Image_Processing_Ch1_Introduction%2F</url>
    <content type="text"><![CDATA[Ch1 Introduction History of DIP Applications Fundamental steps in DIP Code12345678im_input = imread('./Pics/Ch01/Lena.jpg');figure, imshow(im_input), title('original color image');im_gray = rgb2gray(im_input);figure, imshow(im_gray), title('original color image');imwrite(im_gray,'gray.jpg');imfinfo('./Pics/Ch01/Lena.jpg') Output 123456789101112131415ans = Filename: '/Users/jinwang/Desktop/Academy/大三上/Digital_Image_Processing/matlab/Pics/Ch01/Lena.jpg' FileModDate: '27-Jun-2013 21:31:06' FileSize: 91814 Format: 'jpg' FormatVersion: '' Width: 512 Height: 512 BitDepth: 24 ColorType: 'truecolor' FormatSignature: '' NumberOfSamples: 3 CodingMethod: 'Huffman' CodingProcess: 'Sequential' Comment: &#123;'Handmade Software, Inc. Image Alchemy v1.9↵'&#125;]]></content>
      <categories>
        <category>Digital Image Processing</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
        <tag>Digital Image Processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DTFT Implementation Using Matlab]]></title>
    <url>%2F2018%2F09%2F24%2F2018-09-24-DTFT_Implementation_Using_Matlab%2F</url>
    <content type="text"><![CDATA[Definition of DTFTDTFT(Discrete Time Fourier Transform) is the Fourier Transform of a sequence x(n).The definition is:$$X(e^{j\omega}) = DTFT[x(n)] = \sum_{n = -\infty}^{\infty} x(n) e^{-j \omega n}$$ Implementation using matlab123456789101112131415function [amplitude, phase, frequency_domain] = dtft(x, n, M)% This function compute the DTFT(Discrete Time Fourier Transform) of an% input signal x(n). frequency_domain = linspace(0, 2*pi, M); X = zeros(1, M); count = 1; j = sqrt(-1); for omega = frequency_domain exponent_term = exp(- j * omega .* n); X(count) = sum(x .* exponent_term); count = count + 1; end amplitude = abs(X); phase = angle(X);end Code ExampleCode1234567891011121314151617181920212223242526272829303132333435363738394041clear;clc;% Create a input signal x(n) with some frequencies.Fs = 100;T = 5;t = 0: 1/Fs: T;n = 1: length(t);x = cos(2 * pi * 1 * t) ... + cos(2 * pi * 4 * t) ... + cos(2 * pi * 8 * t) ... + cos(2 * pi * 16 * t) ... + cos(2 * pi * 32 * t);% Plot the input signal x(n)subplot(311)plot(n, x);title('Input Signal x(n)');xlabel('n');ylabel('x(n)');% DTFTM = 100;[amplitude, phase, frequency] = dtft(x, n, M);% Plot the amplitude spectrum and phase spectrumsubplot(312)plot(frequency, amplitude);xlim([0, 2*pi]);set(gca,'XTick',0: pi: 2*pi)set(gca,'XTickLabel',&#123;'0','\pi','2\pi'&#125;)title('Amplitude Spectrum'); xlabel('$\omega$', 'Interpreter', 'latex');ylabel('Amplitude');subplot(313)plot(frequency, phase);xlim([0, 2*pi]);set(gca,'XTick',0: pi: 2*pi)set(gca,'XTickLabel',&#123;'0','\pi','2\pi'&#125;)title('Phase Spectrum');xlabel('$\omega$', 'Interpreter', 'latex');ylabel('Phase'); Output]]></content>
      <categories>
        <category>Digital Signal Processing</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
        <tag>Digital Signal Processing</tag>
        <tag>DTFT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Communication System Random Process: A cos wave with random phase]]></title>
    <url>%2F2018%2F09%2F13%2F2018-09-13-Communication_System-Random_Process%2F</url>
    <content type="text"><![CDATA[Cos wave with random phaseThe program below shows a cos wave with a random phase: $$X(t) = Acos(\omega t + \theta)$$ A: amplitude of the cos wave $\omega$: radian frequency of the cos wave $\theta$: a random variable of uniform distribution between $[0, 2\pi]$ Then we generate the autocorrelation function. Code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647clear;clc;% This program creates a cos wave with a random phase.% x = A * cos(w * t + theta)%% A: Amplitude of the cos wave% w: frequency% theta: phase, which is a random variable of % uniform distribution between [0, 2*pi] % Configurationfs = 200; % Sample frequencyf = 5; % Cos wave frequencyA = 1; % Amplitudet = 0: 1/fs: 1; % Sampling timetheta_range = 2 * pi;theta = theta_range * rand(1, length(t));% Create a cos_wave_with_random_phasex = A * cos(2*pi * f * t + theta);mean_value = mean(x);varience_value = var(x);[autocorrelation_value, lags] = xcorr(x);% Theoretical Autocorrelationtheory_autocor = power(A, 2) ./ 2 .* cos(2*pi * f * t + theta_range / 2);[theory_autocor, lags2] = xcorr(theory_autocor);% Plotsubplot(3,1,1);plot(t, x);title(&#123;'$$x(t) = Acos(2\pi ft + \theta)$$', ... '$$\theta :\: a\: random\: variable\: of\: uniform\: distribution\: between\: [0,2\pi ]$$'&#125;, ... 'Interpreter', 'latex');xlabel('$$t / s$$', 'Interpreter', 'latex');ylabel('$$x(t)$$', 'Interpreter', 'latex');subplot(3,1,2);plot(lags/fs, autocorrelation_value);title('$$ Autocorrelation: R_&#123;X&#125;(\tau)$$', 'Interpreter', 'latex');xlabel('$$\tau / s$$', 'Interpreter', 'latex');ylabel('$$R_&#123;X&#125;(\tau)$$', 'Interpreter', 'latex');subplot(3,1,3);plot(lags2/fs, theory_autocor);title('$$ Theoretical Autocorrelation: R_&#123;X&#125;(\tau)$$', 'Interpreter', 'latex');xlabel('$$\tau / s$$', 'Interpreter', 'latex');ylabel('$$R_&#123;X&#125;(\tau)$$', 'Interpreter', 'latex'); Result When $\theta$ is a uniform distribution random variable between $[0, 2\pi]$: When $\theta$ is a uniform distribution random variable between $[0, 1.5\pi]$: When $\theta$ is a uniform distribution random variable between $[0, \pi]$: When $\theta$ is a uniform distribution random variable between $[0, 0.5\pi]$: When $\theta$ is a uniform distribution random variable between $[0, 0]$:]]></content>
      <categories>
        <category>Communication System</category>
        <category>Matlab</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
        <tag>Communication System</tag>
        <tag>Random Process</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Digital Signal Processing Using Matlab Problem 2.1 Solutions & Results]]></title>
    <url>%2F2018%2F09%2F09%2F2018-09-09-Digital_Signal_Processing_Using_Matlab_Problem_2_1%2F</url>
    <content type="text"><![CDATA[Talk less, code more 2.1-11234567891011121314151617clear;clc;% Solutions%% 2.1-1duration_start = -5;duration_end = 15;n = duration_start: duration_end;x = 3 * impseq(-2, duration_start, duration_end) ... + 2 * impseq(0, duration_start,duration_end) ... - impseq(3, duration_start, duration_end) ... + 5 * impseq(7, duration_start, duration_end)% Plotsubplot(4,2,1);stem(n, x);title('Sequence for 2.1-1');xlabel('n');ylabel('x1(n)'); 2.1-2123456789101112131415%% 2.1-2duration_start = -10;duration_end = 10;n = duration_start: duration_end;x = zeros(1, length(n));for k = -5: 5 xk = exp(-abs(k)) * impseq(2 * k, duration_start, duration_end); x = sigadd(x, n, xk, n)end% Plotsubplot(4,2,2);stem(n, x);title('Sequence for 2.1-2');xlabel('n');ylabel('x2(n)'); 2.1-31234567891011121314%% 2.1-3duration_start = 0;duration_end = 15;n = duration_start: duration_end;x = 10 * stepseq(0, duration_start, duration_end) ... -5 * stepseq(5, duration_start, duration_end) ... -10 * stepseq(10, duration_start, duration_end) ... + 5 * stepseq(15, duration_start, duration_end);% Plotsubplot(4,2,3);stem(n, x);title('Sequence for 2.1-3');xlabel('n');ylabel('x2(n)'); 2.1-412345678910%% 2.1-4duration_start = -20;duration_end = 10;n = duration_start: duration_end;x = exp(0.1 * n) .* (stepseq(-20, duration_start, duration_end) - stepseq(10, duration_start, duration_end));subplot(4,2,4);stem(n, x);title('Sequence for 2.1-4');xlabel('n');ylabel('x4(n)'); 2.1-51234567891011%% 2.1-5duration_start = -200;duration_end = 200;n = duration_start: duration_end;x = 5 * (cos(0.49 * pi * n) + cos(0.51 * pi * n));% Plotsubplot(4,2,5);stem(n, x);title('Sequence for 2.1-5');xlabel('n');ylabel('x5(n)'); 2.1-61234567891011%% 2.1-6duration_start = -200;duration_end = 200;n = duration_start: duration_end;x = 2 * sin(0.01 * pi * n) .* cos(0.5 * pi * n);% Plotsubplot(4,2,6);stem(n, x);title('Sequence for 2.1-6');xlabel('n');ylabel('x6(n)'); 2.1-71234567891011%% 2.1-7duration_start = 0;duration_end = 100;n = duration_start: duration_end;x = exp(-0.05 * n) .* sin(0.1*pi*n + pi/3);% Plotsubplot(4,2,7);stem(n, x);title('Sequence for 2.1-7');xlabel('n');ylabel('x7(n)'); 2.1-81234567891011%% 2.1-8duration_start = 0;duration_end = 100;n = duration_start: duration_end;x = exp(0.01 * n) .* sin(0.1*pi*n);% Plotsubplot(4,2,8);stem(n, x);title('Sequence for 2.1-8');xlabel('n');ylabel('x8(n)'); Plot Results]]></content>
      <categories>
        <category>DSP</category>
        <category>Matlab</category>
      </categories>
      <tags>
        <tag>DSP</tag>
        <tag>Matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS20学习笔记（五）]]></title>
    <url>%2F2018%2F08%2F22%2F2018-08-21-CS20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Talk less, code more Lecture 3: birth-life Linear Regression Implementation1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import osos.environ['TF_CPP_MIN_LOG_LEVEL']='2'import tensorflow as tfimport numpy as npimport time# Read data functiondef read_birth_life_data(filename): """ Read in birth_life_2010.txt and return: data in the form of NumPy array n_samples: number of samples """ text = open(filename, 'r').readlines()[1:] data = [line[:-1].split('\t') for line in text] births = [float(line[1]) for line in data] lifes = [float(line[2]) for line in data] data = list(zip(births, lifes)) n_samples = len(data) data = np.asarray(data, dtype=np.float32) return data, n_samplesDATA_FILE = './stanford-tensorflow-tutorials/examples/data/birth_life_2010.txt'data, n_samples = read_birth_life_data(DATA_FILE)# Step 1: Create PlaceholdersX = tf.placeholder(dtype=tf.float32, name='X')Y = tf.placeholder(dtype=tf.float32, name='Y')# Step 2: Create parameters: W, bW = tf.get_variable(name='W', initializer=tf.constant(0.0))b = tf.get_variable(name='b', initializer=tf.constant(0.0))# Step 3: Compute Y_predictY_predict = X * W + b# Step 4: Compute lossloss = tf.square(Y - Y_predict, name='loss')# Step 5: Create Optimizeroptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)writer = tf.summary.FileWriter('./graphs/linear_reg', tf.get_default_graph())start_time = time.time()# Phase 2: Train modelwith tf.Session() as sess: # Step 6: initialize the parameters: W, b sess.run(tf.global_variables_initializer()) # Train every data for i in range(100): total_loss = 0 for x, y in data: _, loss_ = sess.run([optimizer, loss], feed_dict=&#123;X: x, Y: y&#125;) total_loss += loss_ print('Epoch &#123;0&#125;: &#123;1&#125;'.format(i, total_loss/n_samples)) writer.close() W_out, b_out = sess.run([W, b])end_time = time.time()print('Took: %f seconds' % (end_time - start_time))print(W_out, b_out) 感想&emsp;&emsp;从Coursera一路走来，做了很多的Assignment，看了很多的Blog，也读了很多的Official Docs。却一直没有勇气，从头到尾，从无到有，自己去实现一个模型，大多数时候连怎么读取数据，构造数据结构都不会，每次做Assignment就像是在做填空题。但是今天，我从头到尾一步一步的写下这个程序，虽然早已料到会不停的出现error，但可以参考着example，一点一点的改正过来，而不是拿着另一套填空题填空了。我也刚刚对tensorflow的模型构造基本要素和基本层次有了一定的了解，之后要多敲代码，多总结，不管是知识，还是感想。Talk less, code more不要忘记你的那些梦想加油]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Stanford</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS20学习笔记（四）]]></title>
    <url>%2F2018%2F08%2F21%2F2018-08-21-CS20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Talk less, code more read_birth_life_data函数详解1. API1234567891011121314def read_birth_life_data(filename): """ Read in birth_life_2010.txt and return: data in the form of NumPy array n_samples: number of samples """ text = open(filename, 'r').readlines()[1:] data = [line[:-1].split('\t') for line in text] births = [float(line[1]) for line in data] lifes = [float(line[2]) for line in data] data = list(zip(births, lifes)) n_samples = len(data) data = np.asarray(data, dtype=np.float32) return data, n_samples 2. datafilefilename = &#39;./stanford-tensorflow-tutorials/examples/data/birth_life_2010.txt&#39;file_content:一共有190个国家，这里为了显示方便，只取其中的前10个作为例子123456789101112Country Birth rate Life expectancyVietnam 1.822 74.828243902Vanuatu 3.869 70.819487805Tonga 3.911 72.150658537Timor-Leste 5.578 61.999853659Thailand 1.579 73.927658537Solomon Islands 4.229 67.465195122Singapore 1.15 81.641463415Samoa 3.86 72.306390244Philippines 3.142 68.484317073Papua New Guinea 3.951 62.440609756... 3. Dissect the function使用jupyter notebook作为解剖平台 1import numpy as np 1text = open('./stanford-tensorflow-tutorials/examples/data/birth_life_2010_test.txt').readlines()[1:] 123print(type(text))print(len(text))print(text) &lt;class &apos;list&apos;&gt; 10 [&apos;Vietnam\t1.822\t74.828243902\n&apos;, &apos;Vanuatu\t3.869\t70.819487805\n&apos;, &apos;Tonga\t3.911\t72.150658537\n&apos;, &apos;Timor-Leste\t5.578\t61.999853659\n&apos;, &apos;Thailand\t1.579\t73.927658537\n&apos;, &apos;Solomon Islands\t4.229\t67.465195122\n&apos;, &apos;Singapore\t1.15\t81.641463415\n&apos;, &apos;Samoa\t3.86\t72.306390244\n&apos;, &apos;Philippines\t3.142\t68.484317073\n&apos;, &apos;Papua New Guinea\t3.951\t62.440609756\n&apos;] 1print(text[0]) Vietnam 1.822 74.828243902 12# 注意：这里[-1]的目的是去掉每行后面的'\n'print(text[0][:-1]) Vietnam 1.822 74.828243902 1data = [line[:-1].split('\t') for line in text] 1print(data) [[&apos;Vietnam&apos;, &apos;1.822&apos;, &apos;74.828243902&apos;], [&apos;Vanuatu&apos;, &apos;3.869&apos;, &apos;70.819487805&apos;], [&apos;Tonga&apos;, &apos;3.911&apos;, &apos;72.150658537&apos;], [&apos;Timor-Leste&apos;, &apos;5.578&apos;, &apos;61.999853659&apos;], [&apos;Thailand&apos;, &apos;1.579&apos;, &apos;73.927658537&apos;], [&apos;Solomon Islands&apos;, &apos;4.229&apos;, &apos;67.465195122&apos;], [&apos;Singapore&apos;, &apos;1.15&apos;, &apos;81.641463415&apos;], [&apos;Samoa&apos;, &apos;3.86&apos;, &apos;72.306390244&apos;], [&apos;Philippines&apos;, &apos;3.142&apos;, &apos;68.484317073&apos;], [&apos;Papua New Guinea&apos;, &apos;3.951&apos;, &apos;62.440609756&apos;]] 12births = [float(line[1]) for line in data]lifes = [float(line[2]) for line in data] 1print(births) [1.822, 3.869, 3.911, 5.578, 1.579, 4.229, 1.15, 3.86, 3.142, 3.951] 1data = list(zip(births, lifes)) 1print(data) [(1.822, 74.828243902), (3.869, 70.819487805), (3.911, 72.150658537), (5.578, 61.999853659), (1.579, 73.927658537), (4.229, 67.465195122), (1.15, 81.641463415), (3.86, 72.306390244), (3.142, 68.484317073), (3.951, 62.440609756)] 1data = np.asarray(data, dtype=np.float32) 1print(data) [[ 1.82200003 74.82824707] [ 3.86899996 70.81948853] [ 3.91100001 72.15065765] [ 5.57800007 61.99985504] [ 1.579 73.92765808] [ 4.22900009 67.4651947 ] [ 1.14999998 81.64146423] [ 3.8599999 72.30638885] [ 3.14199996 68.48431396] [ 3.95099998 62.44060898]] 1n_samples = len(data) 1print(n_samples) 10]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Stanford</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS20学习笔记（三）]]></title>
    <url>%2F2018%2F08%2F20%2F2018-08-21-CS20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[CS20学习笔记（三）@(Tensorflow) Talk less, code more Basic Models in Tensoflow1. 学习资源 CS20 Lecture3 Slides Tensorflow Official Docs Stanford Tensorflow Tutorial github repo 2. Agenda Review Linear regression on birth/life data Control Flow tf.data Optimizers, gradients Logistic regression on MNIST Loss functions 3. Review3.1 Computation GraphTensorFlow separates definition of computations from their executionPhase 1: assemble a graphPhase 2: use a session to execute operations in the graph. 3.2 TensorBoard 3.3 tf.constant and tf.VariableConstant values are stored in the graph definitionSessions allocate memory to store variable values 3.4 tf.placeholder and feed_dictFeed values into placeholders with a dictionary (feed_dict)Easy to use but poor performance 3.5 Avoid lazy loading Separate the assembling of graph and executing ops Use Python attribute to ensure a function is only loaded the first time it’s called 4. Linear Regression in Tensorflow4.1 Download data from githubsize: 15.85MB1git clone git@github.com:chiphuyen/stanford-tensorflow-tutorials.git 4.2 What is the model about?Model the linear relationship between: dependent variable Y explanatory variables X Life expectancy 寿命 Fertility Rate 出生率 World Development Indicators datasetX: birth rateY: life expectancy190 countries WantFind a linear relationship between X and Yto predict Y from X ModelInference: $Y_predicted = w * X + b$Mean squared error (MSE): $E[(y - y_predicted)^{2}]$ Interactive Codingdata/birth_life_2010.txtexamples/03_linreg_starter.py]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Stanford</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习-深浅copy]]></title>
    <url>%2F2018%2F08%2F19%2F2018-08-19-Python%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E6%B5%85copy%2F</url>
    <content type="text"><![CDATA[废话不多说，直接上代码 1234567891011121314151617181920212223242526&gt;&gt;&gt; import copy# 创建初始变量&gt;&gt;&gt; origin = [1, 2, [3, 4]]&gt;&gt;&gt; origin[1, 2, [3, 4]]# 浅copy&gt;&gt;&gt; shallow_copy = copy.copy(origin)&gt;&gt;&gt; shallow_copy[1, 2, [3, 4]]# 深copy&gt;&gt;&gt; deep_copy = copy.deepcopy(origin)&gt;&gt;&gt; deep_copy[1, 2, [3, 4]]# 修改原始变量&gt;&gt;&gt; origin[2][1] = 99&gt;&gt;&gt; origin[1, 2, [3, 99]]# 浅copy跟着变了，但深copy没有跟着变&gt;&gt;&gt; shallow_copy[1, 2, [3, 99]]&gt;&gt;&gt; deep_copy[1, 2, [3, 4]] 浅copy跟着变了，但深copy没有跟着变]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习-assert的作用]]></title>
    <url>%2F2018%2F08%2F19%2F2018-08-19-Python%E5%AD%A6%E4%B9%A0%E2%80%94assert%2F</url>
    <content type="text"><![CDATA[简单来说，assert就是为程序提供一个判断，如果assert里面的内容成立，为真，那么程序就向下执行，如果发现assert里面的东西是假的，就抛出AssertionError的错误。 下面是几个语句，很清楚的看到了assert的作用12345678910111213141516171819202122# 数值判断&gt;&gt;&gt; a = 3&gt;&gt;&gt; assert(a==3)&gt;&gt;&gt; assert(a &lt; 4)&gt;&gt;&gt; assert(a &gt; 4)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AssertionError# True/Flase判断&gt;&gt;&gt; assert(True)&gt;&gt;&gt; assert(False)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AssertionError# 1/0判断&gt;&gt;&gt; assert(1)&gt;&gt;&gt; assert(0)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AssertionError]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS20学习笔记（二）]]></title>
    <url>%2F2018%2F08%2F19%2F2018-08-20-CS20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Talk less, code more Tensorflow Operations1. 学习资料 CS 20 Slide2 Tensorflow Operations Tensorflow Official Docs 2. Agenda Basic Operations Tensor Types Importing Data Lazy loading 3. TensorBoard3.1 Your first tensorflow program12345678import tensorflow as tfa = tf.constant(2)b = tf.constant(3)c = tf.add(a, b)with tf.Session() as sess: print(sess.run(c)) Output:122018-08-20 13:42:00.773870: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA5 实际上，上面除了5是我们想要的结果，5上面的是一个warning，简单解释一下这个warning。上面的意思是说我的CPU支持AVX2和FMA，但是tensorflow使用不了。AVX2和FMA实际上是CPU内部为了加速矩阵运算所设计出来的东西，说白了就是会让运算变得更快（虽然与GPU相比相去甚远），但是我们的tensorflow默认用不了这些高级玩应儿，所以就会出现这种warning。参考：CSDN我们通过以下代码可以解决这个问题123456789101112# 用下面这两行代码解决了这个问题，虽然我也不知道这两行到底干了啥import osos.environ['TF_CPP_MIN_LOG_LEVEL']='2'import tensorflow as tfa = tf.constant(2)b = tf.constant(3)c = tf.add(a, b)with tf.Session() as sess: print(sess.run(c)) Output:15 3.2 Visualize it with TensorfBoard1234567891011import tensorflow as tfa = tf.constant(2)b = tf.constant(3)c = tf.add(a, b)writer = tf.summary.FileWriter('./graph1', tf.get_default_graph())with tf.Session() as sess: print(sess.run(c))writer.close() 与此同时，我们的当前目录下面多了一个叫做“graph1”的文件夹接着，我们学习如何使用tensorboard导入graph11234#Go to terminal, run:$ python3 [yourprogram].py$ tensorboard --logdir="./graphs" --port 6006#Then open your browser and go to: http://localhost:6006/ 添加name属性1234567891011import tensorflow as tfa = tf.constant(2, name='a')b = tf.constant(3, name='b')c = tf.add(a, b, name='add')writer = tf.summary.FileWriter('./graph1', tf.get_default_graph())with tf.Session() as sess: print(sess.run(c))writer.close() CS20 Note: TensorBoard can do more than just visualizing your graphs. Learn to use TensorBoard well and often! 4. Constants, Sequences, Variables, Ops4.1 Constants12345678# API definitiontf.constant( value, dtype=None, shape=None, name='Const', verify_shape=False) For example,12345678import tensorflow as tfa = tf.constant([2, 2], name='a')b = tf.constant([[0, 1], [2, 3]], name='b')x = tf.multiply(a, b, name='mul') # tf.multiply is an element-wise operationwith tf.Session() as sess: print(sess.run(x)) Output: 这里使用到了broadcasting12[[0 2] [4 6]] 4.2 Tensors filled with a specific valuetf.zeros()1234567891011# APItf.zeros(shape, dtype=tf.float32, name=None)# For example:tf.zeros([2, 3], dtype=tf.int32)# 我就随意改了一下，竟然也是同样的结果，其实有时候我也搞不懂应该用"[]"还是"()"tf.zeros((2, 3), dtype=tf.int32) # 输出和上面是一样的==&gt;[[0 0 0] [0 0 0]] tf.zeros_like()12345678910111213141516171819# APItf.zeros_like(input_tensor, dtype=None, name=None, optimize=True)# creates a tensor of shape and type (unless type is specified) as the input_tensor but all elements are zeros.# For exampleimport tensorflow as tf# input_tensor is [[0, 1], [2, 3], [4, 5]]a = tf.constant([[0, 1], [2, 3], [4, 5]], name='a')b = tf.zeros_like(a)with tf.Session() as sess: print(sess.run(b))==&gt;[[0 0] [0 0] [0 0]] tf.onse()和tf.ones_like()同上 tf.fill()12345678910# APItf.fill(dims, value, name=None)# create a tensor filled with a scalar value# For exampletf.fill([2, 3], 8)==&gt;[[8 8 8] [8 8 8]] 4.3 Constants as sequencestf.lin_space()12345678# APItf.lin_space(start, stop, num, name=None)# For exampletf.lin_space(10.0, 13.0, 4)==&gt;[ 10. 11. 12. 13.] tf.range()12345678910111213141516171819# APItf.range(start, limit=None, delta=1, dtype=None, name='range')# For exampletf.range(3, 18, 3)==&gt;[ 3 6 9 12 15]# For another exampletf.range(5)==&gt;[0 1 2 3 4]NOTENOT THE SAME AS NUMPY SEQUENCESTensor objects are not iterablefor _ in tf.range(4): # TypeError 4.4 Randomly Generated Constants12345678910tf.random_normaltf.truncated_normaltf.random_uniformtf.random_shuffletf.random_croptf.multinomialtf.random_gamma# 设置随机种子tf.set_random_seed(seed) 4.5 Operations 4.6 Arithmetic OperationsPretty standard, quite similar to numpy. 4.7 Wizard of Div矩阵除法，就很乱12345678910a = tf.constant([2, 2], name='a')b = tf.constant([[0, 1], [2, 3]], name='b')with tf.Session() as sess: print(sess.run(tf.div(b, a))) ⇒ [[0 0] [1 1]] print(sess.run(tf.divide(b, a))) ⇒ [[0. 0.5] [1. 1.5]] print(sess.run(tf.truediv(b, a))) ⇒ [[0. 0.5] [1. 1.5]] print(sess.run(tf.floordiv(b, a))) ⇒ [[0 0] [1 1]] print(sess.run(tf.realdiv(b, a))) ⇒ # Error: only works for real values print(sess.run(tf.truncatediv(b, a))) ⇒ [[0 0] [1 1]] print(sess.run(tf.floor_div(b, a))) ⇒ [[0 0] [1 1]] 5. Tensorflow Data Types12345678910111213141516171819202122232425# TensorFlow takes Python natives types: boolean, numeric (int, float), strings# scalars are treated like 0-d tensorst_0 = 19 tf.zeros_like(t_0) ==&gt; 0tf.ones_like(t_0) ==&gt; 1# 1-d arrays are treated like 1-d tensorst_1 = [b"apple", b"peach", b"grape"] tf.zeros_like(t_1) ==&gt; [b'' b'' b'']tf.ones_like(t_1) ==&gt; TypeError: Expected string, got 1 of type 'int' instead.# 2-d arrays are treated like 2-d tensorst_2 = [[True, False, False], [False, False, True], [False, True, False]] tf.zeros_like(t_2) ==&gt; 3x3 tensor, all elements are Falsetf.ones_like(t_2) ==&gt; 3x3 tensor, all elements are True 5.1 TF vs NP Data Types1234567891011121314151617181920212223#TensorFlow integrates seamlessly with NumPytf.int32 == np.int32 ⇒ True#Can pass numpy types to TensorFlow opstf.ones([2, 2], np.float32) ⇒ [[1.0 1.0], [1.0 1.0]]#For tf.Session.run(fetches): if the requested fetch is a Tensor , output will be a NumPy ndarray.sess = tf.Session()a = tf.zeros([2, 3], np.int32)print(type(a)) ⇒ &lt;class 'tensorflow.python.framework.ops.Tensor'&gt;a = sess.run(a)print(type(a)) ⇒ &lt;class 'numpy.ndarray'&gt;sess.close() Use TF DType when possible Python native types: TensorFlow has to infer Python type NumPy arrays: NumPy is not GPU compatible 6. What’s wrong with constants?1234567891011121314151617181920212223242526272829303132333435import tensorflow as tfmy_const = tf.constant([1.0, 2.0], name='my_const')with tf.Session() as sess: print(sess.graph.as_graph_def())================&gt;node &#123; name: "my_const" op: "Const" attr &#123; key: "dtype" value &#123; type: DT_FLOAT &#125; &#125; attr &#123; key: "value" value &#123; tensor &#123; dtype: DT_FLOAT tensor_shape &#123; dim &#123; size: 2 &#125; &#125; tensor_content: "\000\000\200?\000\000\000@" &#125; &#125; &#125;&#125;versions &#123; producer: 26&#125; Constants are stored in the graph definitionThis makes loading graphs expensive when constants are big 7. Variablestf.Variable()1234567import tensorflow as tf# create variabels with tf.Variable()s = tf.Variable(2, name='scalar')m = tf.Variable([[0, 1], [2, 3]], name='matrix')# 这里用的是tf.zeros()来创建Variable，但是tf.zeros()是一个Constant，所以我觉得，tf.Variable()就好像有可以把Constant转换成Variable的功能W = tf.Variable(tf.zeros([784, 10])) tf.get_variable()12345import tensorflow as tfs = tf.get_variable('scalar', initializer=tf.constant(2))m = tf.get_variable('matrix', initializer=tf.constant([1, 2]))W = tf.get_variable('big_matrix', shape=(784, 10), initializer=tf.zeros_initializer()) Q: Why tf.constant but tf.Variable?A: tf.constant is an op tf.Variable is a class with many ops 7.1 tf.Variable() classtf.Variable holds several ops:123456x = tf.Variable(...) x.initializer # init opx.value() # read opx.assign(...) # write opx.assign_add(...) # and more 7.2 We must initialize our variables!!!what if we don’t?1234567W = tf.get_variable("big_matrix", shape=(784, 10), initializer=tf.zeros_initializer())with tf.Session() as sess: print(sess.run(W)) =========&gt;&gt;FailedPreconditionError: Attempting to use uninitialized value Variable so, how to initialize variables?Note: Initializer is an op. You need to execute it within the context of a session12345678910111213# 1 The easiest way is initializing all variables at once:with tf.Session() as sess: sess.run(tf.global_variables_initializer())# 2 Initialize only a subset of variables:with tf.Session() as sess: sess.run(tf.variables_initializer([a, b]))# 3 Initialize a single variableW = tf.Variable(tf.zeros([784, 10]))with tf.Session() as sess: sess.run(W.initializer) 7.3 Eval() a variable12345678910import tensorflow as tfW = tf.Variable(tf.truncated_normal([2, 3]))with tf.Session() as sess: sess.run(W.initializer) print(W)=====&gt;&lt;tf.Variable 'Variable:0' shape=(2, 3) dtype=float32_ref&gt; 123456789101112import tensorflow as tfW = tf.Variable(tf.truncated_normal([2, 3]))with tf.Session() as sess: sess.run(W.initializer) print(W.eval())====&gt;# 这个结果是随机的，我试了几次，结果都不一样，只有指定random seed才会每次都一样[[-1.11789441 -0.48606175 -1.32297659] [-1.96772468 0.62769943 -1.23245573]] 7.4 tf.Variable.assign()1234567891011import tensorflow as tfW = tf.Variable(10)W.assign(100)with tf.Session() as sess: sess.run(W.initializer) print(W.eval())====&gt;10 咦？怎么是10呢，不是用assign重新对W进行赋值操作了嘛，不应该是100才对嘛。为什么？因为W.assign(100)是一个op，所有的op必须在Session里面执行才会奏效，否则不会。记住：所有的op操作，必须在Session里执行才会有效。1234567891011import tensorflow as tfW = tf.Variable(10)with tf.Session() as sess: sess.run(W.initializer) sess.run(W.assign(100)) print(W.eval())====&gt;100 1234567891011# create a variable whose original value is 2my_var = tf.Variable(2, name="my_var") # assign a * 2 to a and call that op a_times_twomy_var_times_two = my_var.assign(2 * my_var)with tf.Session() as sess: sess.run(my_var.initializer) sess.run(my_var_times_two) # &gt;&gt; the value of my_var now is 4 sess.run(my_var_times_two) # &gt;&gt; the value of my_var now is 8 sess.run(my_var_times_two) # &gt;&gt; the value of my_var now is 16 7.5 assign_add() and assign_sub()12345678910111213141516171819202122import tensorflow as tfa = tf.Variable(10, dtype=tf.int32)print(a)with tf.Session() as sess: sess.run(a.initializer) print(a.eval()) # +10 sess.run(a.assign_add(10)) print(a.eval()) # -2 sess.run(a.assign_sub(2)) print(a.eval())====&gt;&lt;tf.Variable 'Variable:0' shape=() dtype=int32_ref&gt;102018 7.6 Two Sessions?Each session maintains its own copy of variables12345678910111213141516171819import tensorflow as tfW = tf.Variable(10)sess1 = tf.Session()sess2 = tf.Session()sess1.run(W.initializer)sess2.run(W.initializer)print(sess1.run(W.assign_add(10)))print(sess2.run(W.assign_sub(2)))sess1.close()sess2.close()====&gt;208 8. PlaceholderA TF program often has 2 phases: Assemble a graph Use a session to execute operations in the graph. 12# APItf.placeholder(dtype. shape=None, name=None) Let’s create a placeholder!1234567891011121314import tensorflow as tfa = tf.placeholder(tf.float32, shape=[3])b = tf.constant([5, 5, 5], tf.float32)c = a + bwith tf.Session() as sess: print(sess.run(c))====&gt;InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3] [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]] feed_dict12345678910111213import tensorflow as tfa = tf.placeholder(tf.float32, shape=[3])b = tf.constant([5, 5, 5], tf.float32)c = a + bwith tf.Session() as sess: print(sess.run(c, feed_dict=&#123;a: [1, 2, 3]&#125;))====&gt;[ 6. 7. 8.] Placeholders are valid ops!从上面这张我们可以看出，Const和Placeholder、add的形状是不一样的，Placeholder的形状和add一样，这表达的意思是两者都是ops！而Const是一个小圈，可能代表的意思是“我不是个ops”，凡是打圈圈，都是ops，都要经过Session才能有效执行，而constant这种在定义就已经执行了。要通过仔细观察这个图发现这个事。 12tf.Graph.is_feedable(tensor) # True if and only if tensor is feedable. 12345678910# create operations, tensors, etc (using the default graph)a = tf.add(2, 5)b = tf.multiply(a, 3)with tf.Session() as sess: # compute the value of b given a is 15 sess.run(b, feed_dict=&#123;a: 15&#125;) ====&gt;45 9. End&emsp;&emsp;Let’s construct a model next time!]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Stanford</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras学习之路（一）]]></title>
    <url>%2F2018%2F08%2F18%2F2018-08-18-Keras%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[参考：Keras教程 1. Sequential模型1.1 创建model这里的Dense我的理解就是一层全连接神经网络（FC），Activation很好理解，就是激活函数层，下面是用类似于list来创建一个model 123456789from keras.models import Sequentialfrom keras.layers import Dense, Activationmodel = Sequential([Dense(32, input_shape=(784,)),Activation('relu'),Dense(10),Activation('softmax'),]) 还可以一层一层往里加12345678910model = Sequential()model.add(Dense(32, input_dim=784))model.add(Activation('relu'))model.add(Dense(10))model.add(Activation('softmax'))# ormodel = Sequential()model.add(Dense(32, input_shape=(784,))) 除了第一层需要我们指定input size的大小，剩下的Keras会根据我们构造的网络自动计算出每层输入输出的size，这就很方便。 1.2 编译我对编译（compile）的理解就是，确定用什么optimize method，用什么cost function，用什么来当作最终模型好坏的指标（下面的例子中全都用的是accuracy，也就是把准确率当作指标）（提问：难道从另一种角度看cost function的值不能当作指标？ 回答：函数的结果只是一个值，不同的model选择不同的cost function得出的cost function值也不一样，但使用accuracy准确率就可以避免这个事，其实，准确率才是判断model好坏的真正标准）123456789101112131415161718192021222324# For a multi-class classification problemmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])# For a binary classification problemmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])# For a mean squared error regression problemmodel.compile(optimizer='rmsprop', loss='mse')# For custom metrics# 这个例子是说，我们可以自定义我们的cost functionimport keras.backend as Kdef mean_pred(y_true, y_pred): return K.mean(y_pred)model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred]) 1.3 训练训练就是最终我们开始一轮一轮的计算GD，找到最优parameter的过程了下面给出两个例子，一个是binary classification，另一个是multi-class classification Binary classification: 12345678910111213141516# For a single-input model with 2 classes (binary classification):model = Sequential()model.add(Dense(32, activation='relu', input_dim=100))model.add(Dense(1, activation='sigmoid'))model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])# Generate dummy dataimport numpy as npdata = np.random.random((1000, 100))labels = np.random.randint(2, size=(1000, 1))# Train the model, iterating on the data in batches of 32 samplesmodel.fit(data, labels, epochs=10, batch_size=32) 运行结果123456789101112131415161718192021Epoch 1/101000/1000 [==============================] - 0s 212us/step - loss: 0.7115 - acc: 0.5170Epoch 2/101000/1000 [==============================] - 0s 52us/step - loss: 0.7043 - acc: 0.5040Epoch 3/101000/1000 [==============================] - 0s 62us/step - loss: 0.6956 - acc: 0.5460Epoch 4/101000/1000 [==============================] - 0s 51us/step - loss: 0.6919 - acc: 0.5250Epoch 5/101000/1000 [==============================] - 0s 56us/step - loss: 0.6888 - acc: 0.5360Epoch 6/101000/1000 [==============================] - 0s 54us/step - loss: 0.6846 - acc: 0.5720Epoch 7/101000/1000 [==============================] - 0s 55us/step - loss: 0.6792 - acc: 0.5710Epoch 8/101000/1000 [==============================] - 0s 53us/step - loss: 0.6796 - acc: 0.5640Epoch 9/101000/1000 [==============================] - 0s 54us/step - loss: 0.6746 - acc: 0.5820Epoch 10/101000/1000 [==============================] - 0s 52us/step - loss: 0.6706 - acc: 0.5860&lt;keras.callbacks.History at 0x7f1192fcdcc0&gt; Multi-class classification:12345678910111213141516171819# For a single-input model with 10 classes (categorical classification):model = Sequential()model.add(Dense(32, activation='relu', input_dim=100))model.add(Dense(10, activation='softmax'))model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])# Generate dummy dataimport numpy as npdata = np.random.random((1000, 100))labels = np.random.randint(10, size=(1000, 1))# Convert labels to categorical one-hot encodingone_hot_labels = keras.utils.to_categorical(labels, num_classes=10)# Train the model, iterating on the data in batches of 32 samplesmodel.fit(data, one_hot_labels, epochs=10, batch_size=32) 运行结果123456789101112131415161718192021Epoch 1/101000/1000 [==============================] - 0s 243us/step - loss: 2.3589 - acc: 0.1010Epoch 2/101000/1000 [==============================] - 0s 53us/step - loss: 2.3228 - acc: 0.0960Epoch 3/101000/1000 [==============================] - 0s 52us/step - loss: 2.3140 - acc: 0.1030Epoch 4/101000/1000 [==============================] - 0s 53us/step - loss: 2.3035 - acc: 0.1050Epoch 5/101000/1000 [==============================] - 0s 54us/step - loss: 2.2928 - acc: 0.1290Epoch 6/101000/1000 [==============================] - 0s 49us/step - loss: 2.2813 - acc: 0.1280Epoch 7/101000/1000 [==============================] - 0s 60us/step - loss: 2.2729 - acc: 0.1300Epoch 8/101000/1000 [==============================] - 0s 54us/step - loss: 2.2647 - acc: 0.1530Epoch 9/101000/1000 [==============================] - 0s 54us/step - loss: 2.2545 - acc: 0.1460Epoch 10/101000/1000 [==============================] - 0s 52us/step - loss: 2.2463 - acc: 0.1580&lt;keras.callbacks.History at 0x7f1192cf8cc0&gt; 2. Model模型12345678910111213141516171819from keras.layers import Input, Densefrom keras.models import Model# This returns a tensorinputs = Input(shape=(784,))# a layer instance is callable on a tensor, and returns a tensorx = Dense(64, activation='relu')(inputs)x = Dense(64, activation='relu')(x)predictions = Dense(10, activation='softmax')(x)# This creates a model that includes the Input layer and three Dense layersmodel = Model(inputs=inputs, outputs=predictions)model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])# Trainmodel.fit(data, labels) 之后调用模型的时候123x = Input(shape=(784,))# This works, and returns the 10-way softmax we defined above.y = model(x) 3. Keras模型的一些工具对于下面这个model123456789# Modelmodel = Sequential()model.add(Dense(32, activation='relu', input_dim=100))model.add(Dense(10, activation='softmax'))# Compilemodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) 如果使用model.summary()1model.summary() 输出123456789Layer (type) Output Shape Param # =================================================================dense_20 (Dense) (None, 32) 3232 _________________________________________________________________dense_21 (Dense) (None, 10) 330 =================================================================Total params: 3,562Trainable params: 3,562Non-trainable params: 0 如果使用model.get_config()1model.get_config() 输出123456789101112131415161718192021222324252627282930313233343536[&#123;'class_name': 'Dense', 'config': &#123;'activation': 'relu', 'activity_regularizer': None, 'batch_input_shape': (None, 100), 'bias_constraint': None, 'bias_initializer': &#123;'class_name': 'Zeros', 'config': &#123;&#125;&#125;, 'bias_regularizer': None, 'dtype': 'float32', 'kernel_constraint': None, 'kernel_initializer': &#123;'class_name': 'VarianceScaling', 'config': &#123;'distribution': 'uniform', 'mode': 'fan_avg', 'scale': 1.0, 'seed': None&#125;&#125;, 'kernel_regularizer': None, 'name': 'dense_22', 'trainable': True, 'units': 32, 'use_bias': True&#125;&#125;, &#123;'class_name': 'Dense', 'config': &#123;'activation': 'softmax', 'activity_regularizer': None, 'bias_constraint': None, 'bias_initializer': &#123;'class_name': 'Zeros', 'config': &#123;&#125;&#125;, 'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': &#123;'class_name': 'VarianceScaling', 'config': &#123;'distribution': 'uniform', 'mode': 'fan_avg', 'scale': 1.0, 'seed': None&#125;&#125;, 'kernel_regularizer': None, 'name': 'dense_23', 'trainable': True, 'units': 10, 'use_bias': True&#125;&#125;]]]></content>
      <categories>
        <category>Keras</category>
      </categories>
      <tags>
        <tag>Keras</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS20学习笔记（一）]]></title>
    <url>%2F2018%2F08%2F18%2F2018-08-20-CS20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Talk less, code more 1. 写在前面&emsp;&emsp;之前看了一些Tensorflow的教学博客，也看了Tensorflow官方Tutorial，也看了Keras的Tutorial，总感觉学了一些Tensorflow但又学的不系统，前几天在网上看到了CS20的课程，虽然没有Video，但是有Slide。所以，就看着slides敲一边代码，作为学习主要方式了。一步一步，踏踏实实，学好Tensorflow，将会是一个新的层次。那么下面就开始吧！ 2. 学习资料 CS20 Lecture 1 Slides Tensorflow Official Docs pycharm 3. Lecture1: Welcome to Tensorflow!3.1 Agenda Welcome Overview of Tensorflow Graphs and Sessions 3.2 Create a tensor1234import tensorflow as tfa = tf.add(3, 5)print(a) Output:1Tensor("Add:0", shape=(), dtype=int32) 3.3 Create a tensor, than run a Session1234567import tensorflow as tfa = tf.add(3, 5)sess = tf.Session()print(sess.run(a))sess.close() Output:122018-08-20 08:39:46.730902: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA8 3.4 with tf.Session() as sess:123456import tensorflow as tfa = tf.add(3, 5)with tf.Session() as sess: print(sess.run(a)) Output:122018-08-20 08:41:55.387805: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA8 3.5 More Graphs3.5.11234567891011import tensorflow as tfx = 2y = 3op1 = tf.add(x, y)op2 = tf.multiply(x, y)op3 = tf.pow(op2, op1)with tf.Session() as sess: print(sess.run(op3)) Output:17776 3.5.2123456789101112import tensorflow as tfx = 2y = 3add_op = tf.add(x, y)mul_op = tf.multiply(x, y)useless = tf.multiply(add_op, mul_op)pow_op = tf.pow(add_op, mul_op)with tf.Session() as sess: z = sess.run(pow_op) 3.5.3123456789101112import tensorflow as tfx = 2y = 3add_op = tf.add(x, y)mul_op = tf.multiply(x, y)useless = tf.multiply(add_op, mul_op)pow_op = tf.pow(add_op, mul_op)with tf.Session() as sess: z, not_useless = sess.run([pow_op, useless]) 123456tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)# fetches is a list of tensors whose values you want 3.6 Distributed Computation1234567891011121314# To put part of the computation on a specific CPU or GPUimport tensorflow as tf# Create a graphwith tf.device('/gpu:2'): a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name='a') b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name='b') c = tf.multiply(a, b)# Create a session with log_device_placement to Truesess = tf.Session(config=tf.ConfigProto(log_device_placement=True))sess.run(c)sess.close() MacPro上没有gpu，下面是在AWS上运行的结果12345678910/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters# 上面这些是每次运行tensorflow程序都会出现的# 下面是运行'/gpu:0'才出现的Device mapping:/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0a: (Const): /job:localhost/replica:0/task:0/device:GPU:0b: (Const): /job:localhost/replica:0/task:0/device:GPU:0[1. 4. 9.] 3.7 More than one Graphs?不建议这样的做法，一个程序里面最好只有一个graph12345678910import tensorflow as tfg = tf.Graph()# add ops to the default grapha = tf.constant(2)# add ops to user created graphwith g.as_default(): b = tf.constant(3) 下面这样做虽然更合理一点，但是还是不建议！因为一张图里最好只有一个Graph！1234567891011121314# Do not mix the default graph and user created graphimport tensorflow as tfg1 = tf.get_default_graph()g2 = tf.Graph()# add ops to the default graphwith g1.as_default(): a = tf.constant(2)# add ops to the user created graphwith g2.as_default(): b = tf.constant(3)]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Stanford</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning Course3 Week1 Programming Assignment2]]></title>
    <url>%2F2018%2F08%2F16%2F2018-08-15-Convolution%2Bmodel%2B-%2BApplication%2B-%2Bv1%2F</url>
    <content type="text"><![CDATA[Convolutional Neural Networks: ApplicationWelcome to Course 4’s second assignment! In this notebook, you will: Implement helper functions that you will use when implementing a TensorFlow model Implement a fully functioning ConvNet using TensorFlow After this assignment you will be able to: Build and train a ConvNet in TensorFlow for a classification problem We assume here that you are already familiar with TensorFlow. If you are not, please refer the TensorFlow Tutorial of the third week of Course 2 (“Improving deep neural networks“). 1.0 - TensorFlow modelIn the previous assignment, you built helper functions using numpy to understand the mechanics behind convolutional neural networks. Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call. As usual, we will start by loading in the packages. 12345678910111213import mathimport numpy as npimport h5pyimport matplotlib.pyplot as pltimport scipyfrom PIL import Imagefrom scipy import ndimageimport tensorflow as tffrom tensorflow.python.framework import opsfrom cnn_utils import *%matplotlib inlinenp.random.seed(1) Run the next cell to load the “SIGNS” dataset you are going to use. 12# Loading the data (signs)X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset() As a reminder, the SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5. The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of index below and re-run to see different examples. 1234# Example of a pictureindex = 6plt.imshow(X_train_orig[index])print ("y = " + str(np.squeeze(Y_train_orig[:, index]))) y = 2 In Course 2, you had built a fully-connected network for this dataset. But since this is an image dataset, it is more natural to apply a ConvNet to it. To get started, let’s examine the shapes of your data. 1234567891011X_train = X_train_orig/255.X_test = X_test_orig/255.Y_train = convert_to_one_hot(Y_train_orig, 6).TY_test = convert_to_one_hot(Y_test_orig, 6).Tprint ("number of training examples = " + str(X_train.shape[0]))print ("number of test examples = " + str(X_test.shape[0]))print ("X_train shape: " + str(X_train.shape))print ("Y_train shape: " + str(Y_train.shape))print ("X_test shape: " + str(X_test.shape))print ("Y_test shape: " + str(Y_test.shape))conv_layers = &#123;&#125; number of training examples = 1080 number of test examples = 120 X_train shape: (1080, 64, 64, 3) Y_train shape: (1080, 6) X_test shape: (120, 64, 64, 3) Y_test shape: (120, 6) 1.1 - Create placeholdersTensorFlow requires that you create placeholders for the input data that will be fed into the model when running the session. Exercise: Implement the function below to create placeholders for the input image X and the output Y. You should not define the number of training examples for the moment. To do so, you could use “None” as the batch size, it will give you the flexibility to choose it later. Hence X should be of dimension [None, n_H0, n_W0, n_C0] and Y should be of dimension [None, n_y]. Hint. 1234567891011121314151617181920212223# GRADED FUNCTION: create_placeholdersdef create_placeholders(n_H0, n_W0, n_C0, n_y): """ Creates the placeholders for the tensorflow session. Arguments: n_H0 -- scalar, height of an input image n_W0 -- scalar, width of an input image n_C0 -- scalar, number of channels of the input n_y -- scalar, number of classes Returns: X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype "float" Y -- placeholder for the input labels, of shape [None, n_y] and dtype "float" """ ### START CODE HERE ### (≈2 lines) X = tf.placeholder(dtype=tf.float32, shape=(None, n_H0, n_W0, n_C0)) Y = tf.placeholder(dtype=tf.float32, shape=(None, n_y)) ### END CODE HERE ### return X, Y 123X, Y = create_placeholders(64, 64, 3, 6)print ("X = " + str(X))print ("Y = " + str(Y)) X = Tensor(&quot;Placeholder:0&quot;, shape=(?, 64, 64, 3), dtype=float32) Y = Tensor(&quot;Placeholder_1:0&quot;, shape=(?, 6), dtype=float32) Expected Output X = Tensor(“Placeholder:0”, shape=(?, 64, 64, 3), dtype=float32) Y = Tensor(“Placeholder_1:0”, shape=(?, 6), dtype=float32) 1.2 - Initialize parametersYou will initialize weights/filters $W1$ and $W2$ using tf.contrib.layers.xavier_initializer(seed = 0). You don’t need to worry about bias variables as you will soon see that TensorFlow functions take care of the bias. Note also that you will only initialize the weights/filters for the conv2d functions. TensorFlow initializes the layers for the fully connected part automatically. We will talk more about that later in this assignment. Exercise: Implement initialize_parameters(). The dimensions for each group of filters are provided below. Reminder - to initialize a parameter $W$ of shape [1,2,3,4] in Tensorflow, use:1W = tf.get_variable("W", [1,2,3,4], initializer = ...) More Info. 12345678910111213141516171819202122# GRADED FUNCTION: initialize_parametersdef initialize_parameters(): """ Initializes weight parameters to build a neural network with tensorflow. The shapes are: W1 : [4, 4, 3, 8] W2 : [2, 2, 8, 16] Returns: parameters -- a dictionary of tensors containing W1, W2 """ tf.set_random_seed(1) # so that your "random" numbers match ours ### START CODE HERE ### (approx. 2 lines of code) W1 = tf.get_variable('W1', [4,4,3,8], initializer = tf.contrib.layers.xavier_initializer(seed = 0)) W2 = tf.get_variable('W2', [2,2,8,16], initializer= tf.contrib.layers.xavier_initializer(seed = 0)) ### END CODE HERE ### parameters = &#123;"W1": W1, "W2": W2&#125; return parameters 1234567tf.reset_default_graph()with tf.Session() as sess_test: parameters = initialize_parameters() init = tf.global_variables_initializer() sess_test.run(init) print("W1 = " + str(parameters["W1"].eval()[1,1,1])) print("W2 = " + str(parameters["W2"].eval()[1,1,1])) W1 = [ 0.00131723 0.14176141 -0.04434952 0.09197326 0.14984085 -0.03514394 -0.06847463 0.05245192] W2 = [-0.08566415 0.17750949 0.11974221 0.16773748 -0.0830943 -0.08058 -0.00577033 -0.14643836 0.24162132 -0.05857408 -0.19055021 0.1345228 -0.22779644 -0.1601823 -0.16117483 -0.10286498] Expected Output: W1 = [ 0.00131723 0.14176141 -0.04434952 0.09197326 0.14984085 -0.03514394 -0.06847463 0.05245192] W2 = [-0.08566415 0.17750949 0.11974221 0.16773748 -0.0830943 -0.08058 -0.00577033 -0.14643836 0.24162132 -0.05857408 -0.19055021 0.1345228 -0.22779644 -0.1601823 -0.16117483 -0.10286498] 1.2 - Forward propagationIn TensorFlow, there are built-in functions that carry out the convolution steps for you. tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = ‘SAME’): given an input $X$ and a group of filters $W1$, this function convolves $W1$’s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation here tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = ‘SAME’): given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window. You can read the full documentation here tf.nn.relu(Z1): computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation here. tf.contrib.layers.flatten(P): given an input P, this function flattens each example into a 1D vector it while maintaining the batch-size. It returns a flattened tensor with shape [batch_size, k]. You can read the full documentation here. tf.contrib.layers.fully_connected(F, num_outputs): given a the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation here. In the last function above (tf.contrib.layers.fully_connected), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters. Exercise: Implement the forward_propagation function below to build the following model: CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED. You should use the functions above. In detail, we will use the following parameters for all the steps: - Conv2D: stride 1, padding is &quot;SAME&quot; - ReLU - Max pool: Use an 8 by 8 filter size and an 8 by 8 stride, padding is &quot;SAME&quot; - Conv2D: stride 1, padding is &quot;SAME&quot; - ReLU - Max pool: Use a 4 by 4 filter size and a 4 by 4 stride, padding is &quot;SAME&quot; - Flatten the previous output. - FULLYCONNECTED (FC) layer: Apply a fully connected layer without an non-linear activation function. Do not call the softmax here. This will result in 6 neurons in the output layer, which then get passed later to a softmax. In TensorFlow, the softmax and cost function are lumped together into a single function, which you&apos;ll call in a different function when computing the cost. 1234567891011121314151617181920212223242526272829303132333435363738394041# GRADED FUNCTION: forward_propagationdef forward_propagation(X, parameters): """ Implements the forward propagation for the model: CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED Arguments: X -- input dataset placeholder, of shape (input size, number of examples) parameters -- python dictionary containing your parameters "W1", "W2" the shapes are given in initialize_parameters Returns: Z3 -- the output of the last LINEAR unit """ # Retrieve the parameters from the dictionary "parameters" W1 = parameters['W1'] W2 = parameters['W2'] ### START CODE HERE ### # CONV2D: stride of 1, padding 'SAME' Z1 = tf.nn.conv2d(input=X, filter=W1, strides=[1,1,1,1], padding='SAME') # RELU A1 = tf.nn.relu(Z1) # MAXPOOL: window 8x8, stride 8, padding 'SAME' P1 = tf.nn.max_pool(value=A1, ksize=[1,8,8,1], strides=[1,8,8,1], padding='SAME') # CONV2D: filters W2, stride 1, padding 'SAME' Z2 = tf.nn.conv2d(input=P1, filter=W2, strides=[1,1,1,1], padding='SAME') # RELU A2 = tf.nn.relu(Z2) # MAXPOOL: window 4x4, stride 4, padding 'SAME' P2 = tf.nn.max_pool(value=A2, ksize=[1,4,4,1], strides=[1,4,4,1], padding='SAME') # FLATTEN P2 = tf.contrib.layers.flatten(P2) # FULLY-CONNECTED without non-linear activation function (not not call softmax). # 6 neurons in output layer. Hint: one of the arguments should be "activation_fn=None" Z3 = tf.contrib.layers.fully_connected(P2, activation_fn=None, num_outputs=6) ### END CODE HERE ### return Z3 1234567891011tf.reset_default_graph()with tf.Session() as sess: np.random.seed(1) X, Y = create_placeholders(64, 64, 3, 6) parameters = initialize_parameters() Z3 = forward_propagation(X, parameters) init = tf.global_variables_initializer() sess.run(init) a = sess.run(Z3, &#123;X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)&#125;) print("Z3 = " + str(a)) Z3 = [[-0.44670227 -1.57208765 -1.53049231 -2.31013036 -1.29104376 0.46852064] [-0.17601591 -1.57972014 -1.4737016 -2.61672091 -1.00810647 0.5747785 ]] Expected Output: Z3 = [[-0.44670227 -1.57208765 -1.53049231 -2.31013036 -1.29104376 0.46852064] [-0.17601591 -1.57972014 -1.4737016 -2.61672091 -1.00810647 0.5747785 ]] 1.3 - Compute costImplement the compute cost function below. You might find these two functions helpful: tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y): computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation here. tf.reduce_mean: computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation here. Exercise: Compute the cost below using the function above. 12345678910111213141516171819# GRADED FUNCTION: compute_cost def compute_cost(Z3, Y): """ Computes the cost Arguments: Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples) Y -- "true" labels vector placeholder, same shape as Z3 Returns: cost - Tensor of the cost function """ ### START CODE HERE ### (1 line of code) cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y)) ### END CODE HERE ### return cost 123456789101112tf.reset_default_graph()with tf.Session() as sess: np.random.seed(1) X, Y = create_placeholders(64, 64, 3, 6) parameters = initialize_parameters() Z3 = forward_propagation(X, parameters) cost = compute_cost(Z3, Y) init = tf.global_variables_initializer() sess.run(init) a = sess.run(cost, &#123;X: np.random.randn(4,64,64,3), Y: np.random.randn(4,6)&#125;) print("cost = " + str(a)) cost = 2.91034 Expected Output: cost = 2.91034 1.4 ModelFinally you will merge the helper functions you implemented above to build a model. You will train it on the SIGNS dataset. You have implemented random_mini_batches() in the Optimization programming assignment of course 2. Remember that this function returns a list of mini-batches. Exercise: Complete the function below. The model below should: create placeholders initialize parameters forward propagate compute the cost create an optimizer Finally you will create a session and run a for loop for num_epochs, get the mini-batches, and then for each mini-batch you will optimize the function. Hint for initializing the variables 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113# GRADED FUNCTION: modeldef model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009, num_epochs = 100, minibatch_size = 64, print_cost = True): """ Implements a three-layer ConvNet in Tensorflow: CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED Arguments: X_train -- training set, of shape (None, 64, 64, 3) Y_train -- test set, of shape (None, n_y = 6) X_test -- training set, of shape (None, 64, 64, 3) Y_test -- test set, of shape (None, n_y = 6) learning_rate -- learning rate of the optimization num_epochs -- number of epochs of the optimization loop minibatch_size -- size of a minibatch print_cost -- True to print the cost every 100 epochs Returns: train_accuracy -- real number, accuracy on the train set (X_train) test_accuracy -- real number, testing accuracy on the test set (X_test) parameters -- parameters learnt by the model. They can then be used to predict. """ ops.reset_default_graph() # to be able to rerun the model without overwriting tf variables tf.set_random_seed(1) # to keep results consistent (tensorflow seed) seed = 3 # to keep results consistent (numpy seed) (m, n_H0, n_W0, n_C0) = X_train.shape n_y = Y_train.shape[1] costs = [] # To keep track of the cost # Create Placeholders of the correct shape ### START CODE HERE ### (1 line) X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y) ### END CODE HERE ### # Initialize parameters ### START CODE HERE ### (1 line) parameters = initialize_parameters() ### END CODE HERE ### # Forward propagation: Build the forward propagation in the tensorflow graph ### START CODE HERE ### (1 line) Z3 = forward_propagation(X, parameters) ### END CODE HERE ### # Cost function: Add cost function to tensorflow graph ### START CODE HERE ### (1 line) cost = compute_cost(Z3, Y) ### END CODE HERE ### # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost. ### START CODE HERE ### (1 line) optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) ### END CODE HERE ### # Initialize all the variables globally init = tf.global_variables_initializer() # Start the session to compute the tensorflow graph with tf.Session() as sess: # Run the initialization sess.run(init) # Do the training loop for epoch in range(num_epochs): minibatch_cost = 0. num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set seed = seed + 1 minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed) for minibatch in minibatches: # Select a minibatch (minibatch_X, minibatch_Y) = minibatch # IMPORTANT: The line that runs the graph on a minibatch. # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y). ### START CODE HERE ### (1 line) _ , temp_cost = sess.run([optimizer, cost], feed_dict=&#123;X:minibatch_X, Y:minibatch_Y&#125;) ### END CODE HERE ### minibatch_cost += temp_cost / num_minibatches # Print the cost every epoch if print_cost == True and epoch % 5 == 0: print ("Cost after epoch %i: %f" % (epoch, minibatch_cost)) if print_cost == True and epoch % 1 == 0: costs.append(minibatch_cost) # plot the cost plt.plot(np.squeeze(costs)) plt.ylabel('cost') plt.xlabel('iterations (per tens)') plt.title("Learning rate =" + str(learning_rate)) plt.show() # Calculate the correct predictions predict_op = tf.argmax(Z3, 1) correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1)) # Calculate accuracy on the test set accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float")) print(accuracy) train_accuracy = accuracy.eval(&#123;X: X_train, Y: Y_train&#125;) test_accuracy = accuracy.eval(&#123;X: X_test, Y: Y_test&#125;) print("Train Accuracy:", train_accuracy) print("Test Accuracy:", test_accuracy) return train_accuracy, test_accuracy, parameters Run the following cell to train your model for 100 epochs. Check if your cost after epoch 0 and 5 matches our output. If not, stop the cell and go back to your code! 1_, _, parameters = model(X_train, Y_train, X_test, Y_test) Cost after epoch 0: 1.917929 Cost after epoch 5: 1.506757 Cost after epoch 10: 0.955359 Cost after epoch 15: 0.845802 Cost after epoch 20: 0.701174 Cost after epoch 25: 0.571977 Cost after epoch 30: 0.518435 Cost after epoch 35: 0.495806 Cost after epoch 40: 0.429827 Cost after epoch 45: 0.407291 Cost after epoch 50: 0.366394 Cost after epoch 55: 0.376922 Cost after epoch 60: 0.299491 Cost after epoch 65: 0.338870 Cost after epoch 70: 0.316400 Cost after epoch 75: 0.310413 Cost after epoch 80: 0.249549 Cost after epoch 85: 0.243457 Cost after epoch 90: 0.200031 Cost after epoch 95: 0.175452 Tensor(&quot;Mean_1:0&quot;, shape=(), dtype=float32) Train Accuracy: 0.940741 Test Accuracy: 0.783333 Expected output: although it may not match perfectly, your expected output should be close to ours and your cost value should decrease. Cost after epoch 0 = 1.917929 Cost after epoch 5 = 1.506757 Train Accuracy = 0.940741 Test Accuracy = 0.783333 Congratulations! You have finised the assignment and built a model that recognizes SIGN language with almost 80% accuracy on the test set. If you wish, feel free to play around with this dataset further. You can actually improve its accuracy by spending more time tuning the hyperparameters, or using regularization (as this model clearly has a high variance). Once again, here’s a thumbs up for your work! 1234fname = "images/thumbs_up.jpg"image = np.array(ndimage.imread(fname, flatten=False))my_image = scipy.misc.imresize(image, size=(64,64))plt.imshow(my_image)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Machine Learning</tag>
        <tag>Deep Learning</tag>
        <tag>Convolutional Neural Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning Course3 Week1 Programming Assignment1]]></title>
    <url>%2F2018%2F08%2F15%2F2018-08-15Convolution%20model%20-%20Step%20by%20Step%20-%20v2%2F</url>
    <content type="text"><![CDATA[Convolutional Neural Networks: Step by StepWelcome to Course 4’s first assignment! In this assignment, you will implement convolutional (CONV) and pooling (POOL) layers in numpy, including both forward propagation and (optionally) backward propagation. Notation: Superscript $[l]$ denotes an object of the $l^{th}$ layer. Example: $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters. Superscript $(i)$ denotes an object from the $i^{th}$ example. Example: $x^{(i)}$ is the $i^{th}$ training example input. Lowerscript $i$ denotes the $i^{th}$ entry of a vector. Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$, assuming this is a fully connected (FC) layer. $n_H$, $n_W$ and $n_C$ denote respectively the height, width and number of channels of a given layer. If you want to reference a specific layer $l$, you can also write $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$. $n_{H_{prev}}$, $n_{W_{prev}}$ and $n_{C_{prev}}$ denote respectively the height, width and number of channels of the previous layer. If referencing a specific layer $l$, this could also be denoted $n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$. We assume that you are already familiar with numpy and/or have completed the previous courses of the specialization. Let’s get started! 1 - PackagesLet’s first import all the packages that you will need during this assignment. numpy is the fundamental package for scientific computing with Python. matplotlib is a library to plot graphs in Python. np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. 12345678910111213import numpy as npimport h5pyimport matplotlib.pyplot as plt%matplotlib inlineplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'%load_ext autoreload%autoreload 2np.random.seed(1) 2 - Outline of the AssignmentYou will be implementing the building blocks of a convolutional neural network! Each function you will implement will have detailed instructions that will walk you through the steps needed: Convolution functions, including: Zero Padding Convolve window Convolution forward Convolution backward (optional) Pooling functions, including: Pooling forward Create mask Distribute value Pooling backward (optional) This notebook will ask you to implement these functions from scratch in numpy. In the next notebook, you will use the TensorFlow equivalents of these functions to build the following model: Note that for every forward function, there is its corresponding backward equivalent. Hence, at every step of your forward module you will store some parameters in a cache. These parameters are used to compute gradients during backpropagation. 3 - Convolutional Neural NetworksAlthough programming frameworks make convolutions easy to use, they remain one of the hardest concepts to understand in Deep Learning. A convolution layer transforms an input volume into an output volume of different size, as shown below. In this part, you will build every step of the convolution layer. You will first implement two helper functions: one for zero padding and the other for computing the convolution function itself. 3.1 - Zero-PaddingZero-padding adds zeros around the border of an image: Figure 1 : Zero-Padding Image (3 channels, RGB) with a padding of 2. The main benefits of padding are the following: It allows you to use a CONV layer without necessarily shrinking the height and width of the volumes. This is important for building deeper networks, since otherwise the height/width would shrink as you go to deeper layers. An important special case is the “same” convolution, in which the height/width is exactly preserved after one layer. It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels as the edges of an image. Exercise: Implement the following function, which pads all the images of a batch of examples X with zeros. Use np.pad. Note if you want to pad the array “a” of shape $(5,5,5,5,5)$ with pad = 1 for the 2nd dimension, pad = 3 for the 4th dimension and pad = 0 for the rest, you would do:1a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), 'constant', constant_values = (..,..)) 1234567891011121314151617181920# GRADED FUNCTION: zero_paddef zero_pad(X, pad): """ Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, as illustrated in Figure 1. Argument: X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images pad -- integer, amount of padding around each image on vertical and horizontal dimensions Returns: X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C) """ ### START CODE HERE ### (≈ 1 line) X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant') ### END CODE HERE ### return X_pad 12345678910111213np.random.seed(1)x = np.random.randn(4, 3, 3, 2)x_pad = zero_pad(x, 2)print ("x.shape =", x.shape)print ("x_pad.shape =", x_pad.shape)print ("x[1,1] =", x[1,1])print ("x_pad[1,1] =", x_pad[1,1])fig, axarr = plt.subplots(1, 2)axarr[0].set_title('x')axarr[0].imshow(x[0,:,:,0])axarr[1].set_title('x_pad')axarr[1].imshow(x_pad[0,:,:,0]) x.shape = (4, 3, 3, 2) x_pad.shape = (4, 7, 7, 2) x[1,1] = [[ 0.90085595 -0.68372786] [-0.12289023 -0.93576943] [-0.26788808 0.53035547]] x_pad[1,1] = [[ 0. 0.] [ 0. 0.] [ 0. 0.] [ 0. 0.] [ 0. 0.] [ 0. 0.] [ 0. 0.]] &lt;matplotlib.image.AxesImage at 0x7f270e22e860&gt; Expected Output: x.shape: (4, 3, 3, 2) x_pad.shape: (4, 7, 7, 2) x[1,1]: [[ 0.90085595 -0.68372786] [-0.12289023 -0.93576943] [-0.26788808 0.53035547]] x_pad[1,1]: [[ 0. 0.] [ 0. 0.] [ 0. 0.] [ 0. 0.] [ 0. 0.] [ 0. 0.] [ 0. 0.]] 3.2 - Single step of convolutionIn this part, implement a single step of convolution, in which you apply the filter to a single position of the input. This will be used to build a convolutional unit, which: Takes an input volume Applies a filter at every position of the input Outputs another volume (usually of different size) Figure 2 : Convolution operation with a filter of 2x2 and a stride of 1 (stride = amount you move the window each time you slide) In a computer vision application, each value in the matrix on the left corresponds to a single pixel value, and we convolve a 3x3 filter with the image by multiplying its values element-wise with the original matrix, then summing them up and adding a bias. In this first step of the exercise, you will implement a single step of convolution, corresponding to applying a filter to just one of the positions to get a single real-valued output. Later in this notebook, you’ll apply this function to multiple positions of the input to implement the full convolutional operation. Exercise: Implement conv_single_step(). Hint. 1234567891011121314151617181920212223242526# GRADED FUNCTION: conv_single_stepdef conv_single_step(a_slice_prev, W, b): """ Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation of the previous layer. Arguments: a_slice_prev -- slice of input data of shape (f, f, n_C_prev) W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev) b -- Bias parameters contained in a window - matrix of shape (1, 1, 1) Returns: Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data """ ### START CODE HERE ### (≈ 2 lines of code) # Element-wise product between a_slice and W. Do not add the bias yet. s = a_slice_prev * W # Sum over all entries of the volume s. Z = np.sum(s) # Add bias b to Z. Cast b to a float() so that Z results in a scalar value. Z = Z + float(b) ### END CODE HERE ### return Z 1234567np.random.seed(1)a_slice_prev = np.random.randn(4, 4, 3)W = np.random.randn(4, 4, 3)b = np.random.randn(1, 1, 1)Z = conv_single_step(a_slice_prev, W, b)print("Z =", Z) Z = -6.99908945068 Expected Output: Z -6.99908945068 3.3 - Convolutional Neural Networks - Forward passIn the forward pass, you will take many filters and convolve them on the input. Each ‘convolution’ gives you a 2D matrix output. You will then stack these outputs to get a 3D volume: Exercise: Implement the function below to convolve the filters W on an input activation A_prev. This function takes as input A_prev, the activations output by the previous layer (for a batch of m inputs), F filters/weights denoted by W, and a bias vector denoted by b, where each filter has its own (single) bias. Finally you also have access to the hyperparameters dictionary which contains the stride and the padding. Hint: To select a 2x2 slice at the upper left corner of a matrix “a_prev” (shape (5,5,3)), you would do:1a_slice_prev = a_prev[0:2,0:2,:] This will be useful when you will define a_slice_prev below, using the start/end indexes you will define. To define a_slice you will need to first define its corners vert_start, vert_end, horiz_start and horiz_end. This figure may be helpful for you to find how each of the corner can be defined using h, w, f and s in the code below. Figure 3 : Definition of a slice using vertical and horizontal start/end (with a 2x2 filter) This figure shows only a single channel. Reminder:The formulas relating the output shape of the convolution to the input shape is:$$ n_H = \lfloor \frac{n_{H_{prev}} - f + 2 \times pad}{stride} \rfloor +1 $$$$ n_W = \lfloor \frac{n_{W_{prev}} - f + 2 \times pad}{stride} \rfloor +1 $$$$ n_C = \text{number of filters used in the convolution}$$ For this exercise, we won’t worry about vectorization, and will just implement everything with for-loops. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# GRADED FUNCTION: conv_forwarddef conv_forward(A_prev, W, b, hparameters): """ Implements the forward propagation for a convolution function Arguments: A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev) W -- Weights, numpy array of shape (f, f, n_C_prev, n_C) b -- Biases, numpy array of shape (1, 1, 1, n_C) hparameters -- python dictionary containing "stride" and "pad" Returns: Z -- conv output, numpy array of shape (m, n_H, n_W, n_C) cache -- cache of values needed for the conv_backward() function """ ### START CODE HERE ### # Retrieve dimensions from A_prev's shape (≈1 line) (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape # Retrieve dimensions from W's shape (≈1 line) (f, f, n_C_prev, n_C) = W.shape # Retrieve information from "hparameters" (≈2 lines) stride = hparameters["stride"] pad = hparameters["pad"] # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines) n_H = int((n_H_prev - f + 2 * pad) / stride) + 1 n_W = int((n_W_prev - f + 2 * pad) / stride) + 1 # Initialize the output volume Z with zeros. (≈1 line) Z = np.zeros((m, n_H, n_W, n_C)) # Create A_prev_pad by padding A_prev A_prev_pad = zero_pad(A_prev, pad) for i in range(m): # loop over the batch of training examples a_prev_pad = A_prev_pad[i] # Select ith training example's padded activation for h in range(n_H): # loop over vertical axis of the output volume for w in range(n_W): # loop over horizontal axis of the output volume for c in range(n_C): # loop over channels (= #filters) of the output volume # Find the corners of the current "slice" (≈4 lines) vert_start = h * stride vert_end = vert_start + f horiz_start = w * stride horiz_end = horiz_start + f # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line) a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line) Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :,c], b[:, :, :, c]) ### END CODE HERE ### # Making sure your output shape is correct assert(Z.shape == (m, n_H, n_W, n_C)) # Save information in "cache" for the backprop cache = (A_prev, W, b, hparameters) return Z, cache 1234567891011np.random.seed(1)A_prev = np.random.randn(10,4,4,3)W = np.random.randn(2,2,3,8)b = np.random.randn(1,1,1,8)hparameters = &#123;"pad" : 2, "stride": 2&#125;Z, cache_conv = conv_forward(A_prev, W, b, hparameters)print("Z's mean =", np.mean(Z))print("Z[3,2,1] =", Z[3,2,1])print("cache_conv[0][1][2][3] =", cache_conv[0][1][2][3]) Z&apos;s mean = 0.0489952035289 Z[3,2,1] = [-0.61490741 -6.7439236 -2.55153897 1.75698377 3.56208902 0.53036437 5.18531798 8.75898442] cache_conv[0][1][2][3] = [-0.20075807 0.18656139 0.41005165] Expected Output: Z’s mean 0.0489952035289 Z[3,2,1] [-0.61490741 -6.7439236 -2.55153897 1.75698377 3.56208902 0.53036437 5.18531798 8.75898442] cache_conv[0][1][2][3] [-0.20075807 0.18656139 0.41005165] Finally, CONV layer should also contain an activation, in which case we would add the following line of code: 1234# Convolve the window to get back one output neuronZ[i, h, w, c] = ...# Apply activationA[i, h, w, c] = activation(Z[i, h, w, c]) You don’t need to do it here. 4 - Pooling layerThe pooling (POOL) layer reduces the height and width of the input. It helps reduce computation, as well as helps make feature detectors more invariant to its position in the input. The two types of pooling layers are: Max-pooling layer: slides an ($f, f$) window over the input and stores the max value of the window in the output. Average-pooling layer: slides an ($f, f$) window over the input and stores the average value of the window in the output. These pooling layers have no parameters for backpropagation to train. However, they have hyperparameters such as the window size $f$. This specifies the height and width of the fxf window you would compute a max or average over. 4.1 - Forward PoolingNow, you are going to implement MAX-POOL and AVG-POOL, in the same function. Exercise: Implement the forward pass of the pooling layer. Follow the hints in the comments below. Reminder:As there’s no padding, the formulas binding the output shape of the pooling to the input shape is:$$ n_H = \lfloor \frac{n_{H_{prev}} - f}{stride} \rfloor +1 $$$$ n_W = \lfloor \frac{n_{W_{prev}} - f}{stride} \rfloor +1 $$$$ n_C = n_{C_{prev}}$$ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# GRADED FUNCTION: pool_forwarddef pool_forward(A_prev, hparameters, mode = "max"): """ Implements the forward pass of the pooling layer Arguments: A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev) hparameters -- python dictionary containing "f" and "stride" mode -- the pooling mode you would like to use, defined as a string ("max" or "average") Returns: A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C) cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters """ # Retrieve dimensions from the input shape (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape # Retrieve hyperparameters from "hparameters" f = hparameters["f"] stride = hparameters["stride"] # Define the dimensions of the output n_H = int(1 + (n_H_prev - f) / stride) n_W = int(1 + (n_W_prev - f) / stride) n_C = n_C_prev # Initialize output matrix A A = np.zeros((m, n_H, n_W, n_C)) ### START CODE HERE ### for i in range(m): # loop over the training examples for h in range(n_H): # loop on the vertical axis of the output volume for w in range(n_W): # loop on the horizontal axis of the output volume for c in range (n_C): # loop over the channels of the output volume # Find the corners of the current "slice" (≈4 lines) vert_start = h * stride vert_end = vert_start + f horiz_start = w * stride horiz_end = w * stride + f # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line) a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean. if mode == "max": A[i, h, w, c] = np.max(a_prev_slice) elif mode == "average": A[i, h, w, c] = np.mean(a_prev_slice) ### END CODE HERE ### # Store the input and hparameters in "cache" for pool_backward() cache = (A_prev, hparameters) # Making sure your output shape is correct assert(A.shape == (m, n_H, n_W, n_C)) return A, cache 1234567891011np.random.seed(1)A_prev = np.random.randn(2, 4, 4, 3)hparameters = &#123;"stride" : 2, "f": 3&#125;A, cache = pool_forward(A_prev, hparameters)print("mode = max")print("A =", A)print()A, cache = pool_forward(A_prev, hparameters, mode = "average")print("mode = average")print("A =", A) mode = max A = [[[[ 1.74481176 0.86540763 1.13376944]]] [[[ 1.13162939 1.51981682 2.18557541]]]] mode = average A = [[[[ 0.02105773 -0.20328806 -0.40389855]]] [[[-0.22154621 0.51716526 0.48155844]]]] Expected Output: A = [[[[ 1.74481176 0.86540763 1.13376944]]] [[[ 1.13162939 1.51981682 2.18557541]]]] A = [[[[ 0.02105773 -0.20328806 -0.40389855]]] [[[-0.22154621 0.51716526 0.48155844]]]] Congratulations! You have now implemented the forward passes of all the layers of a convolutional network. The remainer of this notebook is optional, and will not be graded. 5 - Backpropagation in convolutional neural networks (OPTIONAL / UNGRADED)In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers don’t need to bother with the details of the backward pass. The backward pass for convolutional networks is complicated. If you wish however, you can work through this optional portion of the notebook to get a sense of what backprop in a convolutional network looks like. When in an earlier course you implemented a simple (fully connected) neural network, you used backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in convolutional neural networks you can to calculate the derivatives with respect to the cost in order to update the parameters. The backprop equations are not trivial and we did not derive them in lecture, but we briefly presented them below. 5.1 - Convolutional layer backward passLet’s start by implementing the backward pass for a CONV layer. 5.1.1 - Computing dA:This is the formula for computing $dA$ with respect to the cost for a certain filter $W_c$ and a given training example: $$ dA += \sum _{h=0} ^{n_H} \sum_{w=0} ^{n_W} W_c \times dZ_{hw} \tag{1}$$ Where $W_c$ is a filter and $dZ_{hw}$ is a scalar corresponding to the gradient of the cost with respect to the output of the conv layer Z at the hth row and wth column (corresponding to the dot product taken at the ith stride left and jth stride down). Note that at each time, we multiply the the same filter $W_c$ by a different dZ when updating dA. We do so mainly because when computing the forward propagation, each filter is dotted and summed by a different a_slice. Therefore when computing the backprop for dA, we are just adding the gradients of all the a_slices. In code, inside the appropriate for-loops, this formula translates into:1da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c] 5.1.2 - Computing dW:This is the formula for computing $dW_c$ ($dW_c$ is the derivative of one filter) with respect to the loss: $$ dW_c += \sum _{h=0} ^{n_H} \sum_{w=0} ^ {n_W} a_{slice} \times dZ_{hw} \tag{2}$$ Where $a_{slice}$ corresponds to the slice which was used to generate the acitivation $Z_{ij}$. Hence, this ends up giving us the gradient for $W$ with respect to that slice. Since it is the same $W$, we will just add up all such gradients to get $dW$. In code, inside the appropriate for-loops, this formula translates into:1dW[:,:,:,c] += a_slice * dZ[i, h, w, c] 5.1.3 - Computing db:This is the formula for computing $db$ with respect to the cost for a certain filter $W_c$: $$ db = \sum_h \sum_w dZ_{hw} \tag{3}$$ As you have previously seen in basic neural networks, db is computed by summing $dZ$. In this case, you are just summing over all the gradients of the conv output (Z) with respect to the cost. In code, inside the appropriate for-loops, this formula translates into:1db[:,:,:,c] += dZ[i, h, w, c] Exercise: Implement the conv_backward function below. You should sum over all the training examples, filters, heights, and widths. You should then compute the derivatives using formulas 1, 2 and 3 above. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475def conv_backward(dZ, cache): """ Implement the backward propagation for a convolution function Arguments: dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C) cache -- cache of values needed for the conv_backward(), output of conv_forward() Returns: dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev), numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev) dW -- gradient of the cost with respect to the weights of the conv layer (W) numpy array of shape (f, f, n_C_prev, n_C) db -- gradient of the cost with respect to the biases of the conv layer (b) numpy array of shape (1, 1, 1, n_C) """ ### START CODE HERE ### # Retrieve information from "cache" (A_prev, W, b, hparameters) = None # Retrieve dimensions from A_prev's shape (m, n_H_prev, n_W_prev, n_C_prev) = None # Retrieve dimensions from W's shape (f, f, n_C_prev, n_C) = None # Retrieve information from "hparameters" stride = None pad = None # Retrieve dimensions from dZ's shape (m, n_H, n_W, n_C) = None # Initialize dA_prev, dW, db with the correct shapes dA_prev = None dW = None db = None # Pad A_prev and dA_prev A_prev_pad = None dA_prev_pad = None for i in range(None): # loop over the training examples # select ith training example from A_prev_pad and dA_prev_pad a_prev_pad = None da_prev_pad = None for h in range(None): # loop over vertical axis of the output volume for w in range(None): # loop over horizontal axis of the output volume for c in range(None): # loop over the channels of the output volume # Find the corners of the current "slice" vert_start = None vert_end = None horiz_start = None horiz_end = None # Use the corners to define the slice from a_prev_pad a_slice = None # Update gradients for the window and the filter's parameters using the code formulas given above da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += None dW[:,:,:,c] += None db[:,:,:,c] += None # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :]) dA_prev[i, :, :, :] = None ### END CODE HERE ### # Making sure your output shape is correct assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev)) return dA_prev, dW, db 12345np.random.seed(1)dA, dW, db = conv_backward(Z, cache_conv)print("dA_mean =", np.mean(dA))print("dW_mean =", np.mean(dW))print("db_mean =", np.mean(db)) Expected Output: dA_mean 1.45243777754 dW_mean 1.72699145831 db_mean 7.83923256462 5.2 Pooling layer - backward passNext, let’s implement the backward pass for the pooling layer, starting with the MAX-POOL layer. Even though a pooling layer has no parameters for backprop to update, you still need to backpropagation the gradient through the pooling layer in order to compute gradients for layers that came before the pooling layer. 5.2.1 Max pooling - backward passBefore jumping into the backpropagation of the pooling layer, you are going to build a helper function called create_mask_from_window() which does the following: $$ X = \begin{bmatrix}1 &amp;&amp; 3 \4 &amp;&amp; 2\end{bmatrix} \quad \rightarrow \quad M =\begin{bmatrix}0 &amp;&amp; 0 \1 &amp;&amp; 0\end{bmatrix}\tag{4}$$ As you can see, this function creates a “mask” matrix which keeps track of where the maximum of the matrix is. True (1) indicates the position of the maximum in X, the other entries are False (0). You’ll see later that the backward pass for average pooling will be similar to this but using a different mask. Exercise: Implement create_mask_from_window(). This function will be helpful for pooling backward.Hints: np.max() may be helpful. It computes the maximum of an array. If you have a matrix X and a scalar x: A = (X == x) will return a matrix A of the same size as X such that: 12A[i,j] = True if X[i,j] = xA[i,j] = False if X[i,j] != x Here, you don’t need to consider cases where there are several maxima in a matrix. 12345678910111213141516def create_mask_from_window(x): """ Creates a mask from an input matrix x, to identify the max entry of x. Arguments: x -- Array of shape (f, f) Returns: mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x. """ ### START CODE HERE ### (≈1 line) mask = None ### END CODE HERE ### return mask 12345np.random.seed(1)x = np.random.randn(2,3)mask = create_mask_from_window(x)print('x = ', x)print("mask = ", mask) Expected Output: x =[[ 1.62434536 -0.61175641 -0.52817175] [-1.07296862 0.86540763 -2.3015387 ]] mask =[[ True False False] [False False False]] Why do we keep track of the position of the max? It’s because this is the input value that ultimately influenced the output, and therefore the cost. Backprop is computing gradients with respect to the cost, so anything that influences the ultimate cost should have a non-zero gradient. So, backprop will “propagate” the gradient back to this particular input value that had influenced the cost. 5.2.2 - Average pooling - backward passIn max pooling, for each input window, all the “influence” on the output came from a single input value–the max. In average pooling, every element of the input window has equal influence on the output. So to implement backprop, you will now implement a helper function that reflects this. For example if we did average pooling in the forward pass using a 2x2 filter, then the mask you’ll use for the backward pass will look like:$$ dZ = 1 \quad \rightarrow \quad dZ =\begin{bmatrix}1/4 &amp;&amp; 1/4 \1/4 &amp;&amp; 1/4\end{bmatrix}\tag{5}$$ This implies that each position in the $dZ$ matrix contributes equally to output because in the forward pass, we took an average. Exercise: Implement the function below to equally distribute a value dz through a matrix of dimension shape. Hint 123456789101112131415161718192021222324def distribute_value(dz, shape): """ Distributes the input value in the matrix of dimension shape Arguments: dz -- input scalar shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz Returns: a -- Array of size (n_H, n_W) for which we distributed the value of dz """ ### START CODE HERE ### # Retrieve dimensions from shape (≈1 line) (n_H, n_W) = None # Compute the value to distribute on the matrix (≈1 line) average = None # Create a matrix where every entry is the "average" value (≈1 line) a = None ### END CODE HERE ### return a 12a = distribute_value(2, (2,2))print('distributed value =', a) Expected Output: distributed_value =[[ 0.5 0.5]&lt;br>[ 0.5 0.5]] 5.2.3 Putting it together: Pooling backwardYou now have everything you need to compute backward propagation on a pooling layer. Exercise: Implement the pool_backward function in both modes (&quot;max&quot; and &quot;average&quot;). You will once again use 4 for-loops (iterating over training examples, height, width, and channels). You should use an if/elif statement to see if the mode is equal to &#39;max&#39; or &#39;average&#39;. If it is equal to ‘average’ you should use the distribute_value() function you implemented above to create a matrix of the same shape as a_slice. Otherwise, the mode is equal to ‘max‘, and you will create a mask with create_mask_from_window() and multiply it by the corresponding value of dZ. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869def pool_backward(dA, cache, mode = "max"): """ Implements the backward pass of the pooling layer Arguments: dA -- gradient of cost with respect to the output of the pooling layer, same shape as A cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters mode -- the pooling mode you would like to use, defined as a string ("max" or "average") Returns: dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev """ ### START CODE HERE ### # Retrieve information from cache (≈1 line) (A_prev, hparameters) = None # Retrieve hyperparameters from "hparameters" (≈2 lines) stride = None f = None # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines) m, n_H_prev, n_W_prev, n_C_prev = None m, n_H, n_W, n_C = None # Initialize dA_prev with zeros (≈1 line) dA_prev = None for i in range(None): # loop over the training examples # select training example from A_prev (≈1 line) a_prev = None for h in range(None): # loop on the vertical axis for w in range(None): # loop on the horizontal axis for c in range(None): # loop over the channels (depth) # Find the corners of the current "slice" (≈4 lines) vert_start = None vert_end = None horiz_start = None horiz_end = None # Compute the backward propagation in both modes. if mode == "max": # Use the corners and "c" to define the current slice from a_prev (≈1 line) a_prev_slice = None # Create the mask from a_prev_slice (≈1 line) mask = None # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line) dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += None elif mode == "average": # Get the value a from dA (≈1 line) da = None # Define the shape of the filter as fxf (≈1 line) shape = None # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line) dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += None ### END CODE ### # Making sure your output shape is correct assert(dA_prev.shape == A_prev.shape) return dA_prev 123456789101112131415np.random.seed(1)A_prev = np.random.randn(5, 5, 3, 2)hparameters = &#123;"stride" : 1, "f": 2&#125;A, cache = pool_forward(A_prev, hparameters)dA = np.random.randn(5, 4, 2, 2)dA_prev = pool_backward(dA, cache, mode = "max")print("mode = max")print('mean of dA = ', np.mean(dA))print('dA_prev[1,1] = ', dA_prev[1,1]) print()dA_prev = pool_backward(dA, cache, mode = "average")print("mode = average")print('mean of dA = ', np.mean(dA))print('dA_prev[1,1] = ', dA_prev[1,1]) Expected Output: mode = max: mean of dA =0.145713902729 dA_prev[1,1] =[[ 0. 0. ] [ 5.05844394 -1.68282702] [ 0. 0. ]] mode = average mean of dA =0.145713902729 dA_prev[1,1] =[[ 0.08485462 0.2787552 ] [ 1.26461098 -0.25749373] [ 1.17975636 -0.53624893]] Congratulations !Congratulation on completing this assignment. You now understand how convolutional neural networks work. You have implemented all the building blocks of a neural network. In the next assignment you will implement a ConvNet using TensorFlow.]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GRE AWA MODEL ESSAYS]]></title>
    <url>%2F2018%2F08%2F09%2F2018-08-09-GRE-AWA-MODEL-ESSAYS-ISSUE-1%2F</url>
    <content type="text"><![CDATA[Issue 1“We can usually learn much more from people whose views we share than from people whose views contradict our own.”; disagreement can cause stress and inhibit learning.” Essay&emsp;&emsp;Do we learn more from people whose ideas we share in common than from those whose ideas contradict ours? The speaker deems so, for the reason that disagreement can cause stress and inhibit learning. I concede that undue discord can impede learning. Otherwise, in my view we learn far more from discourse and debate with those whose ideas we oppose than from people whose ideas are in accord with our own. 第一段：作者复述了一下题目中的问题。随后表达了自己的观点，但我们可以看出，作者并不是完全赞同或完全反对一种观点，而是说“我承认争论会阻碍学习，但在另一方面，我认为从观点不同的人那里会学到更多的东西。”这虽然是一边倒的写法，但是倒的不严重，从下文我们也可以看出，作者并不是一直在说“we can learn more from opponents”，而是先用两段的内容写了”debate”的坏处，再用两段写了自己依然认为”we can learn more from people with different views”。这好像是介于“一边倒”与“折中式”的一种论证过程。 首先，“Do we learn more from people whose ideas we share in common than from those whose ideas contradict ours? The speaker deems so, for the reason that disagreement can cause stress and inhibit learning.”这基本就是照抄了题目，为了引出话题。我们可以学习一下这个用法“The speaker deems so, for the reason that…”。 其次作者用两句话分别说明了自己的两个主要观点。第一个“I concede that undue discord can impede learning.”作者说“我承认过度的争吵可以阻碍学习。”这里作者很聪明，表达的观点不是“discord impedes learning”而是”undue discord impedes learning”，下面两段也都是在说“undue discord”为什么会”impede learning”。第二个“Otherwise, in my view we learn far from discourse and debate with those whose ideas we oppose than from those whose ideas are in accord with our own.”说明作者实际上，就是认为“discourse and debate facilitate(help) learning”。(这里的otherwise我不知道具体该怎么理解。) 所以这样看来，虽然作者貌似是偏向于“折中式”，但实际上还是“一边倒”的写法，之前我对文章的理解有误，因为不认识undue是什么意思，所以单词很重要啊。作者也给我们提供了一个很好的写作方法，如果素材不够，我们不一定要一直说某一方面的好，也可以说另一方面有多差，或者另一方面在很极端的情况下就会变的差了。其实我们阅读下文论述也可以知道，作者想表达“undue discord can impede learning”但是“rational and reasonable debate help learning”的意思。可见文章的大局观，从第一段便确定了文章的论述内容。第1段引出论题，确立观点。第2、3段“admit that undue discord impede learning”，第4、5段细说“rational and reasonable debate facilitate(help) learning”，第6段summary。文章写作思路清晰，之后写也好写，该写啥就写啥就行了。 contradict v. 与…对立（这个单词我认识，我只知道它是“对立”的意思，但我从来不知道怎么用这个词，要不要加介词，加什么介词，所以尽管认识这个单词，但是从来也不会使用它，所以今后在学习过程中要积累，记住单词的用法，不仅仅是在文章中能看懂，也要将其使用到自己的文章当中） contradict 这个词的用法很简单，A contradict B，A与B对立 eg. Your actions contradict your words. share in common 以后可以用这个短语来表达立场相同的人”people whose ideas we share in common” concede v. 不情愿的承认 to admit sth in an unwillingly way undue adj. 过度的 more than is reasonable or necessary discord n. 不和，争吵 impede v. 阻碍；妨碍 discourse n. 交谈（这个单词单纯的指两个人的交谈，并没有争论的意思）: the use of word to exchange thoughts and ideas in accord with：accord n. 一致；符合：a formal or official agreement，所以我们又学习了一种说法表示“立场相同的人”：people whose ideas are in accord with our own &emsp;&emsp;Admittedly, under some circumstances disagreement with others can be counterproductive to learning. For supporting examples one need look no further than a television set. On today’s typical television or radio talk show, disagreement usually manifests itself in meaningless rhetorical bouts and shouting matches, during which opponents vie to have their own message heard, but have little interest either in finding common ground with or in acknowledging the merits of the opponent’s viewpoint. Understandably, neither the combatants nor the viewers learn anything meaningful. In fact, these battles only serve to reinforce the predispositions and biases of all concerned. The end result is that learning is impeded. 第二段: 开头“Admittedly(不可否认的)”对应第一段的“concede: unwilling to agree”。“Admittedly, under some circumstances disagreement with others can be counterproductive to learning.”也就是第一段的“undue(under some circumstances) discord can impede learning”。第一句话表达本段的论点，接下来开始举example。”For supporting examples one need to look no further than a television set.”这里最值得我们学习的地方是，我们习惯用“for example / for instance”来举例子，而这里作者使用的是“For supporting examples”这里虽然只是加了一个supporting，但是却脱颖而出，不是大众式的“for example”，与之类似，我们可以在以后的写作中写“For supporting examples, …”, “For supporting instance, …”。“no further than”后面接一个谁都知道的example，所以以后觉得自己举的example太common太simple，就可以用“For supporting examples, one need to look no further than (something common and simple)”，小事折射出大道理。“On today’s typical television or radio talk show, disagreement usually manifest itself in meaningless rhetorical bouts and shouting matches, during which opponents vie to have their own message heard, but have little interest either in finding common ground with or in acknowledging the merits of the opponent’s viewpoint.”这句话比较长，但是结构不难理解，在如今的“typical television and radio talk show”中，人们只是在不停的说自己的观点(opponents vie to have their own message heard)，完全不顾别人的观点(have little interest either in finding common ground with or in acknowledging the merits of the opponents’ viewpoints)，这也是当今社会的一大问题，我们都知道，辩论赛很重要的一个环节就是双方都要先确定好某一个基础(common ground)，咱俩在这个基础之上，再辩论。然而当今社会大家简直是胡乱一气，我说我的，你说你的，他说他的(meaningless rhetorical bouts and shouting matches)，这样的交流简直毫无意义“meaningless”。”Understandably, neither the combatants nor the viewers learn anything meaningful. In fact, there battles only serve to reinforce the predispositions and biases of all concerned. The end result is that learning is impeded.”最后结尾，无论是combatants还是viewers都不能从中受益。这种battles只会加深双方的不和predispositions and biases。最后简单一句话直扣第一段的观点“The end result is that learning is impeded.” manifest v. 使显现，使显露，表明: to show something clearly, especially a feeling, an attitude or a quality rhetorical adj. 语言上的，文字上的，辞藻华丽（却往往无内涵的） bout n. 一回合，较量，一阵 vie v. 竞争：to compete with others in an attempt to get or win something common ground 一致之处，共同点：a basis agreed to by all parties for reaching a mutual understanding combatant n. 战士；好斗者（combat n. 战斗） predisposition n. 倾向 &emsp;&emsp;Disagreement can also inhibit learning when two opponents disagree on fundamental assumptions needed for meaningful discourse and debate. For example, a student of paleontology learns little about the evolution of an animal species under current study by debating with an individual whose religious belief system precludes the possibility of evolution to begin with. And, economics and finance students learn little about the dynamics of a laissez-faire system by debating with a socialist whose view is that a centralized power should control all economic activity. 第三段: 之前看首段后对文章内容的猜测再一次出现了错误，这就是“一边倒”式的文章，我还以为前期会缓和一下。那么接着看第三段如何说明“undue discord can impede learning”。”Disagreement can also inhibit learning when two opponents disagreement on fundamental assumptions needed for meaningful discourse and debate.”这与我们之前说的一致，一个”rational, reasonable, meaningful”的“discourse and debate”必须要有”agreement on fundamental assumptions”。”For example, a student of paleontology learns little about the evolution of an animal species under current study by debating with an individual whose religious belief system precludes the possibility of evolution to begin with.”这个例子很生动形象，一个paleontology专业的人和一个不信仰进化论的人debate前沿的动物种类问题，那个paleontology的哥们能学到啥呢，啥也学不到。这就说明了opponents有一个agreement on fundamental assumptions是多么重要。“And, economics and finance students learn little about the dynamics of a laissez-faire system be debating with a socialist whose view is that a centralized power should control all economic activities.”这里举了另一个形象的例子，经济学的学生们和一个认为“a centralized power should control all economic activities”的政治学家辩论，他们能学到自由市场的活泼性、多样性（dynamics of a laissez-faire system）吗，不能。第三段主要举了两个例子来阐述观点：没有agreements on fundamental assumptions，那么只能导致debate impedes learning的后果了。 paleontology n. 古生物学（这单词，记吧，没别的招，多看几遍就记住了） preclude v. 排除 laissez-faire n. 自由放任政策；放任主义（我个人的理解是指完全放开的市场经济，政府不加以干涉） &emsp;&emsp;Aside from the foregoing two provisos, however, I fundamentally disagree with the speaker’s claim. Assuming common ground between two rational and reasonable opponents willing to debate on intellectual merits, both opponents stand to gain much from that debate. Indeed it is primarily through such debate that human knowledge advances, whether at the personal, community, or global level. 第四段: 从这段开始，文章开始转折。前面都在说“debate impedes learning”，但作者的观点是“debate facilitates(helps) learning”啊。那么现在，就开始转折吧。“Aside from the foregoing two provisions, however, I fundamentally disagree with the speaker’s claim.”然而，除了前面所述的两点，我非常同意“debate facilitates(helps) learning”这一说法。这里我们可以学习一下文章结构的转换，前面写了“(undue) discourse and debate impede learning”现在要写“(rational and reasonable) debate facilitates(helps) learning”，怎么转折呢，就用“Aside from the foregoing two provisions, however, I fundamentally agree with blah blah…”。“Assuming common ground between two rational and reasonable opponents willing to debate on intellectual merits, both opponents stand to gain much from that debate.”与前文形成对比，假设现在有common ground了，那么双方opponents都会gain much from that debate。”Indeed it is primarily through such debate that human knowledge advances, whether at the personal, community or, global level.”这里说的很精彩，正是通过这样的辩论，人类知识才得以进步，无论从个人，社会还是国家层面(whether at personal, community, or global level)。这句话很值得记住，可以在很多题材的文章中升华主题。“Indeed it is primarily through such debate that human knowledge advances, whether at personal, community, or global level.” foregoing adj. 前面的；先前的；前述的 provision n. 条款 &emsp;&emsp;At the personal level, by listening to their parents’ rationale for their seemingly oppressive rules and policies teenagers can learn how certain behaviors naturally carry certain undesirable consequences. At the same time, by listening to their teenagers concerns about autonomy and about peer pressures parents can learn the valuable lesson that effective parenting and control are two different things. At the community level, through dispassionate dialogue an environmental activist can come to understand the legitimate economic concerns of those whose jobs depend on the continued profitable operation of a factory. Conversely, the latter might stand to learn much about the potential public health price to be paid by ensuring job growth and a low unemployment rate. Finally, at the global level, two nations with opposing political or economic interests can reach mutually beneficial agreements by striving to understand the other’s legitimate concerns for its national security, its political sovereignty, the stability of its economy and currency, and so forth. 第五段：这一段实为精彩。分别从个人层面（personal level），社会层面（community level），和国家层面（global level）阐述了“rational and reasonable debate”的重要性和益处。首先是个人层面”At the personal level, by listening to their parents’ rationale for their seemingly oppressive rules and policies, teenagers can learn how certain behaviors naturally carry out undesirable consequences. At the same time, by listening to their teenagers concerns about autonomy and peer pressure, parents can learn the valuable lesson that effective parenting and control are two different things.”这里从个人层面（personal level），写了孩子与家长的矛盾与不和，双方都会从中学到东西。孩子从“parents’ rationale for seemingly oppressive rules and policies”可以学到“how certain behaviors naturally carry out undesirable consequences”，家长从”teenagers concerns about autonomy and peer pressure”可以学到”the valuable lesson that effective parenting and control are two different things”。接下来是社会层面（community level），“At the community level, through dispassionate dialogue an environmental activist can come to understand the legitimate economic concerns of those whose jobs depend on the continued profitable operation of a factory. Conversely, the latter might stand to learn much about the potential public health price to be paid by ensuring job growth and a low unemployment rate.”这里从社会层面（community level），选取了环境保护主义者和立法者之间心平气和的谈话(dispassionate dialogue)，环境保护主义者(environmental activist)可以理解有些人的工作完全依赖日夜不停的工厂(understand the legitimate economic concerns of those whose jobs depend on the continued profitable operation of a factory)，立法者和管理者也会了解到追求低失业率给环境和人民健康付出的潜在代价(the potential public health price to be paid by ensuring job growth and a low unemployment rate)。最后从全球层面（global level），”Finally, at the global level, two nations with opposing political and economical interests can reach mutually beneficial agreement by striving to understand the other’s legitimate concerns for its national security, political sovereignty,the stability of its economic and currency, and so forth.”。从国家层面，政治经济利益相违背的国家(two nations with opposing political and economic interests)，通过理解对方在国家安全、政治主权和经济与货币稳定性的担忧(by striving to understand the other’s legitimate concerns for its national security, political sovereignty, and the stability of its economic and currency, and so forth)，就可以实现共赢的利益(can reach mutually beneficial agreements)。这段通过三个层面的举例论证，表达出“debate facilitate(helps) learning”的意思，而且其含义已经远远扩大，小到个人，大到国家。 rationale n. 基本原理 oppressive adj. 压迫的；沉重的 dispassionate adj. 心平气和的；平心静气的(passionate 激情的) sovereignty n. 主权（reign v. 统治） and so forth 等等，诸如此类 continuing in the same way &emsp;&emsp;In sum, unless two opponents in a debate are each willing to play on the same field and by the same rules, I concede that disagreement can impede learning. Otherwise, reasoned discourse and debate between people with opposing viewpoints is the very foundation upon which human knowledge advances. Accordingly, on balance the speaker is fundamentally correct. 第六段：最后一段要总结啦。”In sum, unless two opponents in a debate are each willing to play on the same file and by the same rules, I concede that disagreement can impede learning.”。除非两者能公平的辩论(play on the same field and by the same rules)，那那个speaker就说的对呗。”Otherwise, reasoned discourse and debate between people with opposing viewpoint is the very foundation upon which human knowledge advances. Accordingly, on balance the speaker is fundamentally correct.”最后两句用到了前几段出现的话再次升华，最后很幽默的说一句，speaker的话也倒不是不对昂。 accordingly adv. 因此；相应的；于是 on balance 总之 写在文章的最后&emsp;&emsp;现在已经1:51am了，这篇文章大概敲了快四个小时吧，以前从没写过GRE的作文，这也是差不多第一次看别人的GRE作文，第一印象就是字数好多，要求的字数也好多，再其次就是这篇文章真的好多好多的词和短语和结构用法都不会，但通过查字典，自己分析，感觉在这一篇文章中学到了太多太多的东西，也再一次认识到了自己知识的匮乏。这些东西也不是一蹴而就的，需要不断的积累，不断的磨练，你也不知道你在什么时候会走到哪一步，也许这就是学习有趣的地方吧，很多都是个未知数。可是留给我们的时间还有多少呢，很多事情都是最近思考才想明白的，大一大二浪费了那么多宝贵的时间，因为无知，因为眼界狭窄，如今称不上眼界宽广的人，但比之前的自己，长大了许多。如今也开始努力，开始不再怀疑梦想，努力的去实现它。这条路必经布满靳棘，太容易让人放弃，而且你也一定是孤身一人的在走，在真正的困难面前，没有人帮的了你，你也心里清楚，就算有人要帮你，你也要拒绝，因为这是你的战斗，你必须一个人战胜它，才叫胜利。有时候也很享受这种生活，很充实，至少学了一天，我不会后悔，但如果玩了一天，那时间过去了就过去了，再也不会回来了。这就叫矫情吧，在痛苦中享受着被虐的快感。不知道这种生活会持续到什么时候，也不知道自己终究会坚持多久，但我知道，要想成功，必须坚持，而且要坚持到最后的最后。听过很多人的故事，但我们终究要写下我们自己的故事，而我希望，我能写的精彩一些。马上也就要开学了，不知道下学期，又会是怎样的自己，马上也就要毕业了，不知道下一站，会到哪里。人生这趟旅途，走走停停，停停走走，你永远不知道自己会走到哪里，也不知道自己在哪里驻足，和谁，看什么样的风景。 &emsp;&emsp;今天的口语还没练，网课还没刷，不矫情了，滚去学习了]]></content>
      <categories>
        <category>GRE</category>
      </categories>
      <tags>
        <tag>GRE</tag>
        <tag>GRE Issue</tag>
      </tags>
  </entry>
</search>
